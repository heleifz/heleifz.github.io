<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  fastText 源码分析 - Helei's Tech Notes
  
  </title>
 <meta name="description" content="(map learn unkown-stream)">
 <link href="atom.xml" rel="alternate" title="Helei's Tech Notes" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />

    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
    <script src="asset/highlightjs/highlight.pack.js"></script>
    <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
    <script>hljs.initHighlightingOnLoad();</script>
    
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>

<div id="header">
    <h1><a href="index.html">Helei's Tech Notes</a></h1>
</div>

</nav>
        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; Helei's Tech Notes</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
      <li><a href="index.html">Home</a></li>
      
        <li class="divider"></li>
        <li><label>技术</label></li>

          
            <li><a title="Apache 配置文件解释" href="14698086378177.html">Apache 配置文件解释</a></li>
          
            <li><a title="shared_ptr 原理及事故" href="14696398760857.html">shared_ptr 原理及事故</a></li>
          
            <li><a title="Git" href="14696393471602.html">Git</a></li>
          

      
        <li class="divider"></li>
        <li><label>算法</label></li>

          
            <li><a title="fastText 源码分析" href="14732610572844.html">fastText 源码分析</a></li>
          
            <li><a title="EM 算法解释" href="14702353940825.html">EM 算法解释</a></li>
          
            <li><a title="SVM 推导" href="14698080869715.html">SVM 推导</a></li>
          
            <li><a title="Text Rank 算法应用" href="14696391901766.html">Text Rank 算法应用</a></li>
          
            <li><a title="神经网络计算模型 - 上篇：理论解释" href="14696391071598.html">神经网络计算模型 - 上篇：理论解释</a></li>
          
            <li><a title="WSABIE 算法解释" href="14696374110477.html">WSABIE 算法解释</a></li>
          

      
      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>

        <section id="main-content" role="main" class="scroll-container">

          <div class="row">
            <div class="large-3 medium-3 columns">
              <div class="hide-for-small">
                <div class="sidebar">
                <nav>
                  <ul id="side-nav" class="side-nav">

                    
                      <li class="side-title"><span>技术</span></li>
                        
                          <li><a title="Apache 配置文件解释" href="14698086378177.html">Apache 配置文件解释</a></li>
                        
                          <li><a title="shared_ptr 原理及事故" href="14696398760857.html">shared_ptr 原理及事故</a></li>
                        
                          <li><a title="Git" href="14696393471602.html">Git</a></li>
                        

                    
                      <li class="side-title"><span>算法</span></li>
                        
                          <li><a title="fastText 源码分析" href="14732610572844.html">fastText 源码分析</a></li>
                        
                          <li><a title="EM 算法解释" href="14702353940825.html">EM 算法解释</a></li>
                        
                          <li><a title="SVM 推导" href="14698080869715.html">SVM 推导</a></li>
                        
                          <li><a title="Text Rank 算法应用" href="14696391901766.html">Text Rank 算法应用</a></li>
                        
                          <li><a title="神经网络计算模型 - 上篇：理论解释" href="14696391071598.html">神经网络计算模型 - 上篇：理论解释</a></li>
                        
                          <li><a title="WSABIE 算法解释" href="14696374110477.html">WSABIE 算法解释</a></li>
                        

                    
                  </ul>
                </nav>
                </div>
              </div>
            </div>
            <div class="large-9 medium-9 columns">

 <div class="markdown-body">
<h1>fastText 源码分析</h1>

<h2 id="toc_0">介绍</h2>

<p><a href="https://github.com/facebookresearch/fastText">fastText</a> 是 facebook 近期开源的一个词向量计算以及文本分类工具，该工具的理论基础是以下两篇论文：</p>

<blockquote>
<p><a href="https://arxiv.org/pdf/1607.04606v1.pdf">Enriching Word Vectors with Subword Information</a></p>
</blockquote>

<p>这篇论文提出了用 word n-gram 的向量之和来代替简单的词向量的方法，以解决简单 word2vec 无法处理同一词的不同形态的问题。fastText 中提供了 maxn 这个参数来确定 word n-gram 的 n 的大小。</p>

<blockquote>
<p><a href="https://arxiv.org/pdf/1607.01759v2.pdf">Bag of Tricks for Efficient Text Classification</a></p>
</blockquote>

<p>这篇论文提出了 fastText 算法，该算法实际上是将目前用来算 word2vec 的网络架构做了个小修改，原先使用一个词的上下文的所有词向量之和来预测词本身（CBOW 模型），现在改为用一段短文本的 词向量之和来对文本进行分类。</p>

<p>在我看来，fastText 的价值是提供了一个 <strong>更具可读性，模块化程度较好</strong> 的 word2vec 的实现，附带一些新的分类功能，本文详细分析它的源码。</p>

<h2 id="toc_1">顶层结构</h2>

<p>fastText 的代码结构以及各模块的功能如下图所示：</p>

<p><img src="media/14732610572844/fasttext-arch.png" alt="fasttext-arch"/></p>

<p>分析各模块时，我只会解释该模块的 <strong>主要调用路径</strong> 下的源码，以 <strong>注释</strong> 的方式说明，其它的功能性代码请大家自行阅读。如果对 word2vec 的理论和相关术语不了解，请先阅读这篇 <a href="http://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a>。</p>

<h2 id="toc_2">训练数据格式</h2>

<p>训练数据格式为一行一个句子，每个词用空格分割，如果一个词带有前缀“<code>__label__</code>”，那么它就作为一个类标签，在文本分类时使用，这个前缀可以通过<code>-label</code>参数自定义。训练文件支持 UTF-8 格式。</p>

<h2 id="toc_3">word n-gram</h2>

<h2 id="toc_4">fasttext 模块</h2>

<p>fasttext 是最顶层的模块，它的主要功能是<code>训练</code>和<code>预测</code>，首先是<code>训练</code>功能的调用路径，第一个函数是 <code>train</code>，它的主要作用是 <strong>初始化参数，启动多线程训练</strong>，请大家留意源码中的相关部分。</p>

<pre><code class="language-cpp">void FastText::train(std::shared_ptr&lt;Args&gt; args) {
  args_ = args;
  dict_ = std::make_shared&lt;Dictionary&gt;(args_);
  std::ifstream ifs(args_-&gt;input);
  if (!ifs.is_open()) {
    std::cerr &lt;&lt; &quot;Input file cannot be opened!&quot; &lt;&lt; std::endl;
    exit(EXIT_FAILURE);
  }
  // 根据输入文件初始化词典
  dict_-&gt;readFromFile(ifs);
  ifs.close();

   // 初始化输入层, 对于普通 word2vec，输入层就是一个词向量的查找表，
   // 所以它的大小为 nwords 行，dim 列（dim 为词向量的长度），但是 fastText 用了
   // word n-gram 作为输入，所以输入矩阵的大小为 (nwords + ngram 种类) * dim
   // 代码中，所有 word n-gram 都被 hash 到固定数目的 bucket 中，所以输入矩阵的大小为
   // (nwords + bucket 个数) * dim
  input_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nwords()+args_-&gt;bucket, args_-&gt;dim);
  
  // 初始化输出层，输出层无论是用负采样，层次 softmax，还是普通 softmax，
  // 对于每种可能的输出，都有一个 dim 维的参数向量与之对应
  // 当 args_-&gt;model == model_name::sup 时，训练分类器，
  // 所以输出的种类是标签总数 dict_-&gt;nlabels()
  if (args_-&gt;model == model_name::sup) {
    output_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nlabels(), args_-&gt;dim);
  } else {
  // 否则训练的是词向量，输出种类就是词的种类 dict_-&gt;nwords()
    output_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nwords(), args_-&gt;dim);
  }
  input_-&gt;uniform(1.0 / args_-&gt;dim);
  output_-&gt;zero();

  start = clock();
  tokenCount = 0;
  
  // 库采用 C++ 标准库的 thread 来实现多线程
  std::vector&lt;std::thread&gt; threads;
  for (int32_t i = 0; i &lt; args_-&gt;thread; i++) {
    // 实际的训练发生在 trainThread 中
    threads.push_back(std::thread([=]() { trainThread(i); }));
  }
  for (auto it = threads.begin(); it != threads.end(); ++it) {
    it-&gt;join();
  }
  
  // Model 的所有参数（input_, output_）是在初始化时由外界提供的，
  // 此时 input_ 和 output_ 已经处于训练结束的状态
  model_ = std::make_shared&lt;Model&gt;(input_, output_, args_, 0);

  saveModel();
  if (args_-&gt;model != model_name::sup) {
    saveVectors();
  }
}
</code></pre>

<p>下面，我们进入 <code>trainThread</code>函数，看看训练的主体逻辑，该函数的主要工作是 <strong>实现了标准的随机梯度下降</strong>，并随着训练的进行逐步降低学习率。</p>

<pre><code class="language-cpp">void FastText::trainThread(int32_t threadId) {

  std::ifstream ifs(args_-&gt;input);
  // 根据线程数，将训练文件按照总字节数（utils::size）均分成多个部分
  // 这么做的一个后果是，每一部分的第一个词有可能从中间被切断，
  // 这样的&quot;小噪音&quot;对于整体的训练结果无影响
  utils::seek(ifs, threadId * utils::size(ifs) / args_-&gt;thread);

  Model model(input_, output_, args_, threadId);
  if (args_-&gt;model == model_name::sup) {
    model.setTargetCounts(dict_-&gt;getCounts(entry_type::label));
  } else {
    model.setTargetCounts(dict_-&gt;getCounts(entry_type::word));
  }

  // 训练文件中的 token 总数
  const int64_t ntokens = dict_-&gt;ntokens();
  // 当前线程处理完毕的 token 总数
  int64_t localTokenCount = 0;
  std::vector&lt;int32_t&gt; line, labels;
  // tokenCount 为所有线程处理完毕的 token 总数
  // 当处理了 args_-&gt;epoch 遍所有 token 后，训练结束 
  while (tokenCount &lt; args_-&gt;epoch * ntokens) {
    // progress = 0 ~ 1，代表当前训练进程，随着训练的进行逐渐增大
    real progress = real(tokenCount) / (args_-&gt;epoch * ntokens);
    // 学习率根据 progress 线性下降
    real lr = args_-&gt;lr * (1.0 - progress);
    localTokenCount += dict_-&gt;getLine(ifs, line, labels, model.rng);
    // 根据训练需求的不同，这里用的更新策略也不同，它们分别是：
    // 1. 有监督学习（分类）
    if (args_-&gt;model == model_name::sup) {
      dict_-&gt;addNgrams(line, args_-&gt;wordNgrams);
      supervised(model, lr, line, labels);
    // 2. word2vec (CBOW)
    } else if (args_-&gt;model == model_name::cbow) {
      cbow(model, lr, line);
    // 3. word2vec (SKIPGRAM)
    } else if (args_-&gt;model == model_name::sg) {
      skipgram(model, lr, line);
    }
    // args_-&gt;lrUpdateRate 是每个线程学习率的变化率，默认为 100，
    // 它的作用是，每处理一定的行数，再更新全局的 tokenCount 变量，从而影响学习率
    if (localTokenCount &gt; args_-&gt;lrUpdateRate) {
      tokenCount += localTokenCount;
      // 每次更新 tokenCount 后，重置计数
      localTokenCount = 0;
      // 0 号线程负责将训练进度输出到屏幕
      if (threadId == 0) {
        printInfo(progress, model.getLoss());
      }
    }
  }
  if (threadId == 0) {
    printInfo(1.0, model.getLoss());
    std::cout &lt;&lt; std::endl;
  }
  ifs.close();
}
</code></pre>

<blockquote>
<p><strong>一哄而上的并行训练</strong>：每个训练线程在更新参数时并没有加锁，这会给参数更新带来一些噪音，但是不会影响最终的结果。无论是 google 的 word2vec 实现，还是 fastText 库，都没有加锁。</p>
</blockquote>

<p>从 <code>trainThread</code> 函数中我们发现，实际的模型更新策略发生在 <code>supervised</code>,<code>cbow</code>,<code>skipgram</code>三个函数中，这三个函数都调用同一个 <code>model.update</code> 函数来更新参数，这个函数属于 model 模块，但在这里我先简单介绍它，以方便大家理解代码。</p>

<p>update 函数的原型为</p>

<pre><code class="language-cpp">void Model::update(const std::vector&lt;int32_t&gt;&amp; input, int32_t target, real lr)
</code></pre>

<p>该函数有三个参数，分别是“输入”，“类标签”，“学习率”。</p>

<ul>
<li>输入是一个 <code>int32_t</code>数组，每个元素代表一个词在 dictionary 里的 ID。对于分类问题，这个数组代表输入的短文本，对于 word2vec，这个数组代表一个词的上下文。</li>
<li>类标签是一个 <code>int32_t</code> 变量。对于 word2vec 来说，它就是带预测的词的 ID，对于分类问题，它就是类的 label 在 dictionary 里的 ID。因为 label 和词在词表里一起存放，所以有统一的 ID 体系。</li>
</ul>

<p>下面，我们回到 fasttext 模块的三个更新函数：</p>

<pre><code class="language-cpp">void FastText::supervised(Model&amp; model, real lr,
                          const std::vector&lt;int32_t&gt;&amp; line,
                          const std::vector&lt;int32_t&gt;&amp; labels) {
  if (labels.size() == 0 || line.size() == 0) return;
  // 因为一个句子可以打上多个 label，但是 fastText 的架构实际上只有支持一个 label
  // 所以这里随机选择一个 label 来更新模型，这样做会让其它 label 被忽略
  // 所以 fastText 不太适合做多标签的分类
  std::uniform_int_distribution&lt;&gt; uniform(0, labels.size() - 1);
  int32_t i = uniform(model.rng);
  model.update(line, labels[i], lr);
}

void FastText::cbow(Model&amp; model, real lr,
                    const std::vector&lt;int32_t&gt;&amp; line) {
  std::vector&lt;int32_t&gt; bow;
  std::uniform_int_distribution&lt;&gt; uniform(1, args_-&gt;ws);
  
  // 在一个句子中，每个词可以进行一次 update
  for (int32_t w = 0; w &lt; line.size(); w++) {
    // 一个词的上下文长度是随机产生的
    int32_t boundary = uniform(model.rng);
    bow.clear();
    // 以当前词为中心，将左右 boundary 个词加入 input
    for (int32_t c = -boundary; c &lt;= boundary; c++) {
      // 当然，不能数组越界
      if (c != 0 &amp;&amp; w + c &gt;= 0 &amp;&amp; w + c &lt; line.size()) {
        // 实际被加入 input 的不止是词本身，还有词的 word n-gram
        const std::vector&lt;int32_t&gt;&amp; ngrams = dict_-&gt;getNgrams(line[w + c]);
        bow.insert(bow.end(), ngrams.cbegin(), ngrams.cend());
      }
    }
    // 完成一次 CBOW 更新
    model.update(bow, line[w], lr);
  }
}

void FastText::skipgram(Model&amp; model, real lr,
                        const std::vector&lt;int32_t&gt;&amp; line) {
  std::uniform_int_distribution&lt;&gt; uniform(1, args_-&gt;ws);
  for (int32_t w = 0; w &lt; line.size(); w++) {
    // 一个词的上下文长度是随机产生的
    int32_t boundary = uniform(model.rng);
    // 采用词+word n-gram 来预测这个词的上下文的所有的词
    const std::vector&lt;int32_t&gt;&amp; ngrams = dict_-&gt;getNgrams(line[w]);
    // 在 skipgram 中，对上下文的每一个词分别更新一次模型
    for (int32_t c = -boundary; c &lt;= boundary; c++) {
      if (c != 0 &amp;&amp; w + c &gt;= 0 &amp;&amp; w + c &lt; line.size()) {
        model.update(ngrams, line[w + c], lr);
      }
    }
  }
}

</code></pre>

<p>训练部分的代码已经分析完毕，预测部分的代码就简单多了，它的主要逻辑都在 <code>model.predict</code> 函数里。</p>

<pre><code class="language-cpp">void FastText::predict(const std::string&amp; filename, int32_t k, bool print_prob) {
  std::vector&lt;int32_t&gt; line, labels;
  std::ifstream ifs(filename);
  if (!ifs.is_open()) {
    std::cerr &lt;&lt; &quot;Test file cannot be opened!&quot; &lt;&lt; std::endl;
    exit(EXIT_FAILURE);
  }
  while (ifs.peek() != EOF) {
    // 读取输入文件的每一行
    dict_-&gt;getLine(ifs, line, labels, model_-&gt;rng);
    // 将一个词的 n-gram 加入词表，用于处理未登录词。（即便一个词不在词表里，我们也可以用它的 word n-gram 来预测一个结果）
    dict_-&gt;addNgrams(line, args_-&gt;wordNgrams);
    if (line.empty()) {
      std::cout &lt;&lt; &quot;n/a&quot; &lt;&lt; std::endl;
      continue;
    }
    std::vector&lt;std::pair&lt;real, int32_t&gt;&gt; predictions;
    // 调用 model 模块的预测接口，获取 k 个最可能的分类
    model_-&gt;predict(line, k, predictions);
    // 输出结果
    for (auto it = predictions.cbegin(); it != predictions.cend(); it++) {
      if (it != predictions.cbegin()) {
        std::cout &lt;&lt; &#39; &#39;;
      }
      std::cout &lt;&lt; dict_-&gt;getLabel(it-&gt;second);
      if (print_prob) {
        std::cout &lt;&lt; &#39; &#39; &lt;&lt; exp(it-&gt;first);
      }
    }
    std::cout &lt;&lt; std::endl;
  }
  ifs.close();
}
</code></pre>

<p>通过对 fasttext 模块的分析，我们发现它最核心的预测和更新逻辑都在 model 模块中，接下来，我们进入 model 模块一探究竟。</p>

<h2 id="toc_5">model 模块</h2>

<p>model 模块对外提供的服务可以分为 <code>update</code> 和 <code>predict</code> 两类，下面我们分别对它们进行分析。由于这里的参数较多，我们先以图示标明各个参数在模型中所处的位置，以免各位混淆。</p>

<p><img src="media/14732610572844/fasttext-model-arch.png" alt="fasttext-model-arch"/></p>

<p>图中所有变量的名字全部与 model 模块中的名字保持一致，注意到 <code>wo_</code> 矩阵在不同的输出层结构中扮演着不同的角色。</p>

<h3 id="toc_6">update</h3>

<p><code>update</code> 函数的作用已经在前面介绍过，下面我们看一下它的实现：</p>

<pre><code class="language-cpp">void Model::update(const std::vector&lt;int32_t&gt;&amp; input, int32_t target, real lr) {
  // target 必须在合法范围内
  assert(target &gt;= 0);
  assert(target &lt; osz_);
  if (input.size() == 0) return;
  // 计算前向传播：输入层 -&gt; 隐层
  hidden_.zero();
  for (auto it = input.cbegin(); it != input.cend(); ++it) {
    // hidden_ 向量保存输入词向量的均值，
    // addRow 的作用是将 wi_ 矩阵的第 *it 列加到 hidden_ 上
    hidden_.addRow(*wi_, *it);
  }
  // 求和后除以输入词个数，得到均值向量
  hidden_.mul(1.0 / input.size());
  
  // 根据输出层的不同结构，调用不同的函数，在各个函数中，
  // 不仅通过前向传播算出了 loss_，还进行了反向传播，计算出了 grad_，后面逐一分析。
  // 1. 负采样
  if (args_-&gt;loss == loss_name::ns) {
    loss_ += negativeSampling(target, lr);
  } else if (args_-&gt;loss == loss_name::hs) {
  // 2. 层次 softmax
    loss_ += hierarchicalSoftmax(target, lr);
  } else {
  // 3. 普通 softmax
    loss_ += softmax(target, lr);
  }
  nexamples_ += 1;

  // 如果是在训练分类器，就将 grad_ 除以 input_ 的大小
  // 原因不明
  if (args_-&gt;model == model_name::sup) {
    grad_.mul(1.0 / input.size());
  }
  // 反向传播，将 hidden_ 上的梯度传播到 wi_ 上的对应行
  for (auto it = input.cbegin(); it != input.cend(); ++it) {
    wi_-&gt;addRow(grad_, *it, 1.0);
  }
}
</code></pre>

<p>下面我们看看三种输出层对应的更新函数：<code>negativeSampling</code>,<code>hierarchicalSoftmax</code>,<code>softmax</code>。</p>

<p>model 模块中最有意思的部分就是将层次 softmax 和负采样统一抽象成多个二元 logistic regression 计算。</p>

<p>如果使用负采样，训练时每次选择一个正样本，随机采样几个负样本，每种输出都对应一个参数向量，保存于 <code>wo_</code> 的各行。对所有样本的参数更新，都是一次独立的 LR 参数更新。</p>

<p>如果使用层次 softmax，对于每个目标词，都可以在构建好的霍夫曼树上确定一条从根节点到叶节点的路径，路径上的每个非叶节点都是一个 LR，参数保存在 <code>wo_</code> 的各行上，训练时，这条路径上的 LR 各自独立进行参数更新。</p>

<p>无论是负采样还是层次 softmax，在神经网络的计算图中，所有 LR 都会依赖于 <code>hidden_</code>的值，所以 <code>hidden_</code> 的梯度 <code>grad_</code> 是各个 LR 的反向传播的梯度的累加。</p>

<p>LR 的代码如下：</p>

<pre><code class="language-cpp">real Model::binaryLogistic(int32_t target, bool label, real lr) {
  // 将 hidden_ 和参数矩阵的第 target 行做内积，并计算 sigmoid
  real score = utils::sigmoid(wo_-&gt;dotRow(hidden_, target));
  // 计算梯度时的中间变量
  real alpha = lr * (real(label) - score);
  // Loss 对于 hidden_ 的梯度累加到 grad_ 上
  grad_.addRow(*wo_, target, alpha);
  // Loss 对于 LR 参数的梯度累加到 wo_ 的对应行上
  wo_-&gt;addRow(hidden_, target, alpha);
  // LR 的 Loss
  if (label) {
    return -utils::log(score);
  } else {
    return -utils::log(1.0 - score);
  }
}
</code></pre>

<p>经过以上的分析，下面三种逻辑就比较容易理解了：</p>

<pre><code class="language-cpp">real Model::negativeSampling(int32_t target, real lr) {
  real loss = 0.0;
  grad_.zero();
  for (int32_t n = 0; n &lt;= args_-&gt;neg; n++) {
    // 对于正样本和负样本，分别更新 LR
    if (n == 0) {
      loss += binaryLogistic(target, true, lr);
    } else {
      loss += binaryLogistic(getNegative(target), false, lr);
    }
  }
  return loss;
}

real Model::hierarchicalSoftmax(int32_t target, real lr) {
  real loss = 0.0;
  grad_.zero();
  // 先确定霍夫曼树上的路径
  const std::vector&lt;bool&gt;&amp; binaryCode = codes[target];
  const std::vector&lt;int32_t&gt;&amp; pathToRoot = paths[target];
  // 分别对路径上的中间节点做 LR
  for (int32_t i = 0; i &lt; pathToRoot.size(); i++) {
    loss += binaryLogistic(pathToRoot[i], binaryCode[i], lr);
  }
  return loss;
}

// 普通 softmax 的参数更新
real Model::softmax(int32_t target, real lr) {
  grad_.zero();
  computeOutputSoftmax();
  for (int32_t i = 0; i &lt; osz_; i++) {
    real label = (i == target) ? 1.0 : 0.0;
    real alpha = lr * (label - output_[i]);
    grad_.addRow(*wo_, i, alpha);
    wo_-&gt;addRow(hidden_, i, alpha);
  }
  return -utils::log(output_[target]);
}
</code></pre>

<h3 id="toc_7">predict</h3>

<p>predict 函数可以用于给输入数据打上 1 ～ K 个类标签，并输出各个类标签对应的概率值，对于层次 softmax，我们需要遍历霍夫曼树，找到 top－K 的结果，对于普通 softmax（包括负采样和 softmax 的输出），我们需要遍历结果数组，找到 top－K。</p>

<pre><code class="language-cpp">void Model::predict(const std::vector&lt;int32_t&gt;&amp; input, int32_t k, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  assert(k &gt; 0);
  heap.reserve(k + 1);
  // 计算 hidden_
  computeHidden(input);
  
  // 如果是层次 softmax，使用 dfs 遍历霍夫曼树的所有叶子节点，找到 top－k 的概率
  if (args_-&gt;loss == loss_name::hs) {
    dfs(k, 2 * osz_ - 2, 0.0, heap);
  } else {
  // 如果是普通 softmax，在结果数组里找到 top-k
    findKBest(k, heap);
  }
  // 对结果进行排序后输出
  // 因为 heap 中虽然一定是 top-k，但并没有排好序
  std::sort_heap(heap.begin(), heap.end(), comparePairs);
}

void Model::findKBest(int32_t k, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  // 计算结果数组
  computeOutputSoftmax();
  for (int32_t i = 0; i &lt; osz_; i++) {
    if (heap.size() == k &amp;&amp; utils::log(output_[i]) &lt; heap.front().first) {
      continue;
    }
    // 使用一个堆来保存 top－k 的结果，这是算 top-k 的标准做法
    heap.push_back(std::make_pair(utils::log(output_[i]), i));
    std::push_heap(heap.begin(), heap.end(), comparePairs);
    if (heap.size() &gt; k) {
      std::pop_heap(heap.begin(), heap.end(), comparePairs);
      heap.pop_back();
    }
  }
}

void Model::dfs(int32_t k, int32_t node, real score, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  if (heap.size() == k &amp;&amp; score &lt; heap.front().first) {
    return;
  }

  if (tree[node].left == -1 &amp;&amp; tree[node].right == -1) {
    // 只输出叶子节点的结果
    heap.push_back(std::make_pair(score, node));
    std::push_heap(heap.begin(), heap.end(), comparePairs);
    if (heap.size() &gt; k) {
      std::pop_heap(heap.begin(), heap.end(), comparePairs);
      heap.pop_back();
    }
    return;
  }
  
  // 将 score 累加后递归向下收集结果
  real f = utils::sigmoid(wo_-&gt;dotRow(hidden_, node - osz_));
  dfs(k, tree[node].left, score + utils::log(1.0 - f), heap);
  dfs(k, tree[node].right, score + utils::log(f), heap);
}
</code></pre>

<h2 id="toc_8">其它模块</h2>

<p>除了以上两个模块，dictionary 模块也相当重要，它完成了训练文件载入，哈希表构建，word n-gram 计算等功能，但是并没有太多算法在里面。</p>

<p>其它模块例如 Matrix, Vector 也只是封装了简单的矩阵向量操作，这里不再做详细分析。</p>

<h2 id="toc_9">附录：构建霍夫曼树算法分析</h2>

<p>在学信息论的时候接触过构建 Huffman 树的算法，课本中的方法描述往往是：</p>

<blockquote>
<p>找到当前权重最小的两个子树，将它们合并</p>
</blockquote>

<p>算法的性能取决于如何实现这个逻辑。网上的很多实现都是在新增节点都时遍历一次当前所有的树，这种算法的复杂度是 \(O(n^2)\)，非常愚蠢。</p>

<p>聪明一点的方法是用一个优先级队列来保存当前所有的树，每次取 top 2，合并，加回队列。这个算法的复杂度是 \(O(nlogn)\)，缺点是必需使用额外的数据结构，而且进堆出堆的操作导致常数项较大。</p>

<p>word2vec 以及 fastText 都采用了一种更好的方法，时间复杂度是 \(O(nlogn)\)，只用了一次排序，一次遍历，简洁优美，但是要理解它需要进行一些推理。</p>

<p>算法如下：</p>

<pre><code class="language-cpp">void Model::buildTree(const std::vector&lt;int64_t&gt;&amp; counts) {
  // counts 数组保存每个叶子节点的词频，降序排列
  // 分配所有节点的空间
  tree.resize(2 * osz_ - 1);
  // 初始化节点属性
  for (int32_t i = 0; i &lt; 2 * osz_ - 1; i++) {
    tree[i].parent = -1;
    tree[i].left = -1;
    tree[i].right = -1;
    tree[i].count = 1e15;
    tree[i].binary = false;
  }
  for (int32_t i = 0; i &lt; osz_; i++) {
    tree[i].count = counts[i];
  }
  // leaf 指向当前未处理的叶子节点的最后一个，也就是权值最小的叶子节点
  int32_t leaf = osz_ - 1;
  // node 指向当前未处理的非叶子节点的第一个，也是权值最小的非叶子节点
  int32_t node = osz_;
  // 逐个构造所有非叶子节点（i &gt;= osz_, i &lt; 2 * osz - 1)
  for (int32_t i = osz_; i &lt; 2 * osz_ - 1; i++) {
    // 最小的两个节点的下标
    int32_t mini[2];
    
    // 计算权值最小的两个节点，候选只可能是 leaf, leaf - 1,
    // 以及 node, node + 1
    for (int32_t j = 0; j &lt; 2; j++) {
      // 从这四个候选里找到 top-2
      if (leaf &gt;= 0 &amp;&amp; tree[leaf].count &lt; tree[node].count) {
        mini[j] = leaf--;
      } else {
        mini[j] = node++;
      }
    }
    // 更新非叶子节点的属性
    tree[i].left = mini[0];
    tree[i].right = mini[1];
    tree[i].count = tree[mini[0]].count + tree[mini[1]].count;
    tree[mini[0]].parent = i;
    tree[mini[1]].parent = i;
    tree[mini[1]].binary = true;
  }
  // 计算霍夫曼编码
  for (int32_t i = 0; i &lt; osz_; i++) {
    std::vector&lt;int32_t&gt; path;
    std::vector&lt;bool&gt; code;
    int32_t j = i;
    while (tree[j].parent != -1) {
      path.push_back(tree[j].parent - osz_);
      code.push_back(tree[j].binary);
      j = tree[j].parent;
    }
    paths.push_back(path);
    codes.push_back(code);
  }
}
</code></pre>

<p>算法首先对输入的叶子节点进行一次排序（\(O(nlogn)\) ），然后确定两个下标 <code>leaf</code> 和 <code>node</code>，<code>leaf</code> 总是指向当前最小的叶子节点，<code>node</code> 总是指向当前最小的非叶子节点，所以，<strong>最小的两个节点可以从 leaf, leaf - 1, node, node + 1 四个位置中取得</strong>，时间复杂度 \(O(1)\)，每个非叶子节点都进行一次，所以总复杂度为 \(O(n)\)，算法整体复杂度为 \(O(nlogn)\)。</p>


</div>

<br /><br />
<hr />

<div class="row clearfix">
  <div class="large-6 columns">
	<div class="text-left" style="padding:15px 0px;">
		
	</div>
  </div>
  <div class="large-6 columns">
	<div class="text-right" style="padding:15px 0px;">
		
	        <a href="14702353940825.html" 
	        title="Next Post: EM 算法解释">EM 算法解释 &raquo;</a>
	    
	</div>
  </div>
</div>

<div class="row">
<div style="padding:0px 0.93em;" class="share-comments">
<!-- 多说评论框 start -->
	<div class="ds-thread"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"heleitechnotes"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->
</div>
</div>
<script type="text/javascript">
	$(function(){
		var currentURL = '14732610572844.html';
		$('#side-nav a').each(function(){
			if($(this).attr('href') == currentURL){
				$(this).parent().addClass('active');
			}
		});
	});
</script>  
</div></div>


<div class="page-bottom">
  <div class="row">
  <hr />
  <div class="small-9 columns">
  <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
  <div class="small-3 columns">
  <p class="copyright text-right"><a href="#header">TOP</a></p>
  </div>
   
  </div>
</div>

        </section>
      </div>
    </div>
    
    
    <script src="asset/js/foundation.min.js"></script>
    <script src="asset/js/foundation/foundation.offcanvas.js"></script>
    <script>
      $(document).foundation();

     
    </script>
    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

  </body>
</html>
