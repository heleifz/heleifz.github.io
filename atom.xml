<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Helei's Tech Notes]]></title>
  <link href="http://heleifz.github.io/atom.xml" rel="self"/>
  <link href="http://heleifz.github.io/"/>
  <updated>2019-01-10T23:20:48+08:00</updated>
  <id>http://heleifz.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im">MWeb</generator>

  
  <entry>
    <title type="html"><![CDATA[Nand2tetris - 触发器，寄存器，RAM]]></title>
    <link href="http://heleifz.github.io/15471318129470.html"/>
    <updated>2019-01-10T22:50:12+08:00</updated>
    <id>http://heleifz.github.io/15471318129470.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>本文是 nand2tetris 课程的读书笔记，不会列举过多细节，仅精炼出有意思的概念，供日后查阅。</p>
</blockquote>

<hr/>

<p>存储的意思是，维持曾经有的状态。<br/>
一旦有了“曾经”，我们的系统就有了时间概念。<br/>
时钟信号为系统带来时间，触发器使系统能看一眼“曾经”。</p>

<blockquote>
<p>\(out(t) = in(t - 1)\)</p>
</blockquote>

<p>如果将输出端接回输入端，我们可以让 \(out(t)\) 保持无限长的时间：</p>

<blockquote>
<p>\(out(t) = out(t-1)\)</p>
</blockquote>

<p>如果给这个函数加上一个控制位，我们就得到了 1-bit 寄存器：</p>

<blockquote>
<p>\(out(t)=\begin{equation}<br/>
\begin{cases}<br/>
out(t-1)&amp;  load = 0\\<br/>
in&amp; load = 1\\<br/>
\end{cases}<br/>
\end{equation}\)</p>
</blockquote>

<p>如果将它与加法器连在一起，我们就得到了计数器：</p>

<blockquote>
<p>\(out(t) = out(t-1) + 1\)</p>
</blockquote>

<p>基于触发器与反馈回路的结合，我们可以构造任意复杂的，具有状态的系统。</p>

<p><img src="media/15471318129470/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-01-10%20%E4%B8%8B%E5%8D%8811.10.58.png" alt="屏幕快照 2019-01-10 下午11.10.58"/></p>

<hr/>

<p>用 1-bit 寄存器并联，我们得到了 n-bit register。<br/>
用 k 个 n-bit register 并联，加上寻址逻辑电路，我们得到了 RAM 的基本架构。<br/>
由简单到复杂演变，全依赖组件间的抽象架构。register 提供单个字的存储 <strong>服务</strong>，RAM 调用了这一服务，并对更上层模块提供了随机存储 <strong>服务</strong>。</p>

<p>计算机技术日新月异，程序员很容易跟不上时代，这是真的吗？：）</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nand2tetris - 从布尔逻辑到算数运算]]></title>
    <link href="http://heleifz.github.io/15468825413653.html"/>
    <updated>2019-01-08T01:35:41+08:00</updated>
    <id>http://heleifz.github.io/15468825413653.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>本文是 nand2tetris 课程的读书笔记，不会列举过多细节，仅精炼出有意思的概念，供日后查阅。</p>
</blockquote>

<hr/>

<p>从布尔逻辑的视角来看，所谓加法器只不过是一种特殊的逻辑结构。但是，我们 <strong>人类</strong> 为其加上的新的 <strong>解释</strong>，即【定长二进制加法】，因此它变得 <strong>有用</strong>。名字与实际结构，共同构成了事物的完整概念，从此，这种电路被称为加法器，不再有其它意义。</p>

<hr/>

<p>加法器的级联，使其扩充至 n-bit。</p>

<pre><code class="language-vhdl">....
HalfAdder(a=a[0], b=b[0], sum=out[0], carry=carry1);
FullAdder(a=a[1], b=b[1], c=carry1, sum=out[1], carry=carry2);
FullAdder(a=a[2], b=b[2], c=carry2, sum=out[2], carry=carry3);
FullAdder(a=a[3], b=b[3], c=carry3, sum=out[3], carry=carry4);
FullAdder(a=a[4], b=b[4], c=carry4, sum=out[4], carry=carry5);
FullAdder(a=a[5], b=b[5], c=carry5, sum=out[5], carry=carry6);
FullAdder(a=a[6], b=b[6], c=carry6, sum=out[6], carry=carry7);
....
</code></pre>

<hr/>

<p>二进制补码，是用来表示有符号整数的工具。<br/>
依然是【名字】的作用，同样的操作序列，一旦我们将其重新解释，它立刻从无符号加法，变为有符号加法。</p>

<blockquote>
<p>但这里有个技术问题，为什么二进制补码能够按照这样的方式工作？</p>
</blockquote>

<p>在上大学的时候，老师直接告诉我们二进制补码的计算方式，即按位取反加1，并用时钟的工作方式，说明它的正确性。</p>

<blockquote>
<p>在时钟上，正着走 3 步，等于反着走 12 - 3 步。</p>
</blockquote>

<p><img src="media/15468825413653/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-01-09%20%E4%B8%8A%E5%8D%8812.35.24.png" alt="屏幕快照 2019-01-09 上午12.35.24"/></p>

<p>我们是否有办法证明补码的正确性呢？具体而言：</p>

<blockquote>
<p>a + b 的补码，是否等于 a 的补码 二进制加 b 的补码<br/>
我把无符号二进制加法写作 \(+_2\)，并将 a 的补码定义为 \(comp2(a)\)<br/>
我们将命题改写为：<br/>
\(comp2(a+b)=comp2(a)+_2comp2(b)\)</p>
</blockquote>

<p>证明：<br/>
将原始无符号二进制表示为 \(bin(a)\)，且要求 \(a \ge 0\)，它具有以下性质：</p>

<ol>
<li>\(bin(a)=bin(a + k * 2^n)\)，其中 \(k \gt 0\)，这个很显然，因为是定长二进制，所以高位都丢弃了。</li>
<li>\(bin(x+y)=bin(x)+_2bin(y)\)，这条代表二进制加法的正确性，如果不成立，我们就别用这个工具了。</li>
</ol>

<p>此外，补码的定义为：</p>

<p>\(comp2(a)=\begin{equation}<br/>
\begin{cases}<br/>
bin(a)&amp;  2^{n-1} \gt a \ge 0\\<br/>
bin(2^n + a)&amp; -(2^{n-1}) \le a \lt 0<br/>
\end{cases}<br/>
\end{equation}\)</p>

<p>于是，可以改写命题为：<br/>
\(bin(k*2^n+a+b)=bin(a+m*2^n) +_2 bin(b+l*2^n)\)，其中 k,m,l 都非负。</p>

<p>使用上面的性质1，可以进一步改写命题为：<br/>
\(bin((k+k+m+l)*2^n+a+b)=bin(a+(m+k)*2^n) +_2 bin(b+(l+k)*2^n)\)<br/>
\(bin(a+(m+k)*2^n+b+(l+k)*2^n)=bin(a+(m+k)*2^n) +_2 bin(b+(l+k)*2^n)\)</p>

<p>令 \(x=a+(m+k)*2^n,y=b+(l+k)*2^n\)，可以得到：<br/>
\(bin(x+y)=bin(x) +_2 bin(y)\)，即上面的性质2，故原命题成立。</p>

<p>对于补码的理解，有一些资料可以参考：</p>

<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Modular_arithmetic">https://en.wikipedia.org/wiki/Modular_arithmetic</a><br/>
<a href="https://igoro.com/archive/why-computers-represent-signed-integers-using-twos-complement/">https://igoro.com/archive/why-computers-represent-signed-integers-using-twos-complement/</a><br/>
<a href="https://courses.engr.illinois.edu/ece199/fa2012/notes/twos-complement.pdf">https://courses.engr.illinois.edu/ece199/fa2012/notes/twos-complement.pdf</a></p>
</blockquote>

<hr/>

<h2 id="toc_0">ALU</h2>

<p>将一系列算术和逻辑运算打包到一个芯片里，并用控制位进行操作选择，就是 ALU。</p>

<pre><code class="language-vhdl">CHIP ALU {
    IN  
        x[16], y[16],  // 16-bit inputs        
        zx, // zero the x input?
        nx, // negate the x input?
        zy, // zero the y input?
        ny, // negate the y input?
        f,  // compute out = x + y (if 1) or x &amp; y (if 0)
        no; // negate the out output?

    OUT 
        out[16], // 16-bit output
        zr, // 1 if (out == 0), 0 otherwise
        ng; // 1 if (out &lt; 0),  0 otherwise

    PARTS:
    // x 的预处理
    Mux16(a=x, b=false, sel=zx, out=zeroedX);
    Not16(in=zeroedX, out=negateX);
    Mux16(a=zeroedX, b=negateX, sel=nx, out=finalX);
    // y 的预处理
    Mux16(a=y, b=false, sel=zy, out=zeroedY);
    Not16(in=zeroedY, out=negateY);
    Mux16(a=zeroedY, b=negateY, sel=ny, out=finalY);
    // 选择 and 或者 add
    And16(a=finalX, b=finalY, out=andXY);
    Add16(a=finalX, b=finalY, out=addXY);
    Mux16(a=andXY, b=addXY, sel=f, out=computed);
    Not16(in=computed, out=negatedComputed);
    Mux16(a=computed, b=negatedComputed, sel=no, out[15]=signBit, out[0..7]=lowBit, out[8..14]=hiBit);
    // 顺便输出状态位
    And(a=signBit, b=true, out=ng);
    And16(a[15]=signBit, a[8..14]=hiBit, a[0..7]=lowBit, b=true, out=out);
    Or8Way(in=lowBit, out=notZeroLow);
    Or8Way(in[0..6]=hiBit, in[7]=signBit, out=notZeroHi);
    Or(a=notZeroLow, b=notZeroHi, out=notZero);
    Not(in=notZero, out=zr);


</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2019 新年快乐]]></title>
    <link href="http://heleifz.github.io/15467823342030.html"/>
    <updated>2019-01-06T21:45:34+08:00</updated>
    <id>http://heleifz.github.io/15467823342030.html</id>
    <content type="html"><![CDATA[
<p>2018 过去了，是收获的一年，我更加清楚了技术的意义。</p>

<blockquote>
<p>技术的目的，是从混乱中找到秩序，从秩序中获得价值。<br/>
技术的方法，是抽象和分析，主要是抽象。</p>
</blockquote>

<p>同时，我渐渐从名称和概念中解脱，思想更加自由，孔子说，君子不器。对于技术人而言，就是不要用名称和概念封死自己的路径。“设计模式”，“架构”，“算法”，“深度学习”，“底层”，有时有用，但不重要。</p>

<p>2018 年也是忙碌的一年，我思考工作甚于关注自己，导致荒废了学业，十分不应该。新的一年，我计划重新开始学习一些有趣且有用的知识，磨练自己的思维。</p>

<blockquote>
<p><a href="https://www.nand2tetris.org/">《The elements of computing systems》</a>手把手构建一个简单的计算机，更深刻理解各类计算系统及其中的抽象方法。<br/>
《控制论与科学方法论》将事物抽象为控制系统，以控制论的语言教你做人。<br/>
...</p>
</blockquote>

<p>在学习过程中，我会尽量写一些东西。曾经我因为担心写的不够全面，不爱做读书笔记，现在我没了这个心理障碍。既然知识的终点就是遗忘，那我不如聊聊自己的思考。</p>

<p>另外，我结婚了，买房了，装修了，这些都与技术关系不大，但让我变成一个更好的人。</p>

<p>2019 年已经到来，祝大家新年快乐。</p>

<p><img src="media/15467823342030/zspace2.png" alt="zspace2"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nand2tetris - 布尔逻辑]]></title>
    <link href="http://heleifz.github.io/15467841044137.html"/>
    <updated>2019-01-06T22:15:04+08:00</updated>
    <id>http://heleifz.github.io/15467841044137.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>本文是 nand2tetris 课程的读书笔记，不会列举过多细节，仅精炼出有意思的概念，供日后查阅。</p>
</blockquote>

<p>应用程序能做的事情，本质上只有字符串操作。从应用程序代码到机器码的一系列转换，也是字符串操作，因为代码本身也是字符串。</p>

<p>字符串在计算机系统中，都是01串。所以，任何字符串操作，都能看作01串的函数。布尔逻辑就是来描述01之间各种变换的工具。</p>

<hr/>

<ul>
<li>布尔逻辑的基本操作可以只有一个：<code>nand</code>。</li>
<li><code>and</code>, <code>or</code>, <code>not</code> 等操作都能以 <code>nand</code> 的组合来表达。</li>
<li>任何01串到01串的映射，都能以表格（真值表）的形式表达，我们能将真值表翻译成由 <code>and</code>, <code>or</code>, <code>not</code> 组成的逻辑表达式。</li>
</ul>

<p>根据这三点，我们可以得出结论：任何布尔逻辑，都能写成 <code>nand</code> 的组合。从简单的组件，生长为复杂的系统，是每个人都爱听的<em>故事</em> ：）。</p>

<p><img src="media/15467841044137/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-01-06%20%E4%B8%8B%E5%8D%8811.05.59.png" alt="屏幕快照 2019-01-06 下午11.05.59"/></p>

<hr/>

<p>书中提供了一个用于逻辑设计的 HDL（硬件描述语言），这是用于制造工具的工具，另外还提供了运行这种语言的硬件模拟器，可以称之为&quot;用于生产制造工具的工具的工具“。如果你觉得工具使用效率低下，可以制造一个更高阶的工具。</p>

<pre><code class="language-vhdl">CHIP Not {
    IN in;
    OUT out;

    PARTS:
    Nand(a=in, b=in, out=out);
}
CHIP And {
    IN a, b;
    OUT out;

    PARTS:
    Nand(a=a, b=b, out=outTmp);
    Not(in=outTmp, out=out);
}
CHIP Or {
    IN a, b;
    OUT out;

    PARTS:
    Not(in=a, out=notA);
    Not(in=b, out=notB);
    Nand(a=notA, b=notB, out=out);
}
CHIP Xor {
    IN a, b;
    OUT out;

    PARTS:
    Or(a=a, b=b, out=orAb);
    Nand(a=a, b=b, out=not11);
    And(a=orAb, b=not11, out=out);
}
CHIP Mux {
    IN a, b, sel;
    OUT out;

    PARTS:
    Not(in=sel, out=notSel);
    // sel = 0, result = a, else resut = 1
    Or(a=a, b=sel, out=selA);
    // sel = 1, result = b else resut = 1
    Or(a=b, b=notSel, out=selB);
    And(a=selA, b=selB, out=out);
}
CHIP DMux {
    IN in, sel;
    OUT a, b;

    PARTS:
    Not(in=sel, out=notSel);
    And(a=in, b=notSel, out=a);
    And(a=in, b=sel, out=b);
}
</code></pre>

<hr/>

<p>任何能实现布尔逻辑<strong>接口</strong>的物理媒介，都能构成计算机，当然，目前主要以电位高低来表达布尔值。数字逻辑设计时，只需要将实现基础布尔逻辑的元件当作黑盒即可，布尔逻辑是电子技术与计算机技术沟通的桥梁。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SVD在图像处理中的基本应用]]></title>
    <link href="http://heleifz.github.io/15084626290253.html"/>
    <updated>2017-10-20T09:23:49+08:00</updated>
    <id>http://heleifz.github.io/15084626290253.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>本文是我2013年左右写的课程作业，非常 Old School，很详细证明了 SVD 分解定理，归档于此。</p>
</blockquote>

<p>矩阵的分解是矩阵理论中的一个重要组成部分。通过将原始矩阵拆分成具有不同特性的矩阵的乘积，不仅能揭示其潜在的属性，加深人们对一个矩阵的理解，还有助于实现各种高效的算法。在众多矩阵分解算法中，矩阵的奇异值分解（Singular Value Decomposition，SVD）占有相当重要的地位。SVD不仅能够将任意矩阵分解为两个单位正交阵以及一个广义对角阵，而且分解后的每个矩阵因子都有明确而重要的数学意义。下面，将给出奇异值的定义和奇异值分解定理，以及它的证明。</p>

<p><strong>奇异值的定义</strong>：</p>

<p>设\(A \in C_r^{m\times n}\)，\(A^HA\)的特征值为\[\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_r \ge \lambda_{r+1}=\cdots = \lambda_n = 0,\]<br/>
则称\(\sigma_i = \sqrt{{\lambda_i}}(i=1,2,\cdots ,r)\)为矩阵\(A\)的正奇异值。</p>

<p><strong>奇异值分解定理</strong>：</p>

<p>设\(A \in C_r^{m\times n}\)，存在\(m\)阶酉矩阵\(U\)和\(n\)阶酉矩阵\(V\)，使得\[A=U\Sigma V^H,\]其中\(\Sigma\)是对角元为\(A\)的\(r\)个正奇异值的广义对角矩阵，酉矩阵\(U\)的每一列称为\(A\)的左奇异向量，\(V\)的每一列称为\(A\)的右奇异向量。</p>

<h2 id="toc_0">为证明奇异值分解定理，首先证明以下三个引理。</h2>

<p>引理1: \(rank(A)=rank(A^HA)=rank(AA^H)\)</p>

<p><strong>证明</strong><br/>
当\(Ax=0\)时，\({A^H}(Ax) = 0\)必然成立。另一方面，当\(A^HAx = 0\)时，\({x^H}{A^H}Ax = {(Ax)^H}Ax = \left\| {Ax} \right\|^2 = 0\)成立，此时\(Ax\)为0。于是\(A^HAx\)与\(Ax\)的零空间(解空间)相同，设其维数为\(k(k &lt; n)\)。<br/>
又因为\(A\)与\(A^HA\)的列数都\(n\)，所以它们的秩也相等\[rank(A^HA)=rank(A)=n-k\]同理，利用左零空间的一致性，很容易证得\(AA^H\)与\(A\)的秩也相同。</p>

<hr/>

<p>引理2: 矩阵\(A^HA\)和\(AA^H\)的所有特征值都是非负的。</p>

<p><strong>证明</strong><br/>
将\(A^HA\)左乘\(x^H\)，右乘\(x\)，得\({x^H}{A^H}Ax = \left\| {Ax} \right\|^2 \ge 0\)，满足半正定矩阵的定义，又因为半正定矩阵具有非负的特征值，所以\(A^HA\)的特征值都是非负的。接着，设\(B=A^H\)，应用上面的结论，得到\(B^HB=(A^H)^HA^H=AA^H\)的特征值也非负。</p>

<hr/>

<p>引理3: 设\(rank(A)=r\)，矩阵\(A^HA\)和\(AA^H\)具有相同的\(r\)个非零特征值。</p>

<p><strong>证明</strong><br/>
设\(\lambda_i\)为\(A^HA\)的一个非零特征值，\(v_i\)为其对应的一个特征向量\[A^HAv_i=\lambda_i v_i,\]将其左乘\(A\)，得\[AA^HAv_i=\lambda_i Av_i\]设\(y_i = Av_i\)，<br/>
因为若\(Av_i\)为0，则\(A^HAv_i=\lambda_i v_i\)为0，又因为\(v_i\)非零，所以只可能\(\lambda_i\)为0，这与\(\lambda_i\)不为0的条件矛盾。所以此时非零向量\(y_i\)为\(AA^H\)对应特征值\(\lambda_i\)的一个特征向量，所以\(A^HA\)与\(AA^H\)的非零特征值集合相同</p>

<p>\(AA^H\)与\(A^HA\)都是Hermitian矩阵，它们都能进行酉对角化，使其与一个实对角矩阵酉相似。因为相似矩阵的特征值和秩相同，而这个对角矩阵具有\(r\)个非零对角元，所以\(AA^H\)与\(A^HA\)都有\(r\)个非零特征值，即所有非零特征值的代数重数之和为\(r\)。又因为Hermitian矩阵的代数重数等于几何重数，所以其任意一个特征值\(\lambda_i\)所对应的特征空间的维数\(r_i\)都小等于\(r\)。</p>

<p>接着，再证明对于某个非零特征值，其在\(A^HA\)与\(AA^H\)上的重数相同。设\[v_i=\left( {\begin{array}{*{20}{c}}<br/>
{{x_1}}&amp;{{x_2}}&amp; \cdots &amp;{{x_{{r_i}}}}<br/>
\end{array}} \right)(r_i \le r)\]为\(A^HA\)一个非零特征值\(\lambda_i\)对应的特征空间的一组基向量，当通过左乘\(A\)将它们变换为\(AA^H\)的特征向量后，新的特征空间的秩满足\[rank(Av_i) \le \min (rank(A),rank({v_i})) = \min (r, r_i) = r_i = rank({v_i})，\]同理，通过对称性易得\[rank(v_i) \le  rank({Av_i})\]所以，每个非零特征值的重数相同。</p>

<p>综上，矩阵\(A^HA\)和\(AA^H\)具有相同的\(r\)个非零特征值。</p>

<hr/>

<p>接下来，证明矩阵的奇异值分解定理。<br/>
<strong>证明</strong>：<br/>
设\(A \in C_r^{m\times n}\)，\[V_1 = \left( {\begin{array}{*{20}{c}}<br/>
{{v_1}}&amp;{{v_2}}&amp; \cdots &amp;{{v_r}}<br/>
\end{array}} \right)\]为\(A^HA\)的\(r\)个正特征值\(\sigma_i^2\)对应的一组标准正交的特征向量（\(\sigma_i\)为\(A\)的第\(i\)个奇异值），满足\[A^HAv_i = \sigma_i^2 v_i (i=1,2,\cdots ,r)\]<br/>
将上式左乘\(v_i^H\)，得\[v_i^HA^HAv_i={\left\| {A{v_i}} \right\|^2}=\sigma_i^2\]<br/>
等式两边同时开根号，得\(\left\| {A{v_i}} \right\|=\sigma_i\)。<br/>
再令\[Y_1 = \left( {{y_1},{y_2}, \cdots ,{y_r}} \right) = A{V_1} = \left( {A{v_1},A{v_2}, \cdots ,A{v_r}} \right)\]由引理3的证明过程可知，\(Y_1\)的各列为\(AA^H\)的特征空间的一组基，而且\[Y_1^HY_1=V_1^HA^HAV_1=\left( {\begin{array}{*{20}{c}}<br/>
{\sigma _1^2}&amp;{}&amp;0\\<br/>
{}&amp; \ddots &amp;{}\\<br/>
0&amp;{}&amp;{\sigma _r^2}<br/>
\end{array}} \right)=D\]由此可知\(y_i(i=1,2,\cdots, r)\)是一组正交向量，且它们的长度为\(\sqrt {{\sigma _i}^2}  = {\sigma _i}\)。将它们单位化，得到各列相互单位正交的\(U_1\)：\[{U_1} = {Y_1}D^{-1}\]综合以上我们可以得到如下关系\[AV_1=U_1D\]这与SVD的最终形式已经很接近了，接下来只需要将\(U_1\)与\(AA^H\)的零特征值对应的单位正交的特征向量组\(U_2\)结合成一个正交矩阵\[U = \left( {\begin{array}{*{20}{c}}<br/>
{{U_1}}&amp;{{U_2}}<br/>
\end{array}} \right),\]类似地将\(V_1\)扩充成正交矩阵\[V = \left( {\begin{array}{*{20}{c}}<br/>
{{V_1}}&amp;{{V_2}}<br/>
\end{array}} \right),\]并令\[\Sigma  = \left( {\begin{array}{*{20}{c}}<br/>
D&amp;O\\<br/>
O&amp;O<br/>
\end{array}} \right)\]我们得到\[\begin{array}{l}<br/>
AV=U\Sigma\\<br/>
A=U\Sigma V^H<br/>
\end{array}\]这就得到了矩阵的奇异值分解。</p>

<h1 id="toc_1">奇异值分解的意义</h1>

<p>SVD后得到的矩阵\(U,V,\Sigma\)都有各自明确的数学意义，下面对其进行说明。</p>

<p>对于任意一个矩阵\(A\in C_r^{m\times n}\)，它都隐含了四个空间，分别为:</p>

<ul>
<li>行空间（Row space）：由矩阵所有行向量张成的空间。</li>
<li>列空间（Column space）：由矩阵所有列向量张成的空间。</li>
<li>零空间（Null space）：方程\(Ax=0\)的所有解构成的空间。</li>
<li>左零空间（Left null space）：方程\(A^Hx=0\)的所有解构成的空间。</li>
</ul>

<p>其中，零空间又称\(A\)作为线性方程组的系数矩阵时的解空间，\(A\)作为线性变换矩阵时的核（Kernel），列空间也叫这个线性变换的值域（Range）。根据线性代数的基本定理（Fundamental theorem of linear algebra），这四个空间之间满足这样的关系：</p>

<ul>
<li>行空间与列空间的维数都为\(r\)。</li>
<li>零空间的维数为\(n-r\)。</li>
<li>左零空间的维数为\(m-r\)。</li>
<li>行空间与零空间互为正交补空间。</li>
<li>列空间与左零空间互为正交补空间。</li>
</ul>

<p>奇异值分解将这四个空间以一种优美的方式联系到了一起。首先，引理1的证明过程可知，矩阵\(A\)与\(A^HA\)有相同的零空间，因为\(A^HA\)的零空间与它的零特征值对应的特征空间也是相同的，所以\(V\)的第\(r+1\)至\(n\)列构成了\(A\)的零空间的一组单位正交基。</p>

<p>有\(V\)的单位正交性可以推知，它的第1至\(r\)列所张成的空间与第\(r+1\)至\(n\)列张成的空间互为正交补空间，根据正交补的唯一性与线性代数基本定理，\(V\)的第1至\(r\)列构成了\(A\)的行空间的一组单位正交基。</p>

<p>同理，\(U\)的第1至\(r\)列构成了\(A\)的列空间的一组单位正交基，第\(r+1\)至\(m\)列构成了\(A\)的左零空间的一组单位正交基。此外，根据奇异值分解定理，\(U\)与\(V\)的前\(r\)列（设为\(U_1\)和\(V_1\)）之间有这样的关系：<br/>
\[AV_1=DU_1(D=diag(\sigma_1,\sigma_2,\cdots ,\sigma_r))\]这样，SVD不仅同时得到了一个矩阵所隐含的四个基本空间的单位正交基，而且让行空间与列空间的基经由一个简单的线性变换联系到了一起。</p>

<h1 id="toc_2">奇异值分解在图像处理中的应用</h1>

<p>SVD在数值计算，文本挖掘等领域都起着重要的作用。下面将介绍其在图像处理领域的基本应用：图像压缩与图像表征。</p>

<h2 id="toc_3">奇异值分解与图像压缩</h2>

<p>数字图像可以看做是一个元素值为对应位置像素灰度值的二维矩阵。随着相关技术的发展，现代数码相机已经能拍摄千万像素级别的照片，因此图像矩阵一般而言尺寸很大。但又因图像中有大量色彩接近的区域，即像素间的相关性较强，所以可想而知这个数据矩阵具有一定的冗余度。</p>

<p>这样的冗余可以通过矩阵的秩来衡量：秩较高的图像矩阵各列之间的相关程度越低，秩较低的图像矩阵各列之间相互依赖较为严重。考虑下图所示的纵向灰度渐变图像（假设图像的灰度值被归一化至\([0,1]\)）它由一个列向量\(u=[0,0.1,0.2,\cdots ,0.9]\)右乘一个长度为10的全1行向量\(v^H\)得来。</p>

<p><img src="media/15084626290253/r1.png" alt=""/></p>

<p>图像矩阵为：\[uv^H=\left( {\begin{array}{*{20}{c}}<br/>
0&amp;0&amp; \cdots &amp;0\\<br/>
{0.1}&amp;{0.1}&amp; \cdots &amp;{0.1}\\<br/>
{0.2}&amp;{0.2}&amp; \cdots &amp;{0.2}\\<br/>
{\begin{array}{*{20}{c}}<br/>
 \vdots \\<br/>
0.9<br/>
\end{array}}&amp;{\begin{array}{*{20}{c}}<br/>
 \vdots \\<br/>
0.9<br/>
\end{array}}&amp;{\begin{array}{*{20}{c}}<br/>
{}\\<br/>
 \cdots <br/>
\end{array}}&amp;{\begin{array}{*{20}{c}}<br/>
 \vdots \\<br/>
0.9<br/>
\end{array}}<br/>
\end{array}} \right)\]</p>

<p>显然，这个矩阵的秩为1，它的每一行都可用除全0行外的其他行乘以一个倍数得到。因此，保留这个矩阵的全部信息只需要\(u,v\)两个向量共20个数即可，剩余的80个矩阵元素都是冗余数据，如此压缩后的图像，获得了相当可观的压缩比：\[CR=\frac{20}{100}=0.2\]当然，这是一种极端的情况，更一般的问题是：对于一个图像矩阵，如何在一定的评判标准之下获得它的低秩的最优近似，而SVD为这一问题提供了解决方案。</p>

<p>将SVD改写成外积展开（Outer product expansion）的形式：<br/>
\[<br/>
\begin{array}{l}<br/>
A = U\Sigma {V^H} = \left( {{u_1},{u_2}, \cdots ,{u_m}} \right)\left( {\begin{array}{*{20}{c}}<br/>
{{\sigma _1}}&amp;{}&amp;{}&amp;O\\<br/>
{}&amp; \ddots &amp;{}&amp;{}\\<br/>
{}&amp;{}&amp;{{\sigma _r}}&amp;{}\\<br/>
O&amp;{}&amp;{}&amp;O<br/>
\end{array}} \right)\left( \begin{array}{l}<br/>
{v_1}^H\\<br/>
{v_2}^H\\<br/>
 \vdots \\<br/>
{v_n}^H<br/>
\end{array} \right)\\<br/>
 = \sum\limits_{i = 1}^r {{\sigma _i}{u_i}{v_i}^H} <br/>
\end{array}<br/>
\]<br/>
在写成求和式以后，SVD仅剩下奇异值非零的前\(r\)项对应的\(u_i\)与\(v_i\)的外积，而剩下的外积项其实是冗余的数据，对\(A\)的构成没有任何贡献。很容易看出，上图的矩阵仅有一个非零特征值。</p>

<p>于是很自然地，我们可以对任意的图像做SVD，舍去它的零奇异值对应的奇异向量，从而实现了它的低秩近似。但是，由于噪声的影响以及图像内容的复杂性，现实中的图像矩阵往往是（行，列）满秩的，它们的非零奇异值数目等于行数或者列数，简单地舍去多余的奇异向量对图像数据压缩没有任何作用。</p>

<p>虽然图像中相似度较强的区域在严格意义上而言是线性无关的，但它们却能被看做是近似地线性相关的，这一事实将导致很小的奇异值的产生，因为这种由像素间微小差异所导致的线性无关对于图像整体表征而言并不重要，所以，我们可以舍去SVD外积展开式中\(\sigma_i\)较小的\(k(0 &lt; k &lt; r)\)项，得到一个\(r-k\)阶的矩阵，将这个矩阵作为原图像的低秩近似。然而，这是一个好的近似吗？</p>

<p>为了说明这一问题，下面首先证明一幅图像在Frobenius范数下最优的秩1近似就是SVD外积展开式中最大奇异值对应的项。</p>

<hr/>

<p>对于一个矩阵\(A \in C_r^{m\times n}\)，最优化问题\[\begin{array}{l}<br/>
O = \min {\left\| {A - {A_1}} \right\|_F}\\<br/>
{\bf{s}}.{\bf{t}}.\;\;rank{\rm{(}}{{\rm{A}}_1}{\rm{) = 1}}<br/>
\end{array}\]在\(A_1=\sigma_1 u_1v_1^H\)时取最小值，其中\(\sigma_1\)是\(A\)的最大奇异值，\(u_1,v_1\)为它对应的左，右奇异向量，\({\left\| {\;}\bullet {\;}\right\|_F}\)为矩阵的Frobenius范数。</p>

<p><strong>证明</strong>：<br/>
将\(A\)进行奇异值分解，得\(A=U\Sigma V^H\)，将其带入目标函数：<br/>
\[<br/>
\begin{array}{l}<br/>
{\left\| {A - {A_1}} \right\|_F}\\<br/>
{\rm{ = }}{\left\| {U\Sigma {V^H} - {A_1}} \right\|_F}<br/>
\end{array}<br/>
\]<br/>
由于Frobenius范数的酉不变性，上式可化为<br/>
\[<br/>
\begin{array}{l}<br/>
{\left\| {U\Sigma {V^H} - {A_1}} \right\|_F}\\<br/>
 = {\left\| {\Sigma  - {U^H}{A_1}V} \right\|_F}<br/>
\end{array}<br/>
\]<br/>
因为\(A_1\)的秩为1，所以\(U^HA_1V\)可以表示为\(\alpha xy^H\)，其中\(x,y\)分别为\(C^M,C^N\)中的单位向量，带入得<br/>
\[<br/>
\begin{array}{l}<br/>
{\left\| {\Sigma  - {U^H}{A_1}V} \right\|_F}\\<br/>
 = {\left\| {\Sigma  - \alpha x{y^H}} \right\|_F}<br/>
\end{array}<br/>
\]<br/>
利用性质\({\left\| X \right\|_F^2} = tr({X^H}X)\)，以及\(tr(XY)=tr(YX)\)将求Frobenius范数化为求矩阵的迹<br/>
\[<br/>
\begin{array}{*{20}{l}}<br/>
{{{\left\| {\Sigma  - \alpha x{y^H}} \right\|}_F^2}}\\<br/>
{ = tr[{{(\Sigma  - \alpha x{y^H})}^H}(\Sigma  - \alpha x{y^H})]}\\<br/>
{ = tr({\Sigma ^H}\Sigma  - {\Sigma ^H}\alpha x{y^H} - \alpha y{x^H}\Sigma  + {\alpha ^2}y{y^H})}\\<br/>
{ = tr({\Sigma ^H}\Sigma ) + {\alpha ^2} - 2\alpha tr[{\Sigma ^H}{\mathop{\rm Re}\nolimits} (x{y^H})]}\\<br/>
{ = {{\left\| \Sigma  \right\|}_F^2} + {\alpha ^2} - 2\alpha \sum\limits_{i = 1}^r {{\sigma _i}{\mathop{\rm Re}\nolimits} ({x_i}{y_i}^*)} }<br/>
\end{array}<br/>
\]<br/>
考察上式中的\({\sum\limits_{i = 1}^r {{\sigma _i}{\mathop{\rm Re}\nolimits} ({x_i}{y_i}^*)} }\)，对其进行不等式放大<br/>
\[<br/>
\begin{array}{l}<br/>
\sum\limits_{i = 1}^r {{\sigma _i}{\mathop{\rm Re}\nolimits} ({x_i}{y_i}^*)}  \le \sum\limits_{i = 1}^r {{\sigma _i}\left\lvert {{x_i}{y_i}^*} \right\rvert}  \le \sum\limits_{i = 1}^r {{\sigma _i}\left\lvert {{x_i}} \right\rvert\left\lvert {{y_i}^*} \right\rvert} \\<br/>
 \le {\sigma _1}\sum\limits_{i = 1}^r {\left\lvert {{x_i}} \right\rvert\left\lvert {{y_i}} \right\rvert} = {\sigma _1}(\tilde x,\tilde y)<br/>
\end{array}<br/>
\]<br/>
其中，\(\tilde x = \left( {\left\lvert {{x_1}} \right\rvert,\left\lvert {{x_2}} \right\rvert, \cdots ,\left\lvert {{x_r}} \right\rvert} \right),\tilde y = \left( {\left\lvert {{y_1}} \right\rvert,\left\lvert {{y_2}} \right\rvert, \cdots ,\left\lvert {{y_r}} \right\rvert} \right)\)，\(( \bullet , \bullet )\)表示\(C^r\)空间的向量内积。根据Cauchy-Schwartz不等式，有<br/>
\[<br/>
\begin{array}{l}<br/>
{\sigma _1}(\tilde x,\tilde y) \le {\sigma _1}\left\lvert {\tilde x} \right\rvert\left\lvert {\tilde y} \right\rvert\\<br/>
 \le {\sigma _1}\left\lvert x \right\rvert\left\lvert y \right\rvert = {\sigma _1}<br/>
\end{array}<br/>
\]<br/>
综上，得到\(\left\| {A - {A_1}} \right\|_F^2\)的下界<br/>
\[<br/>
\begin{array}{l}<br/>
{\left\| {A - {A_1}} \right\|_F^2}\\<br/>
 = {\left\| \Sigma  \right\|_F^2} + {\alpha ^2} - 2\alpha \sum\limits_{i = 1}^r {{\sigma _i}{\rm{Re}}({x_i}{y_i}^*)} \\<br/>
 \ge {\left\| \Sigma  \right\|_F^2} + {\alpha ^2} - 2\alpha {\sigma _1}\\<br/>
 = {\left\| \Sigma  \right\|_F^2} + {(\alpha  - {\sigma _1})^2} - {\sigma _1}^2<br/>
\end{array}<br/>
\]<br/>
当\(\alpha = \sigma_1\)时这个下界取得最小值\({\left\| \Sigma  \right\|_F^2} - {\sigma _1}^2\)，而这个最小值在\(x\)与\(y\)都等于对应空间中的\(e_1=(1,0,\cdots,0)^T\)时取到，这时有<br/>
\[<br/>
{A_1} = \alpha Ux{y^H}{V^H} = \alpha {u_1}{v_1}^H<br/>
\]</p>

<hr/>

<p>在实际应用中，常通过一种迭代n次求秩1近似的贪婪算法来获得矩阵\(A\)的秩k近似。算法流程如下</p>

<p>1) 求矩阵\(A\)的最优秩1近似\(A_1\)<br/>
2) 获得残差矩阵\(E_1 = A - A_1\)<br/>
3) 求\(E_1\)的最优秩1近似\(A_2\)<br/>
4) 求残差矩阵\(E_2 = E_1 - A_2\)<br/>
5) 迭代求k次近似，获得最终的结果 \(\hat A = \sum\limits_{i = 1}^n{A_i}\)</p>

<p>Lawson和Hanson证明了，这种算法得到的近似矩阵是矩阵的最优秩k近似，很容易看出这个最优秩k近似，正是SVD的外积展开式中的前k项和。因为\(E_1 = A - \sigma _1u_1v_1^H = \sum\limits_{i = 2}^r{{\sigma _i}{u_i}{v_i}^H}\)，所以第二轮迭代得到的秩1近似\(A_2\)为\(\sigma_2 u_2v_2^H\)，以此类推，\(A_k=\sigma_k u_kv_k^H\)，最终得到<br/>
\[<br/>
\hat A = \sum\limits_{i = 1}^k{{\sigma _k}{u_k}{v_k}^H}<br/>
\]</p>

<p>综上，矩阵SVD外积展开式的前k项和，就是它的最优k秩近似。下面给出一个具体的图像压缩的例子，原始图像如下图所示，该图像的尺寸为\(66 \times 176\)秩为31。</p>

<p><img src="media/15084626290253/ue.png" alt="ue"/></p>

<p>压缩后的图像如下图所示。</p>

<p><img src="media/15084626290253/sta.ng.png" alt="sta.ng"/></p>

<h2 id="toc_4">奇异值分解与图像表征</h2>

<p>SVD在图像处理中更重要的应用是图像表征，所谓图像表征（Image representation）指的是为图像数据选取某种基向量，使得图像在这组基下能展现出各种有用的特性。例如，离散傅里叶变换就是一种常见的图像表征方法，这种变换所对应的基是一组相互正交离散复指数函数，它能有效提取出图像缓变部分对应的低频分量以及图像细节所对应的高频分量。SVD的左奇异向量组也能构成一种强有力的图像表征，这种表征方式又称为主成分分析（Principle Component Analysis，PCA）。</p>

<p>假设10个2维的零均值的样本数据，如下表所示。</p>

<table>
<thead>
<tr>
<th></th>
<th>\(a_1\)</th>
<th>\(a_2\)</th>
<th>\(a_3\)</th>
<th>\(a_4\)</th>
<th>\(a_5\)</th>
<th>\(a_6\)</th>
<th>\(a_7\)</th>
<th>\(a_8\)</th>
<th>\(a_9\)</th>
<th>\(a_{10}\)</th>
</tr>
</thead>

<tbody>
<tr>
<td>\(x\)</td>
<td>-0.01</td>
<td>0.30</td>
<td>-1.32</td>
<td>-0.42</td>
<td>0.00</td>
<td>1.08</td>
<td>2.42</td>
<td>1.12</td>
<td>0.05</td>
<td>1.47</td>
</tr>
<tr>
<td>\(y\)</td>
<td>-0.31</td>
<td>-0.28</td>
<td>-1.43</td>
<td>-0.57</td>
<td>0.60</td>
<td>0.42</td>
<td>1.99</td>
<td>0.03</td>
<td>0.63</td>
<td>1.74</td>
</tr>
</tbody>
</table>

<p>将这些数据点画在\(xy\)平面上，得到下图，从图中可以看出，这些样本点大致分布在一条过原点直线的周围。若每个\(xy\)对都表示某两个物理量在同一时刻的取值，那么我们可以猜想，这两个物理量可能具有线性关系。现在的问题是，如何计算出这条直线，或者从更一般的意义来说，如何找到一组零均值的\(n\)维数据变化最剧烈的那些方向呢？</p>

<p><img src="http://7xpogj.com1.z0.glb.clouddn.com/2017-10-20-pnts.png" alt="pnts"/></p>

<p>首先考虑一个方向的情况。假定样本矩阵\(X \in R^{m \times n}\)，它的第\(i(i=1,2,\cdots,n)\)列\(x_i\)表示第\(i\)个\(m\)维样本，样本总数为\(n\)个，均值为0：<br/>
\[<br/>
\begin{array}{l}<br/>
X = \left( {{x_1},{x_2}, \cdots ,{x_n}} \right),{x_i} \in {R^m}\\<br/>
\sum\limits_{i = 1}^n {{x_i} = 0\;} <br/>
\end{array}<br/>
\]<br/>
将这些样本投影到某个方向的单位向量\(u\)上，如果\(u\)的方向与样本变化最剧烈的方向相同，那么投影后的各样本向量在\(u\)方向的长度\(u^TX\)的模的平方和\(u^TXX^Tu\)将取到最大值。于是，我们的问题变成了如下的最优化问题：<br/>
\[<br/>
\begin{array}{*{20}{l}}<br/>
{O = \max uX{X^T}u}\\<br/>
{{\bf{s}}.{\bf{t}}.\;\;{u^T}u = 1}<br/>
\end{array}<br/>
\]<br/>
又因为\(XX^T\)是一个对称矩阵，所以可对它进行正交对角化：\(X{X^T} = {Q^T}\Lambda Q\)，于是，我们有<br/>
\[<br/>
{u^T}X{X^T}u = {u^T}{Q^T}\Lambda Qu = {y^T}\Lambda y<br/>
\]<br/>
由于正交变换是保内积的，所以\(y^Ty=1\)。将上式展开，有<br/>
\[<br/>
{y^T}\Lambda y = \sum\limits_{i = 1}^n {{y_i}^2{\lambda _i}}  \le {\lambda _1}\sum\limits_{i = 1}^n {{y_i}^2 = {\lambda _1}}<br/>
\]<br/>
其中\(\lambda_1\)是\(XX^T\)的最大特征值（假设\(\Lambda  = diag({\lambda _1},{\lambda _2}, \cdots ,{\lambda _n})\)中的特征值按降序排列），且当\(y=(1,0,\cdots,0)\)时等号成立。这个时候\(u\)是特征向量矩阵\(Q\)的第一列，即\(\lambda_1\)对应的特征向量。上述性质即是Rayleigh-Ritz定理在实数域的情况，换言之，当\(u\)为样本矩阵\(X\)的最大奇异值对应的左奇异向量时，样本在\(u\)方向的投影的平方和最大，为最大奇异值的平方。类似地，如果我们想找到样本的\(k(k=1,2,..,n)\)个变化最显著的方向，根据Rayleigh-Ritz定理的思想，易得这\(k\)个方向对应着\(X\)的前\(k\)个左奇异向量。</p>

<p>由于这\(k\)个相互正交的向量很好地体现了数据分布的主要趋势，所以能够作为表征数据的一组良好的基向量，它们又称为这组样本的主成分（Principle Component），假设\(k=rank(X)\)，用部分SVD公式表达某个样本数据\(x_j\)在主成分\(u_1,u_2,\cdots,u_k\)下的展开，有<br/>
\[<br/>
\begin{array}{l}<br/>
X = \left( {{u_1},{u_2}, \cdots ,{u_k}} \right)\Sigma {V^T} = \left( {{u_1},{u_2}, \cdots ,{u_k}} \right)C\\<br/>
 \Leftrightarrow \left( {{x_1},{x_2}, \cdots ,{x_n}} \right) = \left( {{u_1},{u_2}, \cdots ,{u_k}} \right)\left( {\begin{array}{*{20}{c}}<br/>
{{c_{11}}}&amp; \ldots &amp;{{c_{1n}}}\\<br/>
 \vdots &amp; \ddots &amp; \vdots \\<br/>
{{c_{k1}}}&amp; \cdots &amp;{{c_{kn}}}<br/>
\end{array}} \right)\\<br/>
 \Leftrightarrow {x_j} = \sum\limits_{i = 1}^k {{u_i}{c_{ij}}} \;\;\;(j = 1,2, \cdots ,n)<br/>
\end{array}<br/>
\]<br/>
在实际应用中，选取的主成分个数\(k\)往往远小于原始数据的维数\(n\)，这样一来，我们就获得了数据的一个更为紧凑的表征形式。我们可以求出表中的数据的第一主成分\(u_1 = {\left( { - 0.74, - 0.67} \right)^T}\)，其对应的奇异值为4.63，将它的平方除以样本个数并开根号，得到投影后样本向量长度的均方根为1.38，上图中的红色箭头就代表了\(u_1\)的方向和样本投影的均方根长度。</p>

<p>在图像处理，尤其是人脸识别领域中，样本矩阵\(X\)的每一列都是一个拉伸成列的图像，假设图像大小是\(256 \times 256\)像素，它对应的向量维数就是65536，随着样本数目的增多，如此高维的数据对于数据存储以及各类算法的运行速度都是一个挑战。然而，相较数万维的图像样本，样本总数一般而言要小得多。假设一个人脸识别系统共有50名用户，每名用户采集10副样本图像，则总样本数为500，它们构成的样本矩阵将是十分“细长”的，即\(X \in R^{65536 \times 500}\)，根据奇异值分解的性质，这个矩阵最多仅有500个非零的奇异值，也就是说，最多只有500个主成分的方向上样本的变化的均方长度大于零，在其它方向上所有样本都没有偏移。</p>

<p>所以，我们可以假定当前用户的所有样本都分布在一个500维的相对低维的空间中，该空间的基向量就是样本矩阵的前500个主成分，当我们读取一副待识别的图片后，就将其投影到这个低维的空间中，并使用某种度量法则对其进行类别判定，最终完成识别功能。</p>

<p>有趣（但并不令人吃惊）的是，如果我们将这些主成分（在这个例子中，即一个65536维的奇异向量）重新还原为图片时，这幅图片像极了一副人脸。所以在模式识别中，这种算法又称为特征脸（Eigenface）<br/>
。作为本文的结束，下图展示了一个特征脸的例子，样本总数为4，选取其前4个主成分并恢复成图像，样本中的人脸采集自我的本科时的同学们。</p>

<p><img src="media/15084626290253/1.png" alt="1"/></p>

<h2 id="toc_5">参考文献</h2>

<blockquote>
<p>Strang, G., Introduction to linear algebra. 2003: Wellesley Cambridge Pr.<br/>
Lawson, C.L. and R.J. Hanson, Solving least squares problems. Vol. 15. 1995: SIAM.<br/>
Turk, M. and A. Pentland, Eigenfaces for recognition. Journal of cognitive neuroscience, 1991. 3(1): p. 71--86.<br/>
黄廷祝, 钟守铭与李正良, 矩阵理论. 2003.</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Flask，从简单开始]]></title>
    <link href="http://heleifz.github.io/15013781349463.html"/>
    <updated>2017-07-30T09:28:54+08:00</updated>
    <id>http://heleifz.github.io/15013781349463.html</id>
    <content type="html"><![CDATA[
<p>前段时间用 Flask 搭建了一个公司内部用的算法工具，对于这类工具，我对 Web 框架只有两个要求：</p>

<ol>
<li>框架好理解，学习成本低</li>
<li>不要写多余的代码</li>
</ol>

<p>从这两点可以看出，我不仅是个精神上懒惰的人，而且是个身体上懒惰的人。PHP 我是不想再碰了，它提供的“效率“是以语言设计的灾难为代价的。</p>

<p>Python 用着还行，所以我选了轻量级框架 Flask，从结果来看，它符合我上面的两个要求，且最终效果令人满意。作为项目的总结，本文就梳理一下如何用 Flask 开发 Web 应用，从最简单的开始。</p>

<h2 id="toc_0">Python Web 应用是什么</h2>

<p>一个 Python Web 应用包含两个部分：</p>

<ol>
<li>应用的开发：实现 Web 应用的逻辑，读数据库，拼装页面，用户登录</li>
<li>应用的部署：将 Web 应用跑起来，多线程，多进程，异步，监听端口，所有服务器该做的事情</li>
</ol>

<p>我们希望用各种不同的技术来开发应用，用各种不同的服务器程序来跑应用，这就要求开发和部署都遵循一个统一的 <strong>通信接口</strong>，这个接口叫做 Web Server Gateway Interface，简称“WSGI”。</p>

<p>WSGI 接口就是一个满足特定要求的函数</p>

<pre><code class="language-python">def application(environ, start_response):
    “”“
    environ：包含所有 HTTP 请求信息的 dict
    start_response：发送 HTTP 响应的函数
    ”“”
    start_response(&#39;200 OK&#39;, [(&#39;Content-Type&#39;, &#39;text/html&#39;)])
    return &#39;&lt;h1&gt;Hello, web!&lt;/h1&gt;&#39;
</code></pre>

<p>只要提供了这个接口，服务端程序就能将它跑起来，不需要关心应用代码如何组织，采用何种框架。<br/>
关于 WSGI 接口，可以参考以下三篇文章：</p>

<blockquote>
<p>WSGI 完整说明：<a href="https://www.python.org/dev/peps/pep-3333/">PEP 333 : Python Web Server Gateway Interface v1.0.1</a><br/>
WSGI 简单说明 <a href="https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386832689740b04430a98f614b6da89da2157ea3efe2000">廖雪峰的官方网站</a><br/>
Python Web 应用概览：<a href="http://pythonguidecn.readthedocs.io/zh/latest/scenarios/web.html">The Hitchhiker&#39;s Guide To Python</a></p>
</blockquote>

<h2 id="toc_1">Flask Web 应用开发</h2>

<p>本节介绍 Flask Web 框架的使用。</p>

<h3 id="toc_2">基本形态：URL 路由，获取请求，构造响应</h3>

<p><a href="http://flask.pocoo.org/">Flask</a> 作为一个 Web 框架，它<strong>遵循 WSGI 接口</strong>，并且在其之上<strong>整合</strong>了 <a href="http://werkzeug.pocoo.org/">Werkzeug</a> 的 URL 路由以及 WSGI 工具库，<a href="https://www.sqlalchemy.org/">SQLAlchemy</a> 的数据库访问，<a href="http://jinja.pocoo.org/docs/2.9/">jinja2</a> 的模版渲染等功能。</p>

<p>但它也是一个轻量级框架，Flask 应用最基本的形态只涉及 URL 路由，不需要配置任何东西就能工作。</p>

<pre><code class="language-python">from flask import Flask
app = Flask(__name__)

@app.route(&quot;/&quot;)
def hello():
    return &quot;Hello World!&quot;
</code></pre>

<blockquote>
<p>Flask 初始化使用的第一个参数的含义是 “import path”，代表当前模块的 import 名称，该参数的作用是 <strong>帮助 Flask 自动找到当前文件的绝对路径</strong> ，从而确定项目中各个资源文件的路径。很多 Web 框架都有这么一个 Root Path 参数，只不过 Flask 做得更漂亮。</p>
</blockquote>

<p><code>app</code> 对象实现了 WSGI 接口，我们现在看看这个怎么实现的。打开 Flask 类的代码，地址 <a href="https://github.com/pallets/flask/blob/master/flask/app.py">https://github.com/pallets/flask/blob/master/flask/app.py</a> ：</p>

<pre><code class="language-python">def wsgi_app(self, environ, start_response):
   # 构造请求上下文
   ctx = self.request_context(environ)
   error = None
   try:
       try:
           # 将请求上下文推入栈中，这部分后续会解释
           ctx.push()
           # URL 路由，产生结果，构造 response
           response = self.full_dispatch_request()
       except Exception as e:
           error = e
           response = self.handle_exception(e)
       except:
           error = sys.exc_info()[1]
           raise
       return response(environ, start_response)
   finally:
       if self.should_ignore_error(error):
           error = None
       ctx.auto_pop(error)

def __call__(self, environ, start_response):
   &quot;&quot;&quot;Shortcut for :attr:`wsgi_app`.&quot;&quot;&quot;
   return self.wsgi_app(environ, start_response)
</code></pre>

<p>我们发现，Flask 类实现了 <code>__call__</code> 魔术方法，所以能像函数一样使用，并且符合 WSGI 规范。实际的逻辑在 <code>wsgi_app</code> 函数中，基本上所有 Flask 的框架功能都是从这个函数中延伸出去的。此外，我们通过<strong>函数装饰器</strong>，往 <code>app</code> 里注册从 URL 到处理逻辑的映射，而 URL 路由使用了 <code>Werkzeug</code> 库的 <code>werkzeug.routing.Map</code> 类，大家可以在同一个文件中搜索并阅读。</p>

<blockquote>
<p>现在，我们使用 Flask 内置的 Debug 服务器，让应用跑起来，假设文件名叫 <code>hello.py</code>。执行 <code>FLASK_APP=hello.py flask run</code>，接着，访问 <code>http://localhost:5000/</code> 就能看到 “Hello World!”的输出了。</p>
</blockquote>

<p>现在，我们给 URL 路径带上参数，把它放到 <code>hello2</code> 函数里：</p>

<pre><code class="language-python">@app.route(&quot;/hello2/&lt;name&gt;/&lt;age&gt;/&quot;)
def hello2(name, age):
    return &quot;Hello! &quot; + name + &quot;, you are &quot; + age + &quot; years old.&quot;
</code></pre>

<p>通常我们需要访问请求的 GET 或 POST 参数，此时，使用 Flask 的 request 对象。注意到它是一个“全局对象”，但它始终包含着<strong>当前请求</strong>的所有信息，这种全局对象的设计让 Flask 写起来十分简单顺手。</p>

<pre><code class="language-python">from flask import request

@app.route(&quot;/hello3/&quot;, methods=[&#39;POST&#39;, &#39;GET&#39;])
def hello3():
    # 获取 get 参数
    param1 = request.args.get(&#39;param1&#39;)
    # 获取 post 参数
    param2 = request.request.form[&#39;param2&#39;]
</code></pre>

<p>到这一步，你已经可以用 Flask 实现一个 Web Service 了，简单直接，当然很多有用的知识点我没写，例如 <code>after_request</code> 装饰器等，你可以在 <strong><a href="http://flask.pocoo.org/snippets/">http://flask.pocoo.org/snippets/</a></strong> 里找到很多有用的代码段！当你需要更复杂的功能时，可以慢慢往里添加，这就是 Flask 的口号 <strong>“one drop at a time“</strong> 的含义，一点一滴地往应用里增加功能。</p>

<blockquote>
<p>Flask 很简单，但它最难以理解的莫过于 Context 机制，个人觉得有点过度设计，日常使用的话无需学习它，但如果你一定要了解，可以看看下面的资料：<br/>
<a href="https://blog.tonyseek.com/post/the-context-mechanism-of-flask/">Flask 的 Context 机制</a><br/>
Flask 作者的一个 Talk，“Flask for Fun and Profit” 里面也讲了 Context 机制 <a href="https://www.youtube.com/watch?v=1ByQhAM5c1I&amp;index=24&amp;list=WL">Youtube 链接</a><br/>
Stackoverflow 上的一个问题，<a href="https://stackoverflow.com/questions/20036520/what-is-the-purpose-of-flasks-context-stacks">为什么 Flask 要用 Context Stack</a></p>
</blockquote>

<h3 id="toc_3">拼装页面：jinja2 模版</h3>

<p>很多时候你要的不止是一个 Web Service，而是一个网站，这就需要在代码中拼装出一个 HTML 页面。 Flask 默认使用 <a href="http://docs.jinkan.org/docs/jinja2/">jinja2</a> 作为<a href="https://en.wikipedia.org/wiki/Comparison_of_web_template_engines">模版引擎</a> 帮助我们生成网页。</p>

<p>基本每个模版引擎都包含了一个<strong>模版语言</strong>，这部分内容请自行参考 jinja2 的文档，这里提一下它与 Flask 相关的部分。</p>

<h4 id="toc_4">使用方式</h4>

<p>首先在项目目录中新建 <code>templates</code> 目录，在其中建立一个 <code>hello.html</code> 文件，内容为</p>

<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
  &lt;head&gt;
    &lt;title&gt;Flask Template&lt;/title&gt;
    &lt;meta charset=&quot;utf-8&quot;&gt;
  &lt;/head&gt;
  &lt;body&gt; 
    &lt;p&gt;{{ greetings }}&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt; 
</code></pre>

<p>接着，使用 Flask 提供的 <code>render_template</code>函数渲染模版，并将渲染结果返回。</p>

<pre><code class="language-python">
from flask import render_template

@app.route(&#39;/hello4&#39;)
def hello4():
    # 将数据传给模版 hello.html
    return render_template(&quot;hello.html&quot;, greetings=&quot;Hello world!&quot;)
</code></pre>

<h4 id="toc_5">模版搜索路径</h4>

<p>大家注意到 <code>templates</code> 是 Flask 默认搜索模版的路径，如果你想修改这个路径，可以在初始化 Flask（或者 Blueprint，后面会提到）对象时，指定自己的路径</p>

<pre><code class="language-python">app = Flask(__name__, template_folder=&#39;my_templates&#39;) 
</code></pre>

<blockquote>
<p><a href="http://docs.jinkan.org/docs/jinja2/templates.html">Jinja2 模版语言文档</a></p>
</blockquote>

<h3 id="toc_6">组织更加复杂的应用：蓝图</h3>

<p>现实中的项目往往带有多个模块，尽管模块之间总是要共享一些页面布局和资源，它们在概念上基本“相互独立”。如果把代码、模版全部揉在一起，整个项目结构混乱，难以维护。Flask 提供了一种组织项目的思路：蓝图（Blueprint）。</p>

<p>蓝图与 Flask App 类似，它可以注册自己的模版目录，静态文件目录，URL 路由，装饰器钩子（例如 <code>before_request</code>)。蓝图上注册的所有东西需要被 <code>app.register_blueprint</code> 函数统一注册到全局 app 对象中才能生效。此外，蓝图还可以拥有自己的 URL 前缀用以区分模块。如下图所示：</p>

<p><img src="media/15013781349463/blueprint.png" alt="blueprint"/></p>

<p>有了蓝图后，我们可以按照下面的结构组织项目</p>

<pre><code>app/
  mod_users/  -- user 模块
    templates/
      index.html
    __init__.py
    users.py
  mod_camera/  -- camera 模块
    templates/
      index.html
    camera.py
    __init__.py
  templates/
    base.html  --  项目基础模版
  app.py -- 项目主文件，注册以上所有蓝图
  static／ -- 项目静态资源
</code></pre>

<p>用过其它传统 Web 框架的朋友可能会将蓝图与所谓的 “MVC架构”做对比，实际上这两者是没有任何关系的。<strong>蓝图并不算一种架构模式</strong>，它存在的意义是帮助项目更好地组织文件，如果你愿意，可以在蓝图内部开发一个自己的 MVC 框架。</p>

<p>总而言之，Flask 没有将任何一种设计模式强加于用户，只在用户需要的时候提供帮助，这也是我喜欢 Flask 的一个原因  ：）。</p>

<blockquote>
<p>阅读以下文章，完全掌握蓝图<br/>
<a href="http://flask.pocoo.org/docs/0.12/blueprints/">蓝图：官方文档</a><br/>
<a href="https://www.digitalocean.com/community/tutorials/how-to-structure-large-flask-applications">如何组织大型 Flask 应用</a><br/>
<a href="http://fewstreet.com/2015/01/16/flask-blueprint-templates.html">绕过蓝图的坑：同名 template</a></p>
</blockquote>

<h2 id="toc_7">使用 gunicorn 部署 Python Web 应用</h2>

<p>应用开发完毕后，就可以部署上线了，任何支持 WSGI 的 Web 容器都能运行 Flask，包括：uWSGI，gnicorn 等，具体参考后面的文档。这里，我们使用 <a href="http://gunicorn.org/">gunicorn</a> 来部署应用：</p>

<pre><code class="language-bash">pip install gunicorn
gunicorn -w 4 -b 127.0.0.1:4000 hello:app
</code></pre>

<p>既然是简单的 Web 开发框架，当然要选最轻松的部署方式啦，反正我是个懒人。</p>

<blockquote>
<p>其它参考资料：<br/>
<a href="http://flask.pocoo.org/docs/0.12/deploying/">部署 Flask 应用</a><br/>
<a href="http://flask-sqlalchemy.pocoo.org/2.3/">Flask-SQLAlchemy 文档</a><br/>
<a href="http://www.pythondoc.com/flask-mega-tutorial/">Flask Mega Tutorial</a></p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何学习新技术]]></title>
    <link href="http://heleifz.github.io/14953815607558.html"/>
    <updated>2017-05-21T23:46:00+08:00</updated>
    <id>http://heleifz.github.io/14953815607558.html</id>
    <content type="html"><![CDATA[
<p>我从 2009年～2010 年左右开始学习信息技术，从初期围绕单片机做电子设计，一路浅尝辄止，走马观花。什么语言都用过，在各种平台上都干过活，有用的没用的都学了一些，一路尝鲜一路丢。</p>

<p>虽然有识之士们都说，光靠广度没法找到工作，要对一门手艺有深度，到达深奥幽玄的境界。我自知能力有限，更何况，工作后每日加班，精力不济，能让我一窥天道的时间窗口越来越窄了。</p>

<p>即便如此，我也不觉得自己不行，这就是愚蠢人的乐趣。因为总有与你同样愚蠢的人，他们很快乐，我为什么不能快乐。我不仅快乐，而且要分享，要写作，本文就谈谈在下走马观花的本领。</p>

<h2 id="toc_0">兴味使然</h2>

<p>首先要解决驱动力的问题，即便走马观花，心里也得有股劲，是不是？所以你得喜欢编程这门手艺，不喜欢，那至少也得好奇，当个好奇宝宝。既不喜欢，也无好奇心，自主学习就没法成立。</p>

<p>既然喜欢这门手艺，热爱这个行业，那么你自然有感兴趣的领域，或者小问题，这就是学习新技术前要做的第一件事，确定一个你感兴趣的方向。</p>

<p>有的朋友爱流行技术，因为网民们在学，所以他也想学。或者就业市场需要某个技能，所以被迫去学。这当然好啦，针对性地优化你的简历，在求职时候有用，但也仅限于此了。</p>

<p>兴味使然的学习完全不同，你能用更加轻松的心态看待每个知识点，也能在适当的时候深入挖掘一些现象的背后原因，这是一个充满快感的旅程，没有人提醒你：那里不是考点。</p>

<h2 id="toc_1">探寻入口</h2>

<p>选择了方向以后，就该选学习材料了。随处可见的大型书单其实用处有限，因为材料的准备是分阶段的，侧重点不同的，这个过程伴随着你对该领域的初步认识的建立，混杂着你的直觉，常识，以及一点点随机性。</p>

<p>首先，做一些”自由探索”，闭上眼睛，听听伟大的互联网在你耳边的低语：Google，维基百科，问答社区。从庞杂的噪声中，你需要找到以下知识点：</p>

<ol>
<li>基本定义：例如，机器学习是什么，不是什么？与相关概念（人工智能）之间的区别和联系。</li>
<li>主要工具：例如，机器学习的理论工具是什么，实践方式是什么？</li>
<li>核心问题或矛盾：例如，机器学习研究或应用中面临的基本问题是什么？</li>
</ol>

<p>同时，也要了解几个主要的资料源（每个资料源不一定都适合你），例如：</p>

<ol>
<li>coursera/edx 公开课</li>
<li>大部头教材英文版，中文版</li>
<li>由网友编写的野鸡教程</li>
</ol>

<p>基本概念的探寻，学习资料的搜索，两个过程并不相互独立。基本概念的逐步建立，能帮助你更高效地筛选资料，而好的学习资料往往会花相当的篇幅来阐明基本概念。</p>

<p>不专业的作者，要么开篇就贴代码，深入细节，要么对某项技术过渡吹捧，盲目狂热。好的学习材料也许轻松幽默，也许精确冷峻，但绝对不愚蠢。请务必花时间浏览各个教材前 5% 的内容，慢慢甄别优劣。</p>

<p>此外，好的学习资料，能够包含适当的实践内容。无论是数学题还是编程题，能否找到参考答案，是否提供代码实验环境，都是很重要的因素。</p>

<p>举几个例子，<a href="https://mitpress.mit.edu/sicp/">SICP</a>，每道题都能在 Racket 上做实验，网上也能找到<a href="http://community.schemewiki.org/?SICP-Solutions">大家的解题方法</a>；斯坦福 <a href="http://web.stanford.edu/class/cs224n/">CS224N</a>课程，完整的习题解答和项目环境；<a href="The%20Elements%20of%20Computing%20Systems%20_%20Nisan%20&amp;%20Schocken">Nand2tetris</a>，甚至有一整套硬件描述语言，虚拟机的环境，非常优秀。也有不好的例子，<a href="https://www.cs.ubc.ca/%7Emurphyk/MLbook/">MLAPP</a>，整本书都是印刷错误，习题无解答，全靠个人感觉，虽说是本好书，但阅读过程中我不停地做一些人工勘误的工作，非常痛苦，不适合入门阅读。</p>

<p>最后，很重要的一点，这个资料是否适合你当前的知识水平。太简单，以至于整本书都在印证你的想法，让你觉得它”很好读”；太难，你无法通过直觉跳过某些推导步骤，每一页的阅读都需要大量的思考，哪怕看完一整章，你也没有对这个章节的完整图景。</p>

<p>合适的难度，就是让你兴奋的难度，你能从中找到你过往的一些经验，也能看到很多全新的方法和观念，学完一章后，是否有一些愉快的想象，如果有，那么它的难度刚刚好。</p>

<h2 id="toc_2">短期专注</h2>

<p>找到资料后，不要浪费它。快速，完整地把它消化掉，不要中断，每天至少两个小时以上的专注，周末也不要停，这是我的方法。</p>

<p>有些朋友喜欢同时做几件重要的事，然后在事情之间做时间片轮转，先做做A，再做做B，这种方式有助于应对给领导长辈跑腿之类的琐事，但对学习来说效率较低。</p>

<p>在短时间内全身心沉入到单一的问题中，在这段时间里，连潜意识都在帮你工作，你不仅在记忆知识点，更是在构建对这个领域的基本直觉。混合多种学习方式也很重要，不仅有被动的听和阅读，也有主动的实验，试错，推导。前文所说的资料准备，正是为了服务这个近乎宗教仪式的过程，如果选不对资料，你就没法找到这条灵性之路。</p>

<h2 id="toc_3">自然遗忘</h2>

<p>人类会遗忘，这件事情曾让我很难过。</p>

<p>半年不接触一门手艺，就会生疏，再过半年，完全遗忘。我想把所有的理论，公式，语言，框架都记住，一有生疏，我就复习，就像抱着满满一筐苹果，一路丢，一路捡。心里疲惫，身体劳累，甚至觉得，如果最终都会遗忘，那一开始为什么要学。</p>

<p>后来才想明白，苹果框满了，我们就倒掉，再来一筐梨，桃子，火龙果，葡萄。对于学习新知识的方法论，逐年在磨练精进，虽然知识点没法记住，但对于该领域的直觉已经建立。</p>

<p>现在我把学习新知识，当时一次次旅行，其中的滋味，留给大家各自体会。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[神经网络计算模型 - 理论解释]]></title>
    <link href="http://heleifz.github.io/14696391071598.html"/>
    <updated>2016-07-28T01:05:07+08:00</updated>
    <id>http://heleifz.github.io/14696391071598.html</id>
    <content type="html"><![CDATA[
<p>传统机器学习教科书中的神经网络通常是简单的多层感知机，采用全联接层作为隐层，并经过一个 softmax 输出，现代的神经网络架构脱胎于此，却早已脱离这样的简单模型，无论是 <a href="http://caffe.berkeleyvision.org">Caffe</a> 还是 <a href="http://www.deeplearning.net/software/theano/">Theano</a>，都具有可定制，可扩展的优点，允许用户自行搭建符合需求的网络架构和运算。</p>

<p>本文从 <em>数据的抽象</em>，<em>运算的抽象</em> 两个角度介绍现代神经网络的计算模型。</p>

<h2 id="toc_0">用 Tensor 抽象数据</h2>

<p>传统机器学习模型严重依赖于 <a href="http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf">feature engineering</a>，所以模型的输入一般是计算好的特征向量，但是基于神经网络的机器学习系统利用中间层自动得到合适的特征，所以数据往往以更为原始而稠密的形式输入网络（图像，音视频），并且在网络内部保持这种 N 维数组的结构向后传播，这种 N 维数组也叫 tensor。</p>

<p>采用 tensor 或向量在计算上并无本质区别，将 tensor 拉伸就变成向量，如下图所示，假设输入标量为 \(z\) ，相对于 tensor \(X\) 计算梯度时<br/>
\[<br/>
\frac {\partial z}{\partial X_{i,j,k} } = \frac {\partial z}{\partial 向量化(X)_n }<br/>
\]<br/>
其中 \(n\) 为 \( X_{i,j,k} \) 在 \( X \) 向量化后所在的位置。Tensor 的好处是降低了用户的思维负担，保持了数据的原始结构，目前所有的神经网络工具都使用 tensor 来存储数据。</p>

<p><img src="media/14696391071598/tensor.png" alt="tenso" style="width:513px;"/></p>

<p>使用向量的好处是方便写公式，后文中 \( X_i \) 表示 tensor \( X \) 以某种方式拉伸成向量后的第 \( i \) 个元素。</p>

<h2 id="toc_1">由运算构成网络</h2>

<p>运算（operation）就是函数，一个运算接收一个或多个 tensor 作为输入，产生一个 tensor 作为输出，不同的运算的组合成完整的神经网络。</p>

<p><img src="media/14696391071598/operator.png" alt="operato"/></p>

<p>下面列举几种常见的运算，默认所有大写字母都代表 tensor：</p>

<h4 id="toc_2">1. 矩阵乘法（全联接层）</h4>

<p><strong>输入</strong>: 数据 \(X\)，参数 \(W\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 运算时将 \(X\) 拉伸为向量，假设长度为 \(n\)，输出 \(Y\) 长度为 \(m\)，那么 \(W\) 矩阵的大小为 \(m\)x\(n\)，且 \(Y=WX\)。</p>

<h4 id="toc_3">2. 卷积</h4>

<p><strong>输入</strong>: 数据 \(X\)，卷积核 \(H\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 运算时将 \(X\) 与 \(H\) 做卷积（神经网络意义下的卷积，不翻转 \(H\)），假设 \(X\) 的长宽为 \(h\)x\(w\), \(H\) 的长宽为 \(m\)x\(n\)，那么输出 \(Y\) 的长宽为 \((h-m+1)\)x\((w-n+1)\)。</p>

<h4 id="toc_4">3. 非线性变换</h4>

<p><strong>输入</strong>: 数据 \(X\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 将 \(X\) 逐元素做非线性变换（例如 sigmoid，ReLu等），输出到 \(Y\)，\(Y\)与\(X\) 形状一致。</p>

<h4 id="toc_5">4. 平方误差</h4>

<p><strong>输入</strong>: 模型输出 \(f\)，训练数据 label \(y\)<br/>
<strong>输出</strong>: \(Loss\)<br/>
<strong>说明</strong>: \(Loss=0.5*(y-f)^2 \)</p>

<h3 id="toc_6">5. 内积</h3>

<p><strong>输入</strong>: 数据 \(X\)，数据 \(Y\)<br/>
<strong>输出</strong>: \(dot(X,Y)\)<br/>
<strong>说明</strong>: \(dot(X,Y)\) 为两路输入的逐元素相乘后求和，是一个标量。</p>

<p>利用以上运算节点以及基本的算术运算，我们就可以像 <strong>搭积木</strong> 一样，构建一个经典的3层神经网络，绿色为输入，黄色为模型参数，为节约空间，运算之间的 tensor 节点被我省去了。注意到，图中我包含了一个L2-正则项。</p>

<p><img src="media/14696391071598/model.png" alt="mode"/></p>

<p>构造完运算图后，我们只需要在网络的输入端提供 tensor \(X\)，并让 \(X\) 随着网络流动 (flow)到输出层即可，我想这就是 <a href="https://www.tensorflow.org">TensorFlow</a> 这个库的命名的由来。</p>

<h2 id="toc_7">使用 backprop 计算梯度</h2>

<p>神经网络相较其它模型的优势之一，就是能够使用 backprop 算法自动且高效地计算出梯度，它本质上是一种 <strong>动态规划</strong>，遵循推导动态规划算法的一般套路，我们首先在运算图中定义梯度计算的递归关系。</p>

<h4 id="toc_8">1. 计算目标</h4>

<p>假设模型的损失函数的输出是一个标量 \(z\)（大部分情况下如此），针对我们感兴趣的参数 tensor \(W_i\) 我们想要得到它们相对于 \(z\) 的梯度<br/>
\[<br/>
\frac {\partial z}{\partial W_1 },\frac {\partial z}{\partial W_2 } ...<br/>
\]</p>

<h4 id="toc_9">2. 递归关系</h4>

<p>运算图中的梯度具有递归关系，对于下图：<br/>
<img src="media/14696391071598/simple_net.png" alt="simple_net"/></p>

<p>利用导数的链式法则，假设 \(C\) 的长度为 \(m\)，\(A\) 的长度为 \(n\)，\(z\) 对于 \(A\) 的每个元素的导数为：<br/>
\[<br/>
\frac {\partial z}{\partial A_i }＝\Sigma_{j=1..m}\frac {\partial z}{\partial C_i } \frac{\partial C_j}{\partial A_i }<br/>
\]<br/>
将 \(A_i\) 组合成 tensor，得到：<br/>
\[<br/>
\frac {\partial z}{\partial A }＝\Sigma_{j=1..m}\frac {\partial z}{\partial C_j } \frac{\partial C_j}{\partial A}<br/>
\]<br/>
其中，\(\frac {\partial z}{\partial A }\) 是 tensor 拉伸成的向量，向量的元素是 \(\frac {\partial z}{\partial A_{i=1..n} }\)。接着，使用矩阵乘法将求和符号省略，我们得到：<br/>
\[<br/>
\frac {\partial z}{\partial A }＝(\frac{\partial C}{\partial A})^T \frac {\partial z}{\partial C } <br/>
\]<br/>
其中，我们利用了 <a href="http://mathworld.wolfram.com/Jacobian.html">雅可比矩阵</a><br/>
\[<br/>
\frac{\partial C}{\partial A}=<br/>
\begin{bmatrix}<br/>
  \frac{\partial C_1}{\partial A_1} &amp; \frac{\partial C_1}{\partial A_2} &amp; ...\\<br/>
  \frac{\partial C_2}{\partial A_1} &amp; \frac{\partial C_2}{\partial A_2} &amp; ...\\<br/>
  ... &amp; ... &amp; ...<br/>
 \end{bmatrix}<br/>
\]<br/>
以上推导说明，网络前级的梯度可以由后级的梯度乘以一个雅可比矩阵得到，这个结论适用于 <strong>任意</strong> 的运算节点，这给神经网络的软件架构带来了极大的灵活性：对于用户定义的任何运算，只要正确实现了这个雅可比矩阵的乘法，就能加入到梯度计算中来，这也是 Caffe 库中的每种 Layer 只要定义 Forward 和 Backward 接口就能参与网络构建的原因。</p>

<p>当然，一个 tensor 可以被一个以上的后续节点使用，如下图：<br/>
<img src="media/14696391071598/simple_net_2.png" alt="simple_net_2"/><br/>
根据导数的性质，我们有：<br/>
\[<br/>
\frac {\partial z}{\partial C_i }＝\Sigma_{j=1..m}\frac {\partial z}{\partial A_j } \frac{\partial A_j}{\partial C_i } + \Sigma_{y=1..k}\frac {\partial z}{\partial B_y } \frac{\partial B_y}{\partial C_i }<br/>
\]<br/>
类似于上面的推导，我们得到矩阵形式的递归公式：<br/>
\[<br/>
\frac {\partial z}{\partial C }＝(\frac{\partial A}{\partial C })^T \frac {\partial z} {\partial A }  + (\frac{\partial B }{\partial C })^T \frac {\partial z}{\partial B } <br/>
\]<br/>
简而言之，从两路流过来的梯度需要加起来，得到 \(C\) 节点的梯度。</p>

<h4 id="toc_10">3. 边界情况</h4>

<p>网络的最后级节点就是在输出端 \(z\)，边界情况十分简单：<br/>
\[<br/>
\frac {\partial z}{\partial z } = 1<br/>
\]</p>

<h4 id="toc_11">4. backprop</h4>

<p>定义了递归关系和边界情况后，我们可以发现大量冗余的计算：所有流向一个节点 \(A\) 的节点，总是需要计算一遍 \(\frac {\partial z}{\partial A } \)。此时我们有两种选择，一种是将计算后的结果缓存到一张查找表里，避免重复计算，也可以调整计算顺序，从网络的末端向前计算，这就是大部分动态规划采用的 bottom-up 策略，在神经网络中，这称作反向传播（backprop）</p>

<h3 id="toc_12">常见运算节点的 backprop</h3>

<p>上面提到，无论进行什么运算，backprop 总是雅可比矩阵的转置乘以后级的梯度，但是，对于某些运算来说，这个矩阵乘法可以表现为不同的形式。</p>

<h4 id="toc_13">1. 矩阵乘法（全联接层）</h4>

<p><strong>输入</strong>: 数据 \(X\)，参数 \(W\)<br/>
<strong>输出</strong>: \(Y＝WX\)<br/>
<strong>backprop</strong>: 很容易得到<br/>
\[\frac {\partial z}{\partial X }＝W^T \frac {\partial z}{\partial Y }\\<br/>
\frac {\partial z}{\partial W }＝\frac {\partial z}{\partial Y } X^T \]</p>

<h4 id="toc_14">2. 非线性变换</h4>

<p><strong>输入</strong>: 数据 \(X\)<br/>
<strong>输出</strong>: \(Y=nonlinear(X)\)<br/>
<strong>backprop</strong>: 将 \(X\) 逐元素做非线性变换（例如 sigmoid，ReLu等），输出到 \(Y\)，\(Y\)与\(X\) 形状一致，所以雅可比矩阵是一个对角矩阵，矩阵乘法退化成逐元素的乘法。</p>

<h4 id="toc_15">3. 卷积</h4>

<p>比较麻烦，留作练习吧，推导时需要草稿纸画一画。在计算雅可比矩阵时，你会发现每一行都是卷积权重加上一个位移，所以最终的矩阵乘法，会等价于卷积。</p>

<p>用矩阵乘法来等价卷积，是一种常用的卷积实现方法，因为许多数值计算库没有实现卷积，但一定有高效的矩阵乘法。</p>

<h2 id="toc_16">包含圈的计算图：RNN</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[fastText 源码分析]]></title>
    <link href="http://heleifz.github.io/14732610572844.html"/>
    <updated>2016-09-07T23:10:57+08:00</updated>
    <id>http://heleifz.github.io/14732610572844.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">介绍</h2>

<p><a href="https://github.com/facebookresearch/fastText">fastText</a> 是 facebook 近期开源的一个词向量计算以及文本分类工具，该工具的理论基础是以下两篇论文：</p>

<blockquote>
<p><a href="https://arxiv.org/pdf/1607.04606v1.pdf">Enriching Word Vectors with Subword Information</a></p>
</blockquote>

<p>这篇论文提出了用 word n-gram 的向量之和来代替简单的词向量的方法，以解决简单 word2vec 无法处理同一词的不同形态的问题。fastText 中提供了 maxn 这个参数来确定 word n-gram 的 n 的大小。</p>

<blockquote>
<p><a href="https://arxiv.org/pdf/1607.01759v2.pdf">Bag of Tricks for Efficient Text Classification</a></p>
</blockquote>

<p>这篇论文提出了 fastText 算法，该算法实际上是将目前用来算 word2vec 的网络架构做了个小修改，原先使用一个词的上下文的所有词向量之和来预测词本身（CBOW 模型），现在改为用一段短文本的词向量之和来对文本进行分类。</p>

<p>在我看来，fastText 的价值是提供了一个 <strong>更具可读性，模块化程度较好</strong> 的 word2vec 的实现，附带一些新的分类功能，本文详细分析它的源码。</p>

<h2 id="toc_1">顶层结构</h2>

<p>fastText 的代码结构以及各模块的功能如下图所示：</p>

<p><img src="media/14732610572844/fasttext-arch.png" alt="fasttext-arch"/></p>

<p>分析各模块时，我只会解释该模块的 <strong>主要调用路径</strong> 下的源码，以 <strong>注释</strong> 的方式说明，其它的功能性代码请大家自行阅读。如果对 word2vec 的理论和相关术语不了解，请先阅读这篇 <a href="http://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a>。</p>

<h2 id="toc_2">训练数据格式</h2>

<p>训练数据格式为一行一个句子，每个词用空格分割，如果一个词带有前缀“<code>__label__</code>”，那么它就作为一个类标签，在文本分类时使用，这个前缀可以通过<code>-label</code>参数自定义。训练文件支持 UTF-8 格式。</p>

<h2 id="toc_3">fasttext 模块</h2>

<p>fasttext 是最顶层的模块，它的主要功能是<code>训练</code>和<code>预测</code>，首先是<code>训练</code>功能的调用路径，第一个函数是 <code>train</code>，它的主要作用是 <strong>初始化参数，启动多线程训练</strong>，请大家留意源码中的相关部分。</p>

<pre><code class="language-cpp">void FastText::train(std::shared_ptr&lt;Args&gt; args) {
  args_ = args;
  dict_ = std::make_shared&lt;Dictionary&gt;(args_);
  std::ifstream ifs(args_-&gt;input);
  if (!ifs.is_open()) {
    std::cerr &lt;&lt; &quot;Input file cannot be opened!&quot; &lt;&lt; std::endl;
    exit(EXIT_FAILURE);
  }
  // 根据输入文件初始化词典
  dict_-&gt;readFromFile(ifs);
  ifs.close();

   // 初始化输入层, 对于普通 word2vec，输入层就是一个词向量的查找表，
   // 所以它的大小为 nwords 行，dim 列（dim 为词向量的长度），但是 fastText 用了
   // word n-gram 作为输入，所以输入矩阵的大小为 (nwords + ngram 种类) * dim
   // 代码中，所有 word n-gram 都被 hash 到固定数目的 bucket 中，所以输入矩阵的大小为
   // (nwords + bucket 个数) * dim
  input_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nwords()+args_-&gt;bucket, args_-&gt;dim);
  
  // 初始化输出层，输出层无论是用负采样，层次 softmax，还是普通 softmax，
  // 对于每种可能的输出，都有一个 dim 维的参数向量与之对应
  // 当 args_-&gt;model == model_name::sup 时，训练分类器，
  // 所以输出的种类是标签总数 dict_-&gt;nlabels()
  if (args_-&gt;model == model_name::sup) {
    output_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nlabels(), args_-&gt;dim);
  } else {
  // 否则训练的是词向量，输出种类就是词的种类 dict_-&gt;nwords()
    output_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nwords(), args_-&gt;dim);
  }
  input_-&gt;uniform(1.0 / args_-&gt;dim);
  output_-&gt;zero();

  start = clock();
  tokenCount = 0;
  
  // 库采用 C++ 标准库的 thread 来实现多线程
  std::vector&lt;std::thread&gt; threads;
  for (int32_t i = 0; i &lt; args_-&gt;thread; i++) {
    // 实际的训练发生在 trainThread 中
    threads.push_back(std::thread([=]() { trainThread(i); }));
  }
  for (auto it = threads.begin(); it != threads.end(); ++it) {
    it-&gt;join();
  }
  
  // Model 的所有参数（input_, output_）是在初始化时由外界提供的，
  // 此时 input_ 和 output_ 已经处于训练结束的状态
  model_ = std::make_shared&lt;Model&gt;(input_, output_, args_, 0);

  saveModel();
  if (args_-&gt;model != model_name::sup) {
    saveVectors();
  }
}
</code></pre>

<p>下面，我们进入 <code>trainThread</code>函数，看看训练的主体逻辑，该函数的主要工作是 <strong>实现了标准的随机梯度下降</strong>，并随着训练的进行逐步降低学习率。</p>

<pre><code class="language-cpp">void FastText::trainThread(int32_t threadId) {

  std::ifstream ifs(args_-&gt;input);
  // 根据线程数，将训练文件按照总字节数（utils::size）均分成多个部分
  // 这么做的一个后果是，每一部分的第一个词有可能从中间被切断，
  // 这样的&quot;小噪音&quot;对于整体的训练结果无影响
  utils::seek(ifs, threadId * utils::size(ifs) / args_-&gt;thread);

  Model model(input_, output_, args_, threadId);
  if (args_-&gt;model == model_name::sup) {
    model.setTargetCounts(dict_-&gt;getCounts(entry_type::label));
  } else {
    model.setTargetCounts(dict_-&gt;getCounts(entry_type::word));
  }

  // 训练文件中的 token 总数
  const int64_t ntokens = dict_-&gt;ntokens();
  // 当前线程处理完毕的 token 总数
  int64_t localTokenCount = 0;
  std::vector&lt;int32_t&gt; line, labels;
  // tokenCount 为所有线程处理完毕的 token 总数
  // 当处理了 args_-&gt;epoch 遍所有 token 后，训练结束 
  while (tokenCount &lt; args_-&gt;epoch * ntokens) {
    // progress = 0 ~ 1，代表当前训练进程，随着训练的进行逐渐增大
    real progress = real(tokenCount) / (args_-&gt;epoch * ntokens);
    // 学习率根据 progress 线性下降
    real lr = args_-&gt;lr * (1.0 - progress);
    localTokenCount += dict_-&gt;getLine(ifs, line, labels, model.rng);
    // 根据训练需求的不同，这里用的更新策略也不同，它们分别是：
    // 1. 有监督学习（分类）
    if (args_-&gt;model == model_name::sup) {
      dict_-&gt;addNgrams(line, args_-&gt;wordNgrams);
      supervised(model, lr, line, labels);
    // 2. word2vec (CBOW)
    } else if (args_-&gt;model == model_name::cbow) {
      cbow(model, lr, line);
    // 3. word2vec (SKIPGRAM)
    } else if (args_-&gt;model == model_name::sg) {
      skipgram(model, lr, line);
    }
    // args_-&gt;lrUpdateRate 是每个线程学习率的变化率，默认为 100，
    // 它的作用是，每处理一定的行数，再更新全局的 tokenCount 变量，从而影响学习率
    if (localTokenCount &gt; args_-&gt;lrUpdateRate) {
      tokenCount += localTokenCount;
      // 每次更新 tokenCount 后，重置计数
      localTokenCount = 0;
      // 0 号线程负责将训练进度输出到屏幕
      if (threadId == 0) {
        printInfo(progress, model.getLoss());
      }
    }
  }
  if (threadId == 0) {
    printInfo(1.0, model.getLoss());
    std::cout &lt;&lt; std::endl;
  }
  ifs.close();
}
</code></pre>

<blockquote>
<p><strong>一哄而上的并行训练</strong>：每个训练线程在更新参数时并没有加锁，这会给参数更新带来一些噪音，但是不会影响最终的结果。无论是 google 的 word2vec 实现，还是 fastText 库，都没有加锁。</p>
</blockquote>

<p>从 <code>trainThread</code> 函数中我们发现，实际的模型更新策略发生在 <code>supervised</code>,<code>cbow</code>,<code>skipgram</code>三个函数中，这三个函数都调用同一个 <code>model.update</code> 函数来更新参数，这个函数属于 model 模块，但在这里我先简单介绍它，以方便大家理解代码。</p>

<p>update 函数的原型为</p>

<pre><code class="language-cpp">void Model::update(const std::vector&lt;int32_t&gt;&amp; input, int32_t target, real lr)
</code></pre>

<p>该函数有三个参数，分别是“输入”，“类标签”，“学习率”。</p>

<ul>
<li>输入是一个 <code>int32_t</code>数组，每个元素代表一个词在 dictionary 里的 ID。对于分类问题，这个数组代表输入的短文本，对于 word2vec，这个数组代表一个词的上下文。</li>
<li>类标签是一个 <code>int32_t</code> 变量。对于 word2vec 来说，它就是带预测的词的 ID，对于分类问题，它就是类的 label 在 dictionary 里的 ID。因为 label 和词在词表里一起存放，所以有统一的 ID 体系。</li>
</ul>

<p>下面，我们回到 fasttext 模块的三个更新函数：</p>

<pre><code class="language-cpp">void FastText::supervised(Model&amp; model, real lr,
                          const std::vector&lt;int32_t&gt;&amp; line,
                          const std::vector&lt;int32_t&gt;&amp; labels) {
  if (labels.size() == 0 || line.size() == 0) return;
  // 因为一个句子可以打上多个 label，但是 fastText 的架构实际上只有支持一个 label
  // 所以这里随机选择一个 label 来更新模型，这样做会让其它 label 被忽略
  // 所以 fastText 不太适合做多标签的分类
  std::uniform_int_distribution&lt;&gt; uniform(0, labels.size() - 1);
  int32_t i = uniform(model.rng);
  model.update(line, labels[i], lr);
}

void FastText::cbow(Model&amp; model, real lr,
                    const std::vector&lt;int32_t&gt;&amp; line) {
  std::vector&lt;int32_t&gt; bow;
  std::uniform_int_distribution&lt;&gt; uniform(1, args_-&gt;ws);
  
  // 在一个句子中，每个词可以进行一次 update
  for (int32_t w = 0; w &lt; line.size(); w++) {
    // 一个词的上下文长度是随机产生的
    int32_t boundary = uniform(model.rng);
    bow.clear();
    // 以当前词为中心，将左右 boundary 个词加入 input
    for (int32_t c = -boundary; c &lt;= boundary; c++) {
      // 当然，不能数组越界
      if (c != 0 &amp;&amp; w + c &gt;= 0 &amp;&amp; w + c &lt; line.size()) {
        // 实际被加入 input 的不止是词本身，还有词的 word n-gram
        const std::vector&lt;int32_t&gt;&amp; ngrams = dict_-&gt;getNgrams(line[w + c]);
        bow.insert(bow.end(), ngrams.cbegin(), ngrams.cend());
      }
    }
    // 完成一次 CBOW 更新
    model.update(bow, line[w], lr);
  }
}

void FastText::skipgram(Model&amp; model, real lr,
                        const std::vector&lt;int32_t&gt;&amp; line) {
  std::uniform_int_distribution&lt;&gt; uniform(1, args_-&gt;ws);
  for (int32_t w = 0; w &lt; line.size(); w++) {
    // 一个词的上下文长度是随机产生的
    int32_t boundary = uniform(model.rng);
    // 采用词+word n-gram 来预测这个词的上下文的所有的词
    const std::vector&lt;int32_t&gt;&amp; ngrams = dict_-&gt;getNgrams(line[w]);
    // 在 skipgram 中，对上下文的每一个词分别更新一次模型
    for (int32_t c = -boundary; c &lt;= boundary; c++) {
      if (c != 0 &amp;&amp; w + c &gt;= 0 &amp;&amp; w + c &lt; line.size()) {
        model.update(ngrams, line[w + c], lr);
      }
    }
  }
}

</code></pre>

<p>训练部分的代码已经分析完毕，预测部分的代码就简单多了，它的主要逻辑都在 <code>model.predict</code> 函数里。</p>

<pre><code class="language-cpp">void FastText::predict(const std::string&amp; filename, int32_t k, bool print_prob) {
  std::vector&lt;int32_t&gt; line, labels;
  std::ifstream ifs(filename);
  if (!ifs.is_open()) {
    std::cerr &lt;&lt; &quot;Test file cannot be opened!&quot; &lt;&lt; std::endl;
    exit(EXIT_FAILURE);
  }
  while (ifs.peek() != EOF) {
    // 读取输入文件的每一行
    dict_-&gt;getLine(ifs, line, labels, model_-&gt;rng);
    // 将一个词的 n-gram 加入词表，用于处理未登录词。（即便一个词不在词表里，我们也可以用它的 word n-gram 来预测一个结果）
    dict_-&gt;addNgrams(line, args_-&gt;wordNgrams);
    if (line.empty()) {
      std::cout &lt;&lt; &quot;n/a&quot; &lt;&lt; std::endl;
      continue;
    }
    std::vector&lt;std::pair&lt;real, int32_t&gt;&gt; predictions;
    // 调用 model 模块的预测接口，获取 k 个最可能的分类
    model_-&gt;predict(line, k, predictions);
    // 输出结果
    for (auto it = predictions.cbegin(); it != predictions.cend(); it++) {
      if (it != predictions.cbegin()) {
        std::cout &lt;&lt; &#39; &#39;;
      }
      std::cout &lt;&lt; dict_-&gt;getLabel(it-&gt;second);
      if (print_prob) {
        std::cout &lt;&lt; &#39; &#39; &lt;&lt; exp(it-&gt;first);
      }
    }
    std::cout &lt;&lt; std::endl;
  }
  ifs.close();
}
</code></pre>

<p>通过对 fasttext 模块的分析，我们发现它最核心的预测和更新逻辑都在 model 模块中，接下来，我们进入 model 模块一探究竟。</p>

<h2 id="toc_4">model 模块</h2>

<p>model 模块对外提供的服务可以分为 <code>update</code> 和 <code>predict</code> 两类，下面我们分别对它们进行分析。由于这里的参数较多，我们先以图示标明各个参数在模型中所处的位置，以免各位混淆。</p>

<p><img src="media/14732610572844/fasttext-model-arch.png" alt="fasttext-model-arch"/></p>

<p>图中所有变量的名字全部与 model 模块中的名字保持一致，注意到 <code>wo_</code> 矩阵在不同的输出层结构中扮演着不同的角色。</p>

<h3 id="toc_5">update</h3>

<p><code>update</code> 函数的作用已经在前面介绍过，下面我们看一下它的实现：</p>

<pre><code class="language-cpp">void Model::update(const std::vector&lt;int32_t&gt;&amp; input, int32_t target, real lr) {
  // target 必须在合法范围内
  assert(target &gt;= 0);
  assert(target &lt; osz_);
  if (input.size() == 0) return;
  // 计算前向传播：输入层 -&gt; 隐层
  hidden_.zero();
  for (auto it = input.cbegin(); it != input.cend(); ++it) {
    // hidden_ 向量保存输入词向量的均值，
    // addRow 的作用是将 wi_ 矩阵的第 *it 列加到 hidden_ 上
    hidden_.addRow(*wi_, *it);
  }
  // 求和后除以输入词个数，得到均值向量
  hidden_.mul(1.0 / input.size());
  
  // 根据输出层的不同结构，调用不同的函数，在各个函数中，
  // 不仅通过前向传播算出了 loss_，还进行了反向传播，计算出了 grad_，后面逐一分析。
  // 1. 负采样
  if (args_-&gt;loss == loss_name::ns) {
    loss_ += negativeSampling(target, lr);
  } else if (args_-&gt;loss == loss_name::hs) {
  // 2. 层次 softmax
    loss_ += hierarchicalSoftmax(target, lr);
  } else {
  // 3. 普通 softmax
    loss_ += softmax(target, lr);
  }
  nexamples_ += 1;

  // 如果是在训练分类器，就将 grad_ 除以 input_ 的大小
  // 原因不明
  if (args_-&gt;model == model_name::sup) {
    grad_.mul(1.0 / input.size());
  }
  // 反向传播，将 hidden_ 上的梯度传播到 wi_ 上的对应行
  for (auto it = input.cbegin(); it != input.cend(); ++it) {
    wi_-&gt;addRow(grad_, *it, 1.0);
  }
}
</code></pre>

<p>下面我们看看三种输出层对应的更新函数：<code>negativeSampling</code>,<code>hierarchicalSoftmax</code>,<code>softmax</code>。</p>

<p>model 模块中最有意思的部分就是将层次 softmax 和负采样统一抽象成多个二元 logistic regression 计算。</p>

<p>如果使用负采样，训练时每次选择一个正样本，随机采样几个负样本，每种输出都对应一个参数向量，保存于 <code>wo_</code> 的各行。对所有样本的参数更新，都是一次独立的 LR 参数更新。</p>

<p>如果使用层次 softmax，对于每个目标词，都可以在构建好的霍夫曼树上确定一条从根节点到叶节点的路径，路径上的每个非叶节点都是一个 LR，参数保存在 <code>wo_</code> 的各行上，训练时，这条路径上的 LR 各自独立进行参数更新。</p>

<p>无论是负采样还是层次 softmax，在神经网络的计算图中，所有 LR 都会依赖于 <code>hidden_</code>的值，所以 <code>hidden_</code> 的梯度 <code>grad_</code> 是各个 LR 的反向传播的梯度的累加。</p>

<p>LR 的代码如下：</p>

<pre><code class="language-cpp">real Model::binaryLogistic(int32_t target, bool label, real lr) {
  // 将 hidden_ 和参数矩阵的第 target 行做内积，并计算 sigmoid
  real score = utils::sigmoid(wo_-&gt;dotRow(hidden_, target));
  // 计算梯度时的中间变量
  real alpha = lr * (real(label) - score);
  // Loss 对于 hidden_ 的梯度累加到 grad_ 上
  grad_.addRow(*wo_, target, alpha);
  // Loss 对于 LR 参数的梯度累加到 wo_ 的对应行上
  wo_-&gt;addRow(hidden_, target, alpha);
  // LR 的 Loss
  if (label) {
    return -utils::log(score);
  } else {
    return -utils::log(1.0 - score);
  }
}
</code></pre>

<p>经过以上的分析，下面三种逻辑就比较容易理解了：</p>

<pre><code class="language-cpp">real Model::negativeSampling(int32_t target, real lr) {
  real loss = 0.0;
  grad_.zero();
  for (int32_t n = 0; n &lt;= args_-&gt;neg; n++) {
    // 对于正样本和负样本，分别更新 LR
    if (n == 0) {
      loss += binaryLogistic(target, true, lr);
    } else {
      loss += binaryLogistic(getNegative(target), false, lr);
    }
  }
  return loss;
}

real Model::hierarchicalSoftmax(int32_t target, real lr) {
  real loss = 0.0;
  grad_.zero();
  // 先确定霍夫曼树上的路径
  const std::vector&lt;bool&gt;&amp; binaryCode = codes[target];
  const std::vector&lt;int32_t&gt;&amp; pathToRoot = paths[target];
  // 分别对路径上的中间节点做 LR
  for (int32_t i = 0; i &lt; pathToRoot.size(); i++) {
    loss += binaryLogistic(pathToRoot[i], binaryCode[i], lr);
  }
  return loss;
}

// 普通 softmax 的参数更新
real Model::softmax(int32_t target, real lr) {
  grad_.zero();
  computeOutputSoftmax();
  for (int32_t i = 0; i &lt; osz_; i++) {
    real label = (i == target) ? 1.0 : 0.0;
    real alpha = lr * (label - output_[i]);
    grad_.addRow(*wo_, i, alpha);
    wo_-&gt;addRow(hidden_, i, alpha);
  }
  return -utils::log(output_[target]);
}
</code></pre>

<h3 id="toc_6">predict</h3>

<p>predict 函数可以用于给输入数据打上 1 ～ K 个类标签，并输出各个类标签对应的概率值，对于层次 softmax，我们需要遍历霍夫曼树，找到 top－K 的结果，对于普通 softmax（包括负采样和 softmax 的输出），我们需要遍历结果数组，找到 top－K。</p>

<pre><code class="language-cpp">void Model::predict(const std::vector&lt;int32_t&gt;&amp; input, int32_t k, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  assert(k &gt; 0);
  heap.reserve(k + 1);
  // 计算 hidden_
  computeHidden(input);
  
  // 如果是层次 softmax，使用 dfs 遍历霍夫曼树的所有叶子节点，找到 top－k 的概率
  if (args_-&gt;loss == loss_name::hs) {
    dfs(k, 2 * osz_ - 2, 0.0, heap);
  } else {
  // 如果是普通 softmax，在结果数组里找到 top-k
    findKBest(k, heap);
  }
  // 对结果进行排序后输出
  // 因为 heap 中虽然一定是 top-k，但并没有排好序
  std::sort_heap(heap.begin(), heap.end(), comparePairs);
}

void Model::findKBest(int32_t k, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  // 计算结果数组
  computeOutputSoftmax();
  for (int32_t i = 0; i &lt; osz_; i++) {
    if (heap.size() == k &amp;&amp; utils::log(output_[i]) &lt; heap.front().first) {
      continue;
    }
    // 使用一个堆来保存 top－k 的结果，这是算 top-k 的标准做法
    heap.push_back(std::make_pair(utils::log(output_[i]), i));
    std::push_heap(heap.begin(), heap.end(), comparePairs);
    if (heap.size() &gt; k) {
      std::pop_heap(heap.begin(), heap.end(), comparePairs);
      heap.pop_back();
    }
  }
}

void Model::dfs(int32_t k, int32_t node, real score, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  if (heap.size() == k &amp;&amp; score &lt; heap.front().first) {
    return;
  }

  if (tree[node].left == -1 &amp;&amp; tree[node].right == -1) {
    // 只输出叶子节点的结果
    heap.push_back(std::make_pair(score, node));
    std::push_heap(heap.begin(), heap.end(), comparePairs);
    if (heap.size() &gt; k) {
      std::pop_heap(heap.begin(), heap.end(), comparePairs);
      heap.pop_back();
    }
    return;
  }
  
  // 将 score 累加后递归向下收集结果
  real f = utils::sigmoid(wo_-&gt;dotRow(hidden_, node - osz_));
  dfs(k, tree[node].left, score + utils::log(1.0 - f), heap);
  dfs(k, tree[node].right, score + utils::log(f), heap);
}
</code></pre>

<h2 id="toc_7">其它模块</h2>

<p>除了以上两个模块，dictionary 模块也相当重要，它完成了训练文件载入，哈希表构建，word n-gram 计算等功能，但是并没有太多算法在里面。</p>

<p>其它模块例如 Matrix, Vector 也只是封装了简单的矩阵向量操作，这里不再做详细分析。</p>

<h2 id="toc_8">附录：构建霍夫曼树算法分析</h2>

<p>在学信息论的时候接触过构建 Huffman 树的算法，课本中的方法描述往往是：</p>

<blockquote>
<p>找到当前权重最小的两个子树，将它们合并</p>
</blockquote>

<p>算法的性能取决于如何实现这个逻辑。网上的很多实现都是在新增节点都时遍历一次当前所有的树，这种算法的复杂度是 \(O(n^2)\)，性能很差。</p>

<p>聪明一点的方法是用一个优先级队列来保存当前所有的树，每次取 top 2，合并，加回队列。这个算法的复杂度是 \(O(nlogn)\)，缺点是必需使用额外的数据结构，而且进堆出堆的操作导致常数项较大。</p>

<p>word2vec 以及 fastText 都采用了一种更好的方法，时间复杂度是 \(O(nlogn)\)，只用了一次排序，一次遍历，简洁优美，但是要理解它需要进行一些推理。</p>

<p>算法如下：</p>

<pre><code class="language-cpp">void Model::buildTree(const std::vector&lt;int64_t&gt;&amp; counts) {
  // counts 数组保存每个叶子节点的词频，降序排列
  // 分配所有节点的空间
  tree.resize(2 * osz_ - 1);
  // 初始化节点属性
  for (int32_t i = 0; i &lt; 2 * osz_ - 1; i++) {
    tree[i].parent = -1;
    tree[i].left = -1;
    tree[i].right = -1;
    tree[i].count = 1e15;
    tree[i].binary = false;
  }
  for (int32_t i = 0; i &lt; osz_; i++) {
    tree[i].count = counts[i];
  }
  // leaf 指向当前未处理的叶子节点的最后一个，也就是权值最小的叶子节点
  int32_t leaf = osz_ - 1;
  // node 指向当前未处理的非叶子节点的第一个，也是权值最小的非叶子节点
  int32_t node = osz_;
  // 逐个构造所有非叶子节点（i &gt;= osz_, i &lt; 2 * osz - 1)
  for (int32_t i = osz_; i &lt; 2 * osz_ - 1; i++) {
    // 最小的两个节点的下标
    int32_t mini[2];
    
    // 计算权值最小的两个节点，候选只可能是 leaf, leaf - 1,
    // 以及 node, node + 1
    for (int32_t j = 0; j &lt; 2; j++) {
      // 从这四个候选里找到 top-2
      if (leaf &gt;= 0 &amp;&amp; tree[leaf].count &lt; tree[node].count) {
        mini[j] = leaf--;
      } else {
        mini[j] = node++;
      }
    }
    // 更新非叶子节点的属性
    tree[i].left = mini[0];
    tree[i].right = mini[1];
    tree[i].count = tree[mini[0]].count + tree[mini[1]].count;
    tree[mini[0]].parent = i;
    tree[mini[1]].parent = i;
    tree[mini[1]].binary = true;
  }
  // 计算霍夫曼编码
  for (int32_t i = 0; i &lt; osz_; i++) {
    std::vector&lt;int32_t&gt; path;
    std::vector&lt;bool&gt; code;
    int32_t j = i;
    while (tree[j].parent != -1) {
      path.push_back(tree[j].parent - osz_);
      code.push_back(tree[j].binary);
      j = tree[j].parent;
    }
    paths.push_back(path);
    codes.push_back(code);
  }
}
</code></pre>

<p>算法首先对输入的叶子节点进行一次排序（\(O(nlogn)\) ），然后确定两个下标 <code>leaf</code> 和 <code>node</code>，<code>leaf</code> 总是指向当前最小的叶子节点，<code>node</code> 总是指向当前最小的非叶子节点，所以，<strong>最小的两个节点可以从 leaf, leaf - 1, node, node + 1 四个位置中取得</strong>，时间复杂度 \(O(1)\)，每个非叶子节点都进行一次，所以总复杂度为 \(O(n)\)，算法整体复杂度为 \(O(nlogn)\)。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SVM 推导]]></title>
    <link href="http://heleifz.github.io/14698080869715.html"/>
    <updated>2016-07-30T00:01:26+08:00</updated>
    <id>http://heleifz.github.io/14698080869715.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1. Intuition</h2>

<p>（每个）样本点距离分类面越远，说明分类面选得越好。为了度量这一点，我们引入<strong>函数间隔</strong>和<strong>几何间隔</strong>。这里的间隔（margin）指的是点与分类面的间隔。</p>

<h3 id="toc_1">函数间隔</h3>

<ul>
<li><p>单个样本<br/>
\[ \hat{\gamma}_i=y_i(\omega^Tx+b) \]</p></li>
<li><p>样本集<br/>
\[ \hat{\gamma}=\min_{1..m}\hat{\gamma}_i \]</p></li>
<li><p>性质<br/>
由于没有对 w 和 b 进行归一化，函数间隔在两者 scale 时候，将会成倍变化。</p></li>
</ul>

<h3 id="toc_2">几何间隔</h3>

<ul>
<li><p>单个样本<br/>
\[ \gamma_i = \hat{\gamma}_i / ||\omega||\]</p></li>
<li><p>样本集<br/>
\[ \gamma=\min_{1..m}\gamma_i \]</p></li>
<li><p>性质<br/>
对 w 进行了归一化，这样，在最优化的时候，我们可以针对 w 的长度施加各种限制，但不影响最终结果。</p></li>
</ul>

<h2 id="toc_3">2. 最优化问题</h2>

<h3 id="toc_4">问题化简</h3>

<ol>
<li><p>基础形态<br/>
\[ \max_{\gamma,\omega,b}\gamma \\ y_i(\omega^tx+b)\ge\gamma \\ ||w||=1\]<br/>
<em>最优化条件非凸</em></p></li>
<li><p>基础形态2<br/>
\[ \max_{\hat{\gamma},\omega,b}\hat{\gamma}/||w|| \\ y_i(\omega^tx+b)\ge\hat{\gamma} \]<br/>
<em>最优化目标非凸</em></p></li>
<li><p>QP形态（施加限制，函数间隔为1，且最大变最小）<br/>
\[ \min_{\omega,b}1/2||w||^2 \\ y_i(\omega^tx+b)\ge1 \]<br/>
<em>二次规划（凸）</em></p></li>
</ol>

<h3 id="toc_5">对偶问题</h3>

<p>对于最优化问题：</p>

<p>\[ \min_\omega f(\omega) \\ g_i(\omega)\le0\ (i=1...k) \\ h_i(\omega)=0\ (i=1..l)\]</p>

<p>有<strong>广义拉格朗日函数</strong>:</p>

<p>\[ L(\omega,\alpha,\beta) = f(\omega)+\Sigma\alpha_ig_i(\omega) + \Sigma\beta_ih_i(\omega) \]</p>

<p>其中要求 alpha 大等于0（显然，否则 Primal 问题不成立）</p>

<ul>
<li><p>Primal 问题 (p*)<br/>
\[ \min_\omega\max_{\alpha,\beta}L(\omega,\alpha,\beta) \]<br/>
一旦 w 超出限定范围，内层的 max 立马飙到正无穷</p></li>
<li><p>Dual 问题 (d*)<br/>
\[ \max_{\alpha,\beta}\min_\omega L(\omega,\alpha,\beta) \]</p></li>
<li><p>两者关系<br/>
\[ d^*\le p^*\]<br/>
根据alpha,beta,g,h的取值范围，在可行域内，拉格朗日函数必须小等于f(w)，自然也小等于p*，无论它怎么min，max，依然小等于。</p></li>
</ul>

<p><strong>在f,g是凸函数，h是仿射函数（wx+b)，且g包围的区域不为空的时候</strong></p>

<p>w*是原问题的解，alpha*，beta*是对偶问题的解，且<br/>
\[ d^*=p^*=L(\omega^*,\alpha^*,\beta^*)\]</p>

<p>此时，有KKT条件成立（充要？）</p>

<p>\[  \frac{\partial}{\partial \omega_i}L(\omega^*,\alpha^*,\beta^*)=0 \\<br/>
    \frac{\partial}{\partial \beta_i}L(\omega^*,\alpha^*,\beta^*)=0 \\<br/>
    \alpha^*_ig_i(\omega^*)=0 \\<br/>
    g_i(\omega^*)\le0 \\<br/>
    \alpha^*\ge0<br/>
\]</p>

<h2 id="toc_6">3. 将QP转换为Dual形式</h2>

<h3 id="toc_7">重写限制条件</h3>

<p>\[ g_i(\omega)=1-y_i(\omega^Tx_i+b)\le0\]<br/>
性质：根据KKT条件，<strong>当alpha_i大于0时，g_i(w)=0</strong>，此时xi是支撑矢量。</p>

<h3 id="toc_8">构造广义拉格朗日函数</h3>

<p>\[ L(\omega,b,\alpha)=1/2||w||^2+\Sigma\alpha_ig_i(\omega)\\<br/>
= 1/2||w||^2+\Sigma_{i=1..m}\alpha_i[1-y_i(\omega^Tx_i+b)]\\<br/>
= 1/2||w||^2+\Sigma\alpha_i-\Sigma\alpha_iy_i\omega^Tx_i-\Sigma\alpha_i y_i b<br/>
\]</p>

<h3 id="toc_9">对于 w 求最小值</h3>

<p>令<br/>
\[<br/>
\frac{\partial}{\partial\omega}L(\omega,b,\alpha)\\<br/>
=\omega-\Sigma\alpha_iy_ix_i=0<br/>
\]<br/>
所以有：<br/>
\[<br/>
\omega^*=\Sigma\alpha_iy_ix_i<br/>
\]<br/>
将其代入到拉格朗日函数中：<br/>
\[<br/>
L(\omega^*,b,\alpha)=1/2(\Sigma\alpha_iy_ix_i^T)(\Sigma\alpha_iy_ix_i)+\Sigma\alpha_i-\Sigma\alpha_iy_i(\Sigma\alpha_iy_ix_i^T)x_i-\Sigma\alpha_iy_ib\\<br/>
=1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i-\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j-\Sigma\alpha_iy_ib<br/>
\\<br/>
=-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i-\Sigma\alpha_iy_ib<br/>
\]</p>

<h3 id="toc_10">对于 b 求最小值</h3>

<p>令<br/>
\[<br/>
\frac{\partial}{\partial b}L(\omega,b,\alpha)\\<br/>
=-\Sigma a_iy_i=0<br/>
\]<br/>
带入到上一步中，消掉最后一项，得到：<br/>
\[<br/>
L(\omega^*,b,\alpha)=-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i<br/>
\]</p>

<h3 id="toc_11">得到对偶问题</h3>

<p>整理得到以下最终的最优化问题：</p>

<p>\[<br/>
\max_\alpha-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i\\<br/>
\alpha_i\ge0\ (i=1..m)\\<br/>
\Sigma a_iy_i=0\ (i=1..m)<br/>
\]</p>

<p>在得到alpha后，w可以根据上面的推导得到，对于b，在w得到后，将正负样本中最接近超平面的两个加起来取平均（根据图示可以很容易看出来）</p>

<p>\[<br/>
b^*=-\frac{\min{\omega^*}^Tx_++\max{\omega^*}^Tx_-}{2}<br/>
\]</p>

<p>在预测时，我们可以不用 w，而是采用如下的公式（kernel trick的基础）</p>

<p>\[<br/>
\omega^Tx+b=\Sigma\alpha_iy_i(x_i^Tx)+b<br/>
\]</p>

<p>因为只有支撑矢量的alpha才非零，所以，我们只需要保存支撑矢量（和b），就能进行分类。</p>

<h2 id="toc_12">4. 核函数</h2>

<p>给定一个feature mapping（x-&gt;phi(x)），核函数K定义为：</p>

<p>\[<br/>
K(x,z)=\phi(x)^T\phi(z)<br/>
\]</p>

<p>可以看出，当x和z相近时，核函数取值越大，x和z越相似。我们可以直接找核函数，而不用找到显式的 feature mapping，为了做到这一点，必须规定核函数所需要满足的条件。</p>

<h3 id="toc_13">Mercer定理</h3>

<p>给定函数K，对于任意m个向量，它们构成的核矩阵对称半正定 &lt;=&gt; K是一个核函数</p>

<h2 id="toc_14">5. 软间隔策略</h2>

<p>为了处理线性不可分情况，SVM可以改写为：</p>

<p>\[<br/>
\min_{\omega,b}1/2||w||^2 + C\Sigma\xi_i\\<br/>
y_i(\omega^tx+b)\ge1-\xi_i\ (i=1..m)\\<br/>
\xi_i\ge0 (i=1..m)<br/>
\]</p>

<p>化简成对偶问题后，和之前的形式惊人相近，除了alpha的范围</p>

<p>\[<br/>
\max_\alpha-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i\\<br/>
C\ge\alpha_i\ge0\ (i=1..m)\\<br/>
\Sigma a_iy_i=0\ (i=1..m)<br/>
\]</p>

<p>此时，b的计算也变了（略），且KKT条件有如下推论：</p>

<p>\[<br/>
\alpha_i=0 ⇒ y_i(\omega^Tx+b)\ge1\\<br/>
\alpha_i=C ⇒ y_i(\omega^Tx+b)\le1\\<br/>
0\le\alpha_i\le C ⇒ y_i(\omega^Tx+b)=1<br/>
\]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WSABIE 算法解释]]></title>
    <link href="http://heleifz.github.io/14696374110477.html"/>
    <updated>2016-07-28T00:36:51+08:00</updated>
    <id>http://heleifz.github.io/14696374110477.html</id>
    <content type="html"><![CDATA[
<p><img src="media/14696374110477/1369025_221049006000_2.jpg" alt="1369025_221049006000_2" style="width:443px;"/></p>

<p>算法论文：<a href="http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf">WSABIE: Scaling Up To Large Vocabulary Image Annotation</a></p>

<h2 id="toc_0">介绍</h2>

<p>WSABIE 是一个通用的打标签算法，无论对象是图像，视频，还是文本，只要能抽取出 feature，并提供一个固定的标签集合，就可以使用 WSABIE，由于算法的通用性，下文将需要打标签的东西称为 <em>对象</em>。</p>

<h2 id="toc_1">模型</h2>

<h3 id="toc_2">1. Joint Word-Image Model</h3>

<p>对于 WSABIE 来说，打标签的过程，就是计算所有标签与当前对象的相似性，并取出相似性最高的标签作为结果。由于对象的 feature 和标签是两种不同的东西，为了计算相似性，WSABIE 将它们映射到 <strong>同一个向量空间</strong> \( R^D \) 。</p>

<p><img src="media/14696374110477/wsb-1.png" alt="wsb-1" style="width:391px;"/></p>

<p>为了将对象 \(I\) 的 feature vector \( x \) 映射到 \( R^D \)，只需做线性变换 \(  \Phi _I(x)=Vx \)，其中 \(V\) 是变换矩阵。</p>

<p>由于标签是离散的符号，为了将其映射到 \( R^D \)，需要采用 Word2Vec 一样的思想，把它们转换成 embedding，换言之，我们把所有标签（假设有 \(N\) 个)对应的向量存在矩阵 \(W\) （大小 \(D \times N\)）中，第 i 个标签就是矩阵的第 i 列： \( \Phi _W(i)=W_i \)</p>

<p>于是，对象 \( I \) 与第 i 个标签的相似度就是：</p>

<p>\[ f_i(x) = \Phi _W(i)^T \Phi _I(x)=W_i^TVx \]</p>

<p>模型中的待学习参数为线性变换矩阵 \(V\)，以及所有标签的 embedding 向量 \(W\)。</p>

<h3 id="toc_3">2. 损失函数</h3>

<p>每条训练数据都以 \( (x,y) \) 的形式存在，其中 \(x\) 是对象，\(y\) 是它唯一的一条正确的标签。</p>

<p>模型的优化目标，是让正确描述对象的标签，与对象有较高的相似度。更具体地说 \[ rank_y(f(x)) = \Sigma _{i \ne y} I(f_i(x) \ge f_y(x)) \]</p>

<p>这个式子表示 <strong>&quot;排名大于 \(y\) 的标签的个数&quot;</strong>（\(I\) 是示性函数)，这条训练数据的误差是</p>

<p>\[ L(rank_y(f(x)))\]</p>

<p>其中 </p>

<p>\[ L(k) = \Sigma _{j=1}^k \alpha_j \]</p>

<p>根据常数 \( \alpha_j \) 的取值的不同，我们可以得到不同的损失函数，所以 \( L \) 其实代表一类函数。</p>

<p>如果 \( \alpha_j \) 取值都为 \(C\)， 那么如果 \(y\) 被模型排到第 \(k\) 名，损失值就是 \(Ck\)。论文中 \( \alpha_j = 1/j \)。</p>

<h2 id="toc_4">学习</h2>

<p>WSABIE 对损失函数进行了一系列近似，让模型能够使用随机梯度下降（SGD）在线更新参数，这个学习算法是本文 <em>最有价值的部分</em>。下面依次介绍近似步骤：</p>

<h3 id="toc_5">1. 改写损失函数</h3>

<p>\[ err(f(x), y) = L(rank_y(f(x)) \frac{rank_y(f(x))}{rank_y(f(x)) }\]</p>

<p>\[ =L(rank_y(f(x)) \frac{\Sigma_{i\ne y}I(f_i(x) \ge f_y(x))}{rank_y(f(x)} \]</p>

<p>\[ =\Sigma_{i \ne y} L(rank_y(f(x)) \frac{ I(f_i(x) \ge f_y(x))}{rank_y(f(x)} \]</p>

<h3 id="toc_6">2. 近似损失函数</h3>

<p>使用 hinge loss 近似 0／1 损失函数，得到如下的近似损失：</p>

<p>\[ \overline{err}(f(x), y)=\Sigma_{i \ne y} L(rank_y^1(f(x)) \frac{ |1-f_y(x)+f_i(x)|_+}{rank_y^1(f(x))} \]</p>

<p>其中：</p>

<p>\[<br/>
rank_y^1(f(x))=\Sigma_{i\ne y}I(1+f_i(x) \ge f_y(x))<br/>
\]</p>

<p>使用这个近似的损失函数，得到在当前数据集分布的期望误差：</p>

<p>\[Risk(f)=\int\overline{err}(f(x), y)dP(x,y)\]</p>

<p>按照分布 \(P(x,y)\) 随机选择一个样本 \((x,y)\)，计算其 \(\overline{err}(f(x), y)\)，可以对这个期望误差进行估计。</p>

<p>实际上 </p>

<p>\[ \Sigma_{i\ne y}\frac{ |1-f_y(x)+f_i(x)|_+}{rank_y^1(f(x))} \]</p>

<p>这个值，也可以通过采样进行估计，方法为，从满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的所有标签中，随机采样一个 \(\overline{y}\)，计算 \( 1-f_y(x)+f_{\overline{y}}(x) \)</p>

<p>综上，总共两步采样：</p>

<ol>
<li>随机选一个 \((x,y)\) </li>
<li>随机选一个标签 \( \overline{y} \)</li>
</ol>

<p>得到 \(Risk(f)\) 的一个估计：</p>

<p>\[ \overline{err}(f(x),y,\overline{y})=L(rank_y^1(f(x))(1-f_y(x)+f_{\overline{y}}(x))\]</p>

<h3 id="toc_7">3. 随机梯度下降</h3>

<p>接下来，就应用标准的随机梯度下降法，通过对 \(Risk(f)\) 的估计 \(\overline{err}(f(x),y,\overline{y})\) 求梯度，优化损失函数：</p>

<p>\[ \beta_{t+1}=\beta{t}-\gamma_t \frac {\partial\overline{err}(f(x),y,\overline{y})} {\partial\beta_{t}}\]</p>

<p>其中 \(\beta\) 是模型参数，也就是 \(W\) 和 \(V\) 矩阵。</p>

<h3 id="toc_8">4. 进一步优化性能</h3>

<p>此时，每一步优化的性能还是不好，原因是计算 \(\overline{err}(f(x),y,\overline{y})\) 时，我们需要知道所有满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的标签，因为：</p>

<ol>
<li>计算 \(L(rank_y^1(f(x))\) 要用到（记得那个求和式子吗）</li>
<li>采样的第二步要用到（因为要从这些标签里采样）</li>
</ol>

<p>论文中第 <strong>最聪明的地方</strong> 来了，作者提出了一个近似 \(\overline{err}(f(x),y,\overline{y})\) 的方法，解决以上两个问题：</p>

<p>在第二步的采样中，不再从满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的标签里采样，而是从 <strong>除去 \(y\) 的所有</strong> 标签里进行采样，如果采样到的标签 i 不满足 \( 1-f_y(x)+f_i(x) &gt; 0 \)，就把它放回，再次采样，直到找到满足条件的标签为止。</p>

<p>表面上，这解决了第二次采样的问题，这种有放回采样的效果，等价于原来的采样，而且不需要事先算出所有满足条件的标签。</p>

<p>但实际上，这也解决了第一个问题。假设一共有 \(k\) 个标签满足  \( 1-f_y(x)+f_i(x) &gt; 0 \)，那么，从所有标签中采样到这种标签的概率就是 \(P=\frac{k}{N－1}\)，由于这种有放回采样满足 <em>几何分布</em> （还记得大学概率课吗？一直投骰子，直到遇到6），所以期望的采样次数为：</p>

<p>\[ E[采样次数] = \frac{N-1}{k} \]</p>

<p>我们可以用实际的采样次数近似期望的采样次数，所以我们可以近似得到 \(k\) 的值为 \( \frac{N-1}{采样次数} \)，于是可以算出 \(L(rank_y^1(f(x))\) 的近似值。</p>

<h2 id="toc_9">其它讨论</h2>

<p>此外，论文还对 \(W\) 和 \(V\) 每一列的大小做了约束：</p>

<p>\[ ||W||_2 &lt;= C,||V||_2 &lt;= C \]</p>

<p>在优化时，如果某一列超过了 C，就将该列的值等比例缩小一下。</p>

<p>如果理解了后续的采样步骤，就会理解对损失函数的近似步骤：</p>

<blockquote>
<p>之所以将 0/1 loss 近似为 hinge loss，是为了让损失函数连续，之所以用 \(rank_y^1(f(x),y) \) 替换 \(rank_y(f(x),y) \) ，是为了在第二步采样时去掉分母</p>
</blockquote>

<p>在训练过程的每一步，都仅涉及一个训练数据 \((x,y)\)，以及后续的几条采样数据，这种 online learning 的学习方法适用于海量数据集。</p>

<p>实际应用中，每个对象会有 <em>多个标签</em>，在训练时，可以针对每个标签单独跑一次采样，</p>

<p>此外，我还有一个想法，模型中的输入 feature 已经是计算好了的，我们也可以把原始的数据直接放入模型，例如，利用 NN 自动学习出特征，实现 end-to-end 的打标签架构。</p>

<p>算法名字很像&quot;芥末&quot;，所以文章的开头我放了芥末。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache 配置文件解释]]></title>
    <link href="http://heleifz.github.io/14698086378177.html"/>
    <updated>2016-07-30T00:10:37+08:00</updated>
    <id>http://heleifz.github.io/14698086378177.html</id>
    <content type="html"><![CDATA[
<p>我是业余 Web 开发，平时主要是做算法，大概每隔几个月会搭一个网站。服务器用的是 Apache，每次配置的时候，都奉行拿来主义，从别的地方把 <strong>httd.conf</strong> 以及 <strong>.htaccess</strong> 文件内容复制过来，对配置只知道个大概，没有深入学习。</p>

<p>周末做新项目的时候，专门去 Apache 官网阅读了一下 <a href="http://httpd.apache.org/docs/2.4/">User Guide</a>，发现还是很好懂的，以前很多的疑惑都解开了。这篇文章我整理一下 Apache 配置架构，给以后的我复习用。</p>

<h2 id="toc_0">Apache 配置架构</h2>

<p>Apache 本身的架构是一个核心＋外围的 Module，它的配置也遵循这个结构，本文介绍 Apache 的核心配置，以及 <strong>mod_wsgi</strong> 和 <strong>mod_rewrite</strong> 两个模块。</p>

<h3 id="toc_1">基本语法</h3>

<p>Apache 的配置位于 <strong>/etc/apache2/httpd.conf</strong> 或者 <strong>apache2.conf</strong>  之类的地方，具体路径各个平台不同。每条配置都是一个指令 (directive) :</p>

<blockquote>
<p>指令名(如Listen) 配置值(如80)</p>
</blockquote>

<p>利用 <strong>Include</strong> 指令，配置文件还可以包含其它的配置文件或文件夹。</p>

<h3 id="toc_2">作用域（Directory, Location, VirtualHost, .htaccess）</h3>

<p>大部分指令都可以作用于某个特定的路径，这个路径可以用本机文件系统路径表示(Directory)，也可以用网络位置表示(Location)，这两种表示方式很多时候可以互换，不过最好用本机路径，这样安全一点，因为多个 <strong>Location</strong> 都可以到达同一个位置。</p>

<p>Directory 的配置是递归传播的。</p>

<p>作用域上的配置是经过 merge 后生效的，merge 的优先级是：<br/>
1. &lt;Directory&gt; (except regular expressions) and .htaccess done simultaneously (with .htaccess, if allowed, overriding &lt;Directory&gt;)<br/>
2. &lt;DirectoryMatch&gt; (and &lt;Directory &quot;~&quot;&gt;)<br/>
3. &lt;Files&gt; and &lt;FilesMatch&gt; done simultaneously<br/>
4. &lt;Location&gt; and &lt;LocationMatch&gt; done simultaneously<br/>
5. &lt;If&gt;<br/>
配置按照出现的顺序处理，不过对于 Directory，处理顺序是 <strong>最短-&gt;最长</strong>，例如，<code>&lt;Directory &quot;/var/web/dir&quot;&gt;</code> 会先于 <code>&lt;Directory &quot;/var/web/dir/subdir&quot;&gt;</code> 处理，如果多个 Directory 指令作用于同一路径，那么后处理的会覆盖到前处理的上面。<strong>(Apache有些逻辑是 first match，例如 Alias，有些是后面覆盖前面）</strong></p>

<p>如果一个 Directory 设定了 <code>AllowOverride All</code>，那么它还可以用 .htaccess 中的配置来覆盖。</p>

<p>VirtualHost 根据访问者使用的域名(name based)或者访问的 IP 地址(IP based)，使用特定的配置，这样可以实现一个服务器 Host 多个网站。VirtualHost 的配置会覆盖全局配置。</p>

<h2 id="toc_3">mod_wsgi</h2>

<p>mod_wsgi 通过 WSGIScriptAlias 这个指令，把某些指令路由给 Python 脚本处理，和 Alias 一样，第一个 Match 的生效。</p>

<p><a href="http://ssmax.net/archives/977.html">mod_wsgi的两种工作模式</a></p>

<h2 id="toc_4">mod_rewrite</h2>

<pre><code class="language-apache">&lt;IfModule mod_rewrite.c &gt;
    RewriteEngine On
    # 拼 URL 时使用的前缀
    RewriteBase /
    # test string 可以用一堆常量，例如 %{REQUEST_URI} 
    # CondPattern 可以是正则表达式，也可以是一些其它的 test，例如文件是否存在
    # 默认是 AND，可以换成 [OR]
    RewriteCond TestString CondPattern [OR]
    # 支持 backref
    RewriteRule Pattern Substitution [flags]
&lt;/IfModuel&gt;
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git]]></title>
    <link href="http://heleifz.github.io/14696393471602.html"/>
    <updated>2016-07-28T01:09:07+08:00</updated>
    <id>http://heleifz.github.io/14696393471602.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1. 对象模型</h2>

<ul>
<li>Git由一堆对象组成，每个对象都有一个SHA1的40位编码</li>
<li>每个文件都是一个blob，由内容进行hash，同样内容的文件在版本库内<strong>只有一份</strong>。</li>
<li>每个目录都是一个tree，tree中可以有tree也可以有blob。（文件名等信息在tree中保存）</li>
<li>每个commit都指向一个tree，一个或多个parent，包含作者，email，日期，消息等数据。（tag就是一个commit的引用而已）</li>
<li><strong>Git Repo就是由commit组成的网络</strong>。</li>
</ul>

<blockquote>
<p>既然如此，我们就得找到表示commit的方式</p>
</blockquote>

<h2 id="toc_1">2. Commit的表示法</h2>

<h3 id="toc_2">基础</h3>

<ul>
<li>branchname : 访问branchname最新的commit</li>
<li>tagname : 访问tagname引用的commit</li>
<li>HEAD</li>
<li>完整的hash</li>
<li>hash的前几位（7位一般就够了）</li>
</ul>

<h3 id="toc_3">组合</h3>

<ul>
<li>name^ : name的parent commit</li>
<li>name<sup>^</sup> : parent的parent</li>
<li>name<sup>n</sup> : 第 n 个 parent（在merge的情况下）</li>
<li>name~n : 倒数第 n 个 parent</li>
<li>name:path : commit中的文件</li>
</ul>

<h3 id="toc_4">范围</h3>

<ul>
<li>name1..name2 : name2能回溯到的，但是name1不能回溯到的（master..，相当于 mater..HEAD，当前分支的修改）</li>
<li>name1...name2 : name1的和name2的祖先，但不是共同祖先</li>
</ul>

<h3 id="toc_5">描述</h3>

<pre><code class="language-bash">--since=&quot;two weeks ago&quot;
--until=&quot;1 week ago&quot;
--grep=pattern
--committer=pattern
--author=pattern
--no-merges
</code></pre>

<h2 id="toc_6">3. 三个位置</h2>

<h3 id="toc_7">Working Tree</h3>

<p>当前的工作目录，包含文件。<code>git add</code>以后，进入 index</p>

<h3 id="toc_8">Index</h3>

<p>暂存区。所谓 index 是指，暂存区里包含了各种新增 object 的引用（索引）。<code>git commit</code>以后，进入 branch。</p>

<h3 id="toc_9">Branch</h3>

<p>当前的分支。</p>

<h2 id="toc_10">4. 部分操作</h2>

<h3 id="toc_11"><code>config</code></h3>

<p>在使用 git 前必须设置好名字和邮箱，如果不用 global 参数，则是设置项目专用的作者信息。</p>

<pre><code class="language-bash">$ git config --global user.name &#39;Your Name&#39;
$ git config --global user.email you@somedomain.com
</code></pre>

<h3 id="toc_12"><code>add</code></h3>

<p>将文件从 working tree 添加到 index<br/>
<code>bash<br/>
git add &lt;path&gt; # 新增，修改的文件（文件夹）<br/>
git add -u &lt;path&gt; # 修改，删除的文件（文件夹）<br/>
git add -A &lt;path&gt; # 新增，修改，删除的文件（文件夹）<br/>
</code></p>

<h3 id="toc_13"><code>commit</code></h3>

<p>将 index 的 tree 作为下一个 commit，加入当前 branch。<br/>
<code>bash<br/>
git commit -m &#39;my commit message..&#39;<br/>
git commit -a # 先 add 再 commit （-am)<br/>
git commit --amend -m &quot;correct some error&quot; # amend产生新的ID，请不要amend和别人共享的commit<br/>
</code></p>

<h3 id="toc_14"><code>diff</code></h3>

<pre><code class="language-bash">git diff # unstaged change
git diff --cached # difference between index and branch
git diff HEAD # diff between working tree + index and branch
</code></pre>

<h3 id="toc_15"><code>checkout</code></h3>

<pre><code class="language-bash"># 注：忽略未commit的文件，--force参数可以暴力覆盖

git checkout &lt;branch&gt;  # 更新三棵树和HEAD
git checkout &lt;commit&gt;  # 更新HEAD和working tree
git checkout           # 用 last commit 或 index 来复位 working tree
</code></pre>

<h3 id="toc_16"><code>reset</code></h3>

<p>reset 指令不是单纯修改HEAD（与checkout不同），而是先修改branch，然后把HEAD指向新的branch。</p>

<pre><code class="language-bash">git reset --soft  # 只修改 branch
git reset --mixed # 默认，修改 branch + index 
git reset --hard  # 修改 branch + index + working tree
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[shared_ptr 原理及事故]]></title>
    <link href="http://heleifz.github.io/14696398760857.html"/>
    <updated>2016-07-28T01:17:56+08:00</updated>
    <id>http://heleifz.github.io/14696398760857.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">new与赋值的坑</h2>

<p>赋值（assignment）和new运算符在C++与Java（或C#）中的行为有本质的区别。在Java中，new是对象的构造，而赋值运算是引用的传递；而在C++中，赋值运算符意味着&quot;构造&quot;，或者&quot;值的拷贝&quot;，new运算符意味着在堆上分配内存空间，并将这块内存的管理权（责任）交给用户。C++中的不少坑，就是由new和赋值引起的。</p>

<p>在C++中使用new的原因除了堆上能定义体积更大的数据结构之外，就是能使用C++中的dynamic dispatch（也叫多态）了：只有指针（和引用）才能使用虚函数来展现多态性。在这时，new出来的指针变得很像Java中的普通对象，赋值意味着引用的传递，方法调用会呈现出多态性，我们进入了面向对象的世界，一切十分美好，除了&quot;要手动释放内存&quot;。</p>

<p>在简单的程序中，我们不大可能忘记释放new出来的内存，随着程序规模的增大，我们忘了delete的概率也随之增大，这是因为C++是如此一个精神分裂的语言，赋值运算符竟然同时展现出&quot;值拷贝&quot;和&quot;引用传递&quot;两种截然不同的语义，这种不一致性导致&quot;内存泄漏&quot;成为C++新手最常犯的错误之一。当然你可以说，只要细心一点，一定能把所有内存泄漏从代码中清除。但手动管理内存更严重的问题在于，内存究竟要由谁来分配和释放呢？指针的赋值将同一对象的引用散播到程序每个角落，但是该对象的删除却只能发生一次，当你在代码中用完这么一个资源指针：resourcePtr，你敢delete它吗？它极有可能同时被多个对象拥有着，而这些对象中的任何一个都有可能在之后使用该资源，而这些对象中的另外一个，可能在它的析构函数中释放该资源。&quot;那我不delete不就行了吗？&quot;，你可能这么问，当然行， 这时候你要面对另外一种可能性：也许你是这个指针的唯一使用者，如果你用完不delete，内存就泄漏了。</p>

<p>开发者日常需要在工作中使用不同的库，而以上两种情况可能会在这些库中出现，假设库作者们的性格截然不同，导致这两个库在资源释放上采取了不同的风格，在这个时候，你面对一个用完了的资源指针，是删还是不删呢？这个问题从根本上来说，是因为C++的语言特性让人容易搞错&quot;资源的拥有者&quot;这个概念，资源的拥有者，从来都只能是系统，当我们需要时便向系统请求，当我们不需要时就让系统自己捡回去（Garbage Collector），当我们试图自己当资源的主人时，一系列坑爹的问题就会接踵而来。</p>

<h2 id="toc_1">异常安全的类</h2>

<p>我们再来看另外一个与new运算符紧密相关的问题：如何写一个异常安全（exception safe）的类。</p>

<p>异常安全简单而言就是：当你的类抛出异常后，你的程序会不会爆掉。爆掉的情况主要包括：内存泄漏，以及不一致的类状态（例如一个字符串类，它的size()方法返回的字符串大小与实际的字符串大小不同），这里仅讨论内存泄漏的情况。</p>

<p>为了让用户免去手动delete资源的烦恼，不少类库采用了RAII风格，即<em>Resource Acquisition Is Initialization</em>，这种风格采用类来封装资源，在类的构造函数中获取资源，在类的析构函数中释放资源，这个资源可以是内存，可以是一个网络连接，也可以是mutex这样的线程同步量。在RAII的感召下，我们来写这么一个人畜无害的类：</p>

<pre><code class="language-cpp">class TooSimple {
private:
    Resource *a;
    Resource *b;
public
    TooSimple() {
        a = new Resource();
        b = new Resource(); //在这里抛出异常
    }
    ~TooSimple() {
        delete a;
        delete b;
    }
};
</code></pre>

<p>这个看似简单的类，是有内存泄漏危险的哟！为了理解这一点，首先简单介绍一下C++在抛出异常时所做的事吧：</p>

<p>如果一个new操作（及其调用的构造函数）中抛出了异常，那么它分配的内存空间将自动被释放。<br/>
一个函数（或方法）抛出异常，那么它首先将当前栈上的变量全部清空(unwinding)，如果变量是类对象的话，将调用其析构函数，接着，异常来到call stack的上一层，做相同操作，直到遇到catch语句。<br/>
指针是一个普通的变量，不是类对象，所以在清空call stack时，指针指向资源的析构函数将不会调用。<br/>
根据这三条规则，我们很容易发现，如果<code>b = new Resource()</code>句抛出异常，那么构造函数将被强行终止，根据规则1，b分配的资源将被释放（假设Resource类本身是异常安全的），指针a，b从call stack上清除，由于此时构造函数还未完成，所以TooSimple的析构函数也不会被调用（都没构造完呢，现在只是一个&quot;部分初始化&quot;的对象，析构函数自然没理由被调用），a已经被分配了资源，但是call stack被清空，地址已经找不到了，于是delete永远无法执行，于是内存泄漏发生了。</p>

<p>这个问题有一个很直接的&quot;解决&quot;方案，那就是把<code>b = new Resource()</code>包裹在一个try-catch块中，并在catch里将执行delete a，这样做当然没问题，但我们的代码逻辑变得复杂了，且当类需要分配的资源种类增多的时候，这种处理办法会让程序的可读性急剧下降。这时候我们不禁想：要是指针变量能像类对象一样地&quot;析构&quot;就好了，一旦指针具有类似析构的行为，那么在call stack被清空时，指针会在&quot;析构&quot;时实现自动的delete。怀着这种想法，我们写了这么一个类模版：</p>

<pre><code class="language-cpp">template &lt;typename T&gt;
class StupidPointer {
public:
    T *ptr;
    StupidPointer(T *p) : ptr(p) {}
    ~StupidPointer() { delete ptr; }
};
有了这个&quot;酷炫&quot;的类，现在我们的构造函数可以这么写：
TooSimple() {
    a = StupidPointer&lt;Resource&gt;(new Resource());
    b = StupidPointer&lt;Resource&gt;(new Resource());
};
</code></pre>

<p>由于此时的a，已经不再是指针，而是<code>StupidPointer&lt;Resource&gt;</code>类，在清空call stack时，它的析构函数被调用，于是a指向的资源被释放了。但是，StupidPointer类有一个严重的问题：当多个StupidPointer对象管理同一个指针时，一个对象析构后，剩下对象中保存的指针将变成指向无效内存地址的&quot;野指针&quot;（因为已经被delete过了啊），如果delete一个野指针，电脑就会爆炸（严肃）。</p>

<p>C++11的标准库提供了两种解决问题的思路：1、不允许多个对象管理一个指针（unique_ptr）；2、允许多个对象管理同一个指针，但仅当管理这个指针的最后一个对象析构时才调用delete（shared_ptr）。这两个思路的共同点是：只！允！许！delete一次！</p>

<p>本篇文章里，我们仅讨论shared_ptr。</p>

<h2 id="toc_2">shared_ptr</h2>

<p>在将shared_ptr的使用之前，我们首先来看看它的基本实现原理。</p>

<p>刚才说到，当多个shared_ptr管理同一个指针，仅当最后一个shared_ptr析构时，指针才被delete。这是怎么实现的呢？答案是：引用计数（reference counting）。引用计数指的是，所有管理同一个裸指针（raw pointer）的shared_ptr，都共享一个引用计数器，每当一个shared_ptr被赋值（或拷贝构造）给其它shared_ptr时，这个共享的引用计数器就加1，当一个shared_ptr析构或者被用于管理其它裸指针时，这个引用计数器就减1，如果此时发现引用计数器为0，那么说明它是管理这个指针的最后一个shared_ptr了，于是我们释放指针指向的资源。</p>

<p>在底层实现中，这个引用计数器保存在某个内部类型里（这个类型中还包含了deleter，它控制了指针的释放策略，默认情况下就是普通的delete操作），而这个内部类型对象在shared_ptr第一次构造时以指针的形式保存在shared_ptr中。shared_ptr重载了赋值运算符，在赋值和拷贝构造另一个shared_ptr时，这个指针被另一个shared_ptr共享。在引用计数归零时，这个内部类型指针与shared_ptr管理的资源一起被释放。此外，为了保证线程安全性，引用计数器的加1，减1操作都是原子操作，它保证shared_ptr由多个线程共享时不会爆掉。</p>

<p>这就是shared_ptr的实现原理，现在我们来看看怎么用它吧！（超简单）</p>

<p><code>std::shared_ptr</code>位于头文件<memory>中（这里只讲C++11，boost的shared_ptr当然是放在boost的头文件中），下面我以代码示例的形式展现它的用法，具体文档可以看这里。</p>

<pre><code class="language-cpp">// 初始化
shared_ptr&lt;int&gt; x = shared_ptr&lt;int&gt;(new int); // 这个方法有缺陷，下面我会说
shared_ptr&lt;int&gt; y = make_shared&lt;int&gt;();
shared_ptr&lt;Resource&gt; obj = make_shared&lt;Resource&gt;(arg1, arg2); // arg1, arg2是Resource构造函数的参数
// 赋值
shared_ptr&lt;int&gt; z = x; // 此时z和x共享同一个引用计数器
// 像普通指针一样使用
int val = *x;
assert (x == z);
assert (y != z);
assert (x != nullptr);
obj-&gt;someMethod();
// 其它辅助操作
x.swap(z); // 交换两个shared_ptr管理的裸指针（当然，包含它们的引用计数）
obj.reset(); // 重置该shared_ptr（引用计数减1）
</code></pre>

<p>太好用了！</p>

<h2 id="toc_3">错误用法1：循环引用</h2>

<p>shared_ptr的一个最大的缺点，或者说，引用计数策略最大的缺点，就是循环引用（cyclic reference），下面是一个典型的事故现场：</p>

<pre><code class="language-cpp">class Observer; // 前向声明
class Subject {
private:
    std::vector&lt;shared_ptr&lt;Observer&gt;&gt; observers;
public:
    Subject() {}
    addObserver(shared_ptr&lt;Observer&gt; ob) {
        observers.push_back(ob);
    }
    // 其它代码
    ..........
};
class Observer {
private:
    shared_ptr&lt;Subject&gt; object;
public:
    Observer(shared_ptr&lt;Object&gt; obj) : object(obj) {}
    // 其它代码
    ...........
};
</code></pre>

<p>目标（Subject）类连接着多个观察者（Observer）类，当某个事件发生时，目标类可以遍历观察者数组observers，对每个观察者进行&quot;通知&quot;，而观察者类中，也保存着目标类的shared_ptr，这样多个观察者之间可以以目标类为桥梁进行沟通，除了会发生内存泄漏以外，这是很不错的设计模式嘛！等等，不是说用了shared_ptr管理资源后就不会内存泄漏了吗？怎么又漏了？</p>

<p>这就是引用计数模型失效的唯一的情况：循环引用。循环引用指的是，一个引用通过一系列的引用链，竟然引用回自身，上面的例子中，<code>Subject-&gt;Observer-&gt;Subject</code>就是这么一条环形的引用链。假设我们的程序中只有一个变量<code>shared_ptr&lt;Subject&gt; p</code>，此时，p指向的对象不仅通过该shared_ptr引用自己，还通过它包含的Observer中的object成员变量引用回自己，于是它的引用计数是2，每个Observer的引用计数都是1。当p析构时，它的引用计数减1，变成2-1=1（大于0!），p指向对象的析构函数将不会被调用，于是p和它包含的每个Observer对象在程序结束时依然驻留在内存中没被delete，形成内存泄漏。</p>

<h2 id="toc_4">weak_ptr</h2>

<p>为了解决这一问题，标准库提供了<code>std::weak_ptr</code>（弱引用），它也位于<memory>中。</p>

<p>weak_ptr是shared_ptr的&quot;观察者&quot;，它与一个shared_ptr绑定，但却不参与引用计数的计算，在需要时，它还能摇身一变，生成一个与它所&quot;观察&quot;的shared_ptr共享引用计数器的新shared_ptr。总而言之，weak_ptr的作用就是：在需要时变出一个shared_ptr，在其它时候不干扰shared_ptr的引用计数。</p>

<p>在上面的例子中，我们只需简单地将Observer中object成员的类型换成<code>std::weak_ptr&lt;Subject&gt;</code>即可解决内存泄漏的问题，此刻（接着上面的例子），p指向对象的引用计数为1，所以在p析构时，Subject指针将被delete，其中包含的observers数组在析构时，内部的Observer对象的引用计数也将变为0，故它们也被delete了，资源释放得干干净净。</p>

<p>下面，是weak_ptr的使用方法：</p>

<pre><code class="language-cpp">std::shared_ptr&lt;int&gt; sh = std::make_shared&lt;int&gt;();
// 用一个shared_ptr初始化
std::weak_ptr&lt;int&gt; w(sh);
// 变出shared_ptr
std::shared_ptr&lt;int&gt; another = w.lock();
// 判断weak_ptr所观察的shared_ptr的资源是否已经释放
bool isDeleted = w.expired();
</code></pre>

<h2 id="toc_5">错误用法2：多个无关的shared_ptr管理同一裸指针</h2>

<p>考虑下面这个情况：</p>

<pre><code class="language-cpp">int *a = new int;
std::shared_ptr&lt;int&gt; p1(a);
std::shared_ptr&lt;int&gt; p2(a);
</code></pre>

<p>p1和p2同时管理同一裸指针a，与之前的例子不同的是，此时的p1和p2有着完全独立的两个引用计数器（初始化p2时，用的是裸指针a，于是我们没有任何办法获取p1的引用计数！），于是，上面的代码会导致a被delete两次，分别由p1和p2的析构导致，电脑再一次爆炸了。</p>

<p>为了避免这种情况的发生，我们永远不要将new用在shared_ptr构造函数参数列表以外的地方，或者干脆不用new，改用make_shared。</p>

<p>即便我们的程序严格采取上述做法，C++还提供另外一种绕过shared_ptr，直接获取裸指针的方式，那就是this指针。请看下面的事故现场：</p>

<pre><code class="language-cpp">class A {
public:
    std::shared_ptr&lt;A&gt; getShared() {
        return std::shared_ptr&lt;A&gt;(this);
    }
};
int main() {
    std::shared_ptr&lt;A&gt; pa = std::make_shared&lt;A&gt;();
    std::shared_ptr&lt;A&gt; pbad = pa-&gt;getShared();
    return 0;
}
</code></pre>

<p>在此次事故中，pa和pbad拥有各自独立的引用计数器，所以程序将发生相同的&quot;delete野指针&quot;错误。总而言之，管理同一资源的shared_ptr，只能由同一个初始shared_ptr通过一系列赋值或者拷贝构造途径得来。更抽象的说，管理同一资源的shared_ptr的构造顺序，必须是一个无环有向的连通图，无环能够保证没有循环引用，连通性能够保证每个shared_ptr都来自于相同的源。</p>

<p>另外，标准库提供了一种特殊的接口，来解决&quot;生成this指针的shared_ptr&quot;的问题。</p>

<h2 id="toc_6">enable_shared_from_this</h2>

<p>enable_shared_from_this是标准库中提供的接口（一个基类啦）：</p>

<pre><code class="language-cpp">template&lt;typename T&gt;
class enable_shared_from_this {
public:
    shared_ptr&lt;T&gt; shared_from_this();
}
</code></pre>

<p>如果想要一个由shared_ptr管理的类A对象能够在方法内部得到this指针的shared_ptr，且返回的shared_ptr和管理这个类的shared_ptr共享引用计数，只需让这个类派生自<code>enable_shared_from_this&lt;A&gt;</code>即可，之后调用<code>shared_from_this()</code>即可获得正确的 shared_ptr。</p>

<p>一般来说，这个接口是通过weak_ptr实现的：enable_shared_from_this中包含一个weak_ptr，在初始化shared_ptr时，构造函数会检测到这个该类派生于enable_shared_from_this（通过模版黑魔法很容易就能实现这个功能啦），于是将这个weak_ptr指向初始化的shared_ptr。调用shared_from_this，本质上就是weak_ptr的一个lock操作：</p>

<pre><code class="language-cpp">class A : enable_shared_from_this&lt;A&gt; {
    // ......
};
int main() {
    std::shared_ptr&lt;A&gt; pa = std::make_shared&lt;A&gt;();
    std::shared_ptr&lt;A&gt; pgood = pa-&gt;shared_from_this();
    return 0;
}
</code></pre>

<h2 id="toc_7">错误用法3：直接用new构造多个shared_ptr作为实参</h2>

<p>之前提到的C++异常处理机制，让我们可以很容易发现下面的代码有内存泄漏的危险：</p>

<pre><code class="language-cpp">// 声明
void f(A *p1, B *p2);
// 使用
f(new A, new B);
</code></pre>

<p>假如new A先于new B发生（我说&quot;假如&quot;，是因为C++的函数参数的计算顺序是不确定的），那么如果new B抛出异常，那么new A分配的内存将会发生泄漏。作为一个刚学会shared_ptr的优秀程序员，我们可以如此&quot;解决&quot;该问题：</p>

<pre><code class="language-cpp">// 声明
void f(shared_ptr&lt;A&gt; p1, shared_ptr&lt;B&gt; p2);
// 使用
f(shared_ptr&lt;A&gt;(new A), shared_ptr&lt;B&gt;(new B));
</code></pre>

<p>可惜，这么写依然有可能发生内存泄漏，因为两个shared_ptr的构造有可能发生在new A与new B之后，这涉及到C++里称作sequence after，或sequence point的性质，该性质保证：</p>

<ol>
<li>new A在<code>shared_ptr&lt;A&gt;</code>构造之前发生</li>
<li>new B在<code>shared_ptr&lt;B&gt;</code>构造之前发生</li>
<li>两个shared_ptr的构造在f调用之前发生</li>
</ol>

<p>在满足以上三条性质的前提下，各操作可以以任意顺序执行。详情请见Herb Shutter的文章：Exception-Safe Function Calls 。</p>

<h2 id="toc_8">make_shared</h2>

<p>若我们这么调用f：</p>

<pre><code class="language-cpp">f(make_shared&lt;A&gt;(), make_shared&lt;B&gt;());
</code></pre>

<p>那么就不可能发生内存泄漏了，原因依然是sequence after性质。sequence after性质保证，如果两个函数的执行顺序不确定（如本例，作为另一个函数的两个参数），那么在一个函数执行时，另一个不会执行（倘若参数是１＋１和3 + 3*6这种表达式，那么加法和乘法甚至允许交错执行，sequence after性质真是有够复杂），于是，如果<code>make_shared&lt;A&gt;</code>构造完成了，<code>make_shared&lt;B&gt;</code>中抛出异常，那么A的资源能被正确释放。与上面用new来初始化的情形对比，make_shared保证了第二new发生的时候，第一个new所分配的资源已经被shared_ptr管理起来了，故在异常发生时，能正确释放资源。</p>

<p>一句话建议：总是使用make_shared来生成shared_ptr！</p>

<h2 id="toc_9">结论</h2>

<p>用shared_ptr，不用new<br/>
使用weak_ptr来打破循环引用<br/>
用make_shared来生成shared_ptr<br/>
用enable_shared_from_this来使一个类能获取自身的shared_ptr</p>

]]></content>
  </entry>
  
</feed>
