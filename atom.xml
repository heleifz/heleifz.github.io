<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Helei's Tech Notes]]></title>
  <link href="http://heleifz.github.io/atom.xml" rel="self"/>
  <link href="http://heleifz.github.io/"/>
  <updated>2016-08-24T23:43:09+08:00</updated>
  <id>http://heleifz.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im">MWeb</generator>

  
  <entry>
    <title type="html"><![CDATA[EM 算法解释]]></title>
    <link href="http://heleifz.github.io/14702353940825.html"/>
    <updated>2016-08-03T22:43:14+08:00</updated>
    <id>http://heleifz.github.io/14702353940825.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">原因</h2>

<h2 id="toc_1">框架</h2>

<h2 id="toc_2">几种实现</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache 配置文件解释]]></title>
    <link href="http://heleifz.github.io/14698086378177.html"/>
    <updated>2016-07-30T00:10:37+08:00</updated>
    <id>http://heleifz.github.io/14698086378177.html</id>
    <content type="html"><![CDATA[
<p>我是业余 Web 开发，平时主要是做算法，大概每隔几个月会搭一个网站。服务器用的是 Apache，每次配置的时候，都奉行拿来主义，从别的地方把 <strong>httd.conf</strong> 以及 <strong>.htaccess</strong> 文件内容复制过来，对配置只知道个大概，没有深入学习。</p>

<p>周末做新项目的时候，专门去 Apache 官网阅读了一下 <a href="http://httpd.apache.org/docs/2.4/">User Guide</a>，发现还是很好懂的，以前很多的疑惑都解开了。这篇文章我整理一下 Apache 配置架构，给以后的我复习用。</p>

<h2 id="toc_0">Apache 配置架构</h2>

<p>Apache 本身的架构是一个核心＋外围的 Module，它的配置也遵循这个结构，本文介绍 Apache 的核心配置，以及 <strong>mod_wsgi</strong> 和 <strong>mod_rewrite</strong> 两个模块。</p>

<h3 id="toc_1">基本语法</h3>

<p>Apache 的配置位于 <strong>/etc/apache2/httpd.conf</strong> 或者 <strong>apache2.conf</strong>  之类的地方，具体路径各个平台不同。每条配置都是一个指令 (directive) :</p>

<blockquote>
<p>指令名(如Listen) 配置值(如80)</p>
</blockquote>

<p>利用 <strong>Include</strong> 指令，配置文件还可以包含其它的配置文件或文件夹。</p>

<h3 id="toc_2">作用域（Directory, Location, VirtualHost, .htaccess）</h3>

<p>大部分指令都可以作用于某个特定的路径，这个路径可以用本机文件系统路径表示(Directory)，也可以用网络位置表示(Location)，这两种表示方式很多时候可以互换，不过最好用本机路径，这样安全一点，因为多个 <strong>Location</strong> 都可以到达同一个位置。</p>

<p>Directory 的配置是递归传播的。</p>

<p>作用域上的配置是经过 merge 后生效的，merge 的优先级是：<br/>
1. &lt;Directory&gt; (except regular expressions) and .htaccess done simultaneously (with .htaccess, if allowed, overriding &lt;Directory&gt;)<br/>
2. &lt;DirectoryMatch&gt; (and &lt;Directory &quot;~&quot;&gt;)<br/>
3. &lt;Files&gt; and &lt;FilesMatch&gt; done simultaneously<br/>
4. &lt;Location&gt; and &lt;LocationMatch&gt; done simultaneously<br/>
5. &lt;If&gt;<br/>
配置按照出现的顺序处理，不过对于 Directory，处理顺序是 <strong>最短-&gt;最长</strong>，例如，<code>&lt;Directory &quot;/var/web/dir&quot;&gt;</code> 会先于 <code>&lt;Directory &quot;/var/web/dir/subdir&quot;&gt;</code> 处理，如果多个 Directory 指令作用于同一路径，那么后处理的会覆盖到前处理的上面。<strong>(Apache有些逻辑是 first match，例如 Alias，有些是后面覆盖前面）</strong></p>

<p>如果一个 Directory 设定了 <code>AllowOverride All</code>，那么它还可以用 .htaccess 中的配置来覆盖。</p>

<p>VirtualHost 根据访问者使用的域名(name based)或者访问的 IP 地址(IP based)，使用特定的配置，这样可以实现一个服务器 Host 多个网站。VirtualHost 的配置会覆盖全局配置。</p>

<h2 id="toc_3">mod_wsgi</h2>

<p>mod_wsgi 通过 WSGIScriptAlias 这个指令，把某些指令路由给 Python 脚本处理，和 Alias 一样，第一个 Match 的生效。</p>

<p><a href="http://ssmax.net/archives/977.html">mod_wsgi的两种工作模式</a></p>

<h2 id="toc_4">mod_rewrite</h2>

<pre><code class="language-apache">&lt;IfModule mod_rewrite.c &gt;
    RewriteEngine On
    # 拼 URL 时使用的前缀
    RewriteBase /
    # test string 可以用一堆常量，例如 %{REQUEST_URI} 
    # CondPattern 可以是正则表达式，也可以是一些其它的 test，例如文件是否存在
    # 默认是 AND，可以换成 [OR]
    RewriteCond TestString CondPattern [OR]
    # 支持 backref
    RewriteRule Pattern Substitution [flags]
&lt;/IfModuel&gt;
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SVM 推导]]></title>
    <link href="http://heleifz.github.io/14698080869715.html"/>
    <updated>2016-07-30T00:01:26+08:00</updated>
    <id>http://heleifz.github.io/14698080869715.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1. Intuition</h2>

<p>（每个）样本点距离分类面越远，说明分类面选得越好。为了度量这一点，我们引入<strong>函数间隔</strong>和<strong>几何间隔</strong>。这里的间隔（margin）指的是点与分类面的间隔。</p>

<h3 id="toc_1">函数间隔</h3>

<ul>
<li><p>单个样本<br/>
\[ \hat{\gamma}_i=y_i(\omega^Tx+b) \]</p></li>
<li><p>样本集<br/>
\[ \hat{\gamma}=\min_{1..m}\hat{\gamma}_i \]</p></li>
<li><p>性质<br/>
由于没有对 w 和 b 进行归一化，函数间隔在两者 scale 时候，将会成倍变化。</p></li>
</ul>

<h3 id="toc_2">几何间隔</h3>

<ul>
<li><p>单个样本<br/>
\[ \gamma_i = \hat{\gamma}_i / ||\omega||\]</p></li>
<li><p>样本集<br/>
\[ \gamma=\min_{1..m}\gamma_i \]</p></li>
<li><p>性质<br/>
对 w 进行了归一化，这样，在最优化的时候，我们可以针对 w 的长度施加各种限制，但不影响最终结果。</p></li>
</ul>

<h2 id="toc_3">2. 最优化问题</h2>

<h3 id="toc_4">问题化简</h3>

<ol>
<li><p>基础形态<br/>
\[ \max_{\gamma,\omega,b}\gamma \\ y_i(\omega^tx+b)\ge\gamma \\ ||w||=1\]<br/>
<em>最优化条件非凸</em></p></li>
<li><p>基础形态2<br/>
\[ \max_{\hat{\gamma},\omega,b}\hat{\gamma}/||w|| \\ y_i(\omega^tx+b)\ge\hat{\gamma} \]<br/>
<em>最优化目标非凸</em></p></li>
<li><p>QP形态（施加限制，函数间隔为1，且最大变最小）<br/>
\[ \min_{\omega,b}1/2||w||^2 \\ y_i(\omega^tx+b)\ge1 \]<br/>
<em>二次规划（凸）</em></p></li>
</ol>

<h3 id="toc_5">对偶问题</h3>

<p>对于最优化问题：</p>

<p>\[ \min_\omega f(\omega) \\ g_i(\omega)\le0\ (i=1...k) \\ h_i(\omega)=0\ (i=1..l)\]</p>

<p>有<strong>广义拉格朗日函数</strong>:</p>

<p>\[ L(\omega,\alpha,\beta) = f(\omega)+\Sigma\alpha_ig_i(\omega) + \Sigma\beta_ih_i(\omega) \]</p>

<p>其中要求 alpha 大等于0（显然，否则 Primal 问题不成立）</p>

<ul>
<li><p>Primal 问题 (p*)<br/>
\[ \min_\omega\max_{\alpha,\beta}L(\omega,\alpha,\beta) \]<br/>
一旦 w 超出限定范围，内层的 max 立马飙到正无穷</p></li>
<li><p>Dual 问题 (d*)<br/>
\[ \max_{\alpha,\beta}\min_\omega L(\omega,\alpha,\beta) \]</p></li>
<li><p>两者关系<br/>
\[ d^*\le p^*\]<br/>
根据alpha,beta,g,h的取值范围，在可行域内，拉格朗日函数必须小等于f(w)，自然也小等于p*，无论它怎么min，max，依然小等于。</p></li>
</ul>

<p><strong>在f,g是凸函数，h是仿射函数（wx+b)，且g包围的区域不为空的时候</strong></p>

<p>w*是原问题的解，alpha*，beta*是对偶问题的解，且<br/>
\[ d^*=p^*=L(\omega^*,\alpha^*,\beta^*)\]</p>

<p>此时，有KKT条件成立（充要？）</p>

<p>\[  \frac{\partial}{\partial \omega_i}L(\omega^*,\alpha^*,\beta^*)=0 \\<br/>
    \frac{\partial}{\partial \beta_i}L(\omega^*,\alpha^*,\beta^*)=0 \\<br/>
    \alpha^*_ig_i(\omega^*)=0 \\<br/>
    g_i(\omega^*)\le0 \\<br/>
    \alpha^*\ge0<br/>
\]</p>

<h2 id="toc_6">3. 将QP转换为Dual形式</h2>

<h3 id="toc_7">重写限制条件</h3>

<p>\[ g_i(\omega)=1-y_i(\omega^Tx_i+b)\le0\]<br/>
性质：根据KKT条件，<strong>当alpha_i大于0时，g_i(w)=0</strong>，此时xi是支撑矢量。</p>

<h3 id="toc_8">构造广义拉格朗日函数</h3>

<p>\[ L(\omega,b,\alpha)=1/2||w||^2+\Sigma\alpha_ig_i(\omega)\\<br/>
= 1/2||w||^2+\Sigma_{i=1..m}\alpha_i[1-y_i(\omega^Tx_i+b)]\\<br/>
= 1/2||w||^2+\Sigma\alpha_i-\Sigma\alpha_iy_i\omega^Tx_i-\Sigma\alpha_i y_i b<br/>
\]</p>

<h3 id="toc_9">对于 w 求最小值</h3>

<p>令<br/>
\[<br/>
\frac{\partial}{\partial\omega}L(\omega,b,\alpha)\\<br/>
=\omega-\Sigma\alpha_iy_ix_i=0<br/>
\]<br/>
所以有：<br/>
\[<br/>
\omega^*=\Sigma\alpha_iy_ix_i<br/>
\]<br/>
将其代入到拉格朗日函数中：<br/>
\[<br/>
L(\omega^*,b,\alpha)=1/2(\Sigma\alpha_iy_ix_i^T)(\Sigma\alpha_iy_ix_i)+\Sigma\alpha_i-\Sigma\alpha_iy_i(\Sigma\alpha_iy_ix_i^T)x_i-\Sigma\alpha_iy_ib\\<br/>
=1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i-\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j-\Sigma\alpha_iy_ib<br/>
\\<br/>
=-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i-\Sigma\alpha_iy_ib<br/>
\]</p>

<h3 id="toc_10">对于 b 求最小值</h3>

<p>令<br/>
\[<br/>
\frac{\partial}{\partial b}L(\omega,b,\alpha)\\<br/>
=-\Sigma a_iy_i=0<br/>
\]<br/>
带入到上一步中，消掉最后一项，得到：<br/>
\[<br/>
L(\omega^*,b,\alpha)=-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i<br/>
\]</p>

<h3 id="toc_11">得到对偶问题</h3>

<p>整理得到以下最终的最优化问题：</p>

<p>\[<br/>
\max_\alpha-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i\\<br/>
\alpha_i\ge0\ (i=1..m)\\<br/>
\Sigma a_iy_i=0\ (i=1..m)<br/>
\]</p>

<p>在得到alpha后，w可以根据上面的推导得到，对于b，在w得到后，将正负样本中最接近超平面的两个加起来取平均（根据图示可以很容易看出来）</p>

<p>\[<br/>
b^*=-\frac{\min{\omega^*}^Tx_++\max{\omega^*}^Tx_-}{2}<br/>
\]</p>

<p>在预测时，我们可以不用 w，而是采用如下的公式（kernel trick的基础）</p>

<p>\[<br/>
\omega^Tx+b=\Sigma\alpha_iy_i(x_i^Tx)+b<br/>
\]</p>

<p>因为只有支撑矢量的alpha才非零，所以，我们只需要保存支撑矢量（和b），就能进行分类。</p>

<h2 id="toc_12">4. 核函数</h2>

<p>给定一个feature mapping（x-&gt;phi(x)），核函数K定义为：</p>

<p>\[<br/>
K(x,z)=\phi(x)^T\phi(z)<br/>
\]</p>

<p>可以看出，当x和z相近时，核函数取值越大，x和z越相似。我们可以直接找核函数，而不用找到显式的 feature mapping，为了做到这一点，必须规定核函数所需要满足的条件。</p>

<h3 id="toc_13">Mercer定理</h3>

<p>给定函数K，对于任意m个向量，它们构成的核矩阵对称半正定 &lt;=&gt; K是一个核函数</p>

<h2 id="toc_14">5. 软间隔策略</h2>

<p>为了处理线性不可分情况，SVM可以改写为：</p>

<p>\[<br/>
\min_{\omega,b}1/2||w||^2 + C\Sigma\xi_i\\<br/>
y_i(\omega^tx+b)\ge1-\xi_i\ (i=1..m)\\<br/>
\xi_i\ge0 (i=1..m)<br/>
\]</p>

<p>化简成对偶问题后，和之前的形式惊人相近，除了alpha的范围</p>

<p>\[<br/>
\max_\alpha-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i\\<br/>
C\ge\alpha_i\ge0\ (i=1..m)\\<br/>
\Sigma a_iy_i=0\ (i=1..m)<br/>
\]</p>

<p>此时，b的计算也变了（略），且KKT条件有如下推论：</p>

<p>\[<br/>
\alpha_i=0 ⇒ y_i(\omega^Tx+b)\ge1\\<br/>
\alpha_i=C ⇒ y_i(\omega^Tx+b)\le1\\<br/>
0\le\alpha_i\le C ⇒ y_i(\omega^Tx+b)=1<br/>
\]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[shared_ptr 原理及事故]]></title>
    <link href="http://heleifz.github.io/14696398760857.html"/>
    <updated>2016-07-28T01:17:56+08:00</updated>
    <id>http://heleifz.github.io/14696398760857.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">new与赋值的坑</h2>

<p>赋值（assignment）和new运算符在C++与Java（或C#）中的行为有本质的区别。在Java中，new是对象的构造，而赋值运算是引用的传递；而在C++中，赋值运算符意味着&quot;构造&quot;，或者&quot;值的拷贝&quot;，new运算符意味着在堆上分配内存空间，并将这块内存的管理权（责任）交给用户。C++中的不少坑，就是由new和赋值引起的。</p>

<p>在C++中使用new的原因除了堆上能定义体积更大的数据结构之外，就是能使用C++中的dynamic dispatch（也叫多态）了：只有指针（和引用）才能使用虚函数来展现多态性。在这时，new出来的指针变得很像Java中的普通对象，赋值意味着引用的传递，方法调用会呈现出多态性，我们进入了面向对象的世界，一切十分美好，除了&quot;要手动释放内存&quot;。</p>

<p>在简单的程序中，我们不大可能忘记释放new出来的内存，随着程序规模的增大，我们忘了delete的概率也随之增大，这是因为C++是如此一个精神分裂的语言，赋值运算符竟然同时展现出&quot;值拷贝&quot;和&quot;引用传递&quot;两种截然不同的语义，这种不一致性导致&quot;内存泄漏&quot;成为C++新手最常犯的错误之一。当然你可以说，只要细心一点，一定能把所有内存泄漏从代码中清除。但手动管理内存更严重的问题在于，内存究竟要由谁来分配和释放呢？指针的赋值将同一对象的引用散播到程序每个角落，但是该对象的删除却只能发生一次，当你在代码中用完这么一个资源指针：resourcePtr，你敢delete它吗？它极有可能同时被多个对象拥有着，而这些对象中的任何一个都有可能在之后使用该资源，而这些对象中的另外一个，可能在它的析构函数中释放该资源。&quot;那我不delete不就行了吗？&quot;，你可能这么问，当然行， 这时候你要面对另外一种可能性：也许你是这个指针的唯一使用者，如果你用完不delete，内存就泄漏了。</p>

<p>开发者日常需要在工作中使用不同的库，而以上两种情况可能会在这些库中出现，假设库作者们的性格截然不同，导致这两个库在资源释放上采取了不同的风格，在这个时候，你面对一个用完了的资源指针，是删还是不删呢？这个问题从根本上来说，是因为C++的语言特性让人容易搞错&quot;资源的拥有者&quot;这个概念，资源的拥有者，从来都只能是系统，当我们需要时便向系统请求，当我们不需要时就让系统自己捡回去（Garbage Collector），当我们试图自己当资源的主人时，一系列坑爹的问题就会接踵而来。</p>

<h2 id="toc_1">异常安全的类</h2>

<p>我们再来看另外一个与new运算符紧密相关的问题：如何写一个异常安全（exception safe）的类。</p>

<p>异常安全简单而言就是：当你的类抛出异常后，你的程序会不会爆掉。爆掉的情况主要包括：内存泄漏，以及不一致的类状态（例如一个字符串类，它的size()方法返回的字符串大小与实际的字符串大小不同），这里仅讨论内存泄漏的情况。</p>

<p>为了让用户免去手动delete资源的烦恼，不少类库采用了RAII风格，即<em>Resource Acquisition Is Initialization</em>，这种风格采用类来封装资源，在类的构造函数中获取资源，在类的析构函数中释放资源，这个资源可以是内存，可以是一个网络连接，也可以是mutex这样的线程同步量。在RAII的感召下，我们来写这么一个人畜无害的类：</p>

<pre><code class="language-cpp">class TooSimple {
private:
    Resource *a;
    Resource *b;
public
    TooSimple() {
        a = new Resource();
        b = new Resource(); //在这里抛出异常
    }
    ~TooSimple() {
        delete a;
        delete b;
    }
};
</code></pre>

<p>这个看似简单的类，是有内存泄漏危险的哟！为了理解这一点，首先简单介绍一下C++在抛出异常时所做的事吧：</p>

<p>如果一个new操作（及其调用的构造函数）中抛出了异常，那么它分配的内存空间将自动被释放。<br/>
一个函数（或方法）抛出异常，那么它首先将当前栈上的变量全部清空(unwinding)，如果变量是类对象的话，将调用其析构函数，接着，异常来到call stack的上一层，做相同操作，直到遇到catch语句。<br/>
指针是一个普通的变量，不是类对象，所以在清空call stack时，指针指向资源的析构函数将不会调用。<br/>
根据这三条规则，我们很容易发现，如果<code>b = new Resource()</code>句抛出异常，那么构造函数将被强行终止，根据规则1，b分配的资源将被释放（假设Resource类本身是异常安全的），指针a，b从call stack上清除，由于此时构造函数还未完成，所以TooSimple的析构函数也不会被调用（都没构造完呢，现在只是一个&quot;部分初始化&quot;的对象，析构函数自然没理由被调用），a已经被分配了资源，但是call stack被清空，地址已经找不到了，于是delete永远无法执行，于是内存泄漏发生了。</p>

<p>这个问题有一个很直接的&quot;解决&quot;方案，那就是把<code>b = new Resource()</code>包裹在一个try-catch块中，并在catch里将执行delete a，这样做当然没问题，但我们的代码逻辑变得复杂了，且当类需要分配的资源种类增多的时候，这种处理办法会让程序的可读性急剧下降。这时候我们不禁想：要是指针变量能像类对象一样地&quot;析构&quot;就好了，一旦指针具有类似析构的行为，那么在call stack被清空时，指针会在&quot;析构&quot;时实现自动的delete。怀着这种想法，我们写了这么一个类模版：</p>

<pre><code class="language-cpp">template &lt;typename T&gt;
class StupidPointer {
public:
    T *ptr;
    StupidPointer(T *p) : ptr(p) {}
    ~StupidPointer() { delete ptr; }
};
有了这个&quot;酷炫&quot;的类，现在我们的构造函数可以这么写：
TooSimple() {
    a = StupidPointer&lt;Resource&gt;(new Resource());
    b = StupidPointer&lt;Resource&gt;(new Resource());
};
</code></pre>

<p>由于此时的a，已经不再是指针，而是<code>StupidPointer&lt;Resource&gt;</code>类，在清空call stack时，它的析构函数被调用，于是a指向的资源被释放了。但是，StupidPointer类有一个严重的问题：当多个StupidPointer对象管理同一个指针时，一个对象析构后，剩下对象中保存的指针将变成指向无效内存地址的&quot;野指针&quot;（因为已经被delete过了啊），如果delete一个野指针，电脑就会爆炸（严肃）。</p>

<p>C++11的标准库提供了两种解决问题的思路：1、不允许多个对象管理一个指针（unique_ptr）；2、允许多个对象管理同一个指针，但仅当管理这个指针的最后一个对象析构时才调用delete（shared_ptr）。这两个思路的共同点是：只！允！许！delete一次！</p>

<p>本篇文章里，我们仅讨论shared_ptr。</p>

<h2 id="toc_2">shared_ptr</h2>

<p>在将shared_ptr的使用之前，我们首先来看看它的基本实现原理。</p>

<p>刚才说到，当多个shared_ptr管理同一个指针，仅当最后一个shared_ptr析构时，指针才被delete。这是怎么实现的呢？答案是：引用计数（reference counting）。引用计数指的是，所有管理同一个裸指针（raw pointer）的shared_ptr，都共享一个引用计数器，每当一个shared_ptr被赋值（或拷贝构造）给其它shared_ptr时，这个共享的引用计数器就加1，当一个shared_ptr析构或者被用于管理其它裸指针时，这个引用计数器就减1，如果此时发现引用计数器为0，那么说明它是管理这个指针的最后一个shared_ptr了，于是我们释放指针指向的资源。</p>

<p>在底层实现中，这个引用计数器保存在某个内部类型里（这个类型中还包含了deleter，它控制了指针的释放策略，默认情况下就是普通的delete操作），而这个内部类型对象在shared_ptr第一次构造时以指针的形式保存在shared_ptr中。shared_ptr重载了赋值运算符，在赋值和拷贝构造另一个shared_ptr时，这个指针被另一个shared_ptr共享。在引用计数归零时，这个内部类型指针与shared_ptr管理的资源一起被释放。此外，为了保证线程安全性，引用计数器的加1，减1操作都是原子操作，它保证shared_ptr由多个线程共享时不会爆掉。</p>

<p>这就是shared_ptr的实现原理，现在我们来看看怎么用它吧！（超简单）</p>

<p><code>std::shared_ptr</code>位于头文件<memory>中（这里只讲C++11，boost的shared_ptr当然是放在boost的头文件中），下面我以代码示例的形式展现它的用法，具体文档可以看这里。</p>

<pre><code class="language-cpp">// 初始化
shared_ptr&lt;int&gt; x = shared_ptr&lt;int&gt;(new int); // 这个方法有缺陷，下面我会说
shared_ptr&lt;int&gt; y = make_shared&lt;int&gt;();
shared_ptr&lt;Resource&gt; obj = make_shared&lt;Resource&gt;(arg1, arg2); // arg1, arg2是Resource构造函数的参数
// 赋值
shared_ptr&lt;int&gt; z = x; // 此时z和x共享同一个引用计数器
// 像普通指针一样使用
int val = *x;
assert (x == z);
assert (y != z);
assert (x != nullptr);
obj-&gt;someMethod();
// 其它辅助操作
x.swap(z); // 交换两个shared_ptr管理的裸指针（当然，包含它们的引用计数）
obj.reset(); // 重置该shared_ptr（引用计数减1）
</code></pre>

<p>太好用了！</p>

<h2 id="toc_3">错误用法1：循环引用</h2>

<p>shared_ptr的一个最大的缺点，或者说，引用计数策略最大的缺点，就是循环引用（cyclic reference），下面是一个典型的事故现场：</p>

<pre><code class="language-cpp">class Observer; // 前向声明
class Subject {
private:
    std::vector&lt;shared_ptr&lt;Observer&gt;&gt; observers;
public:
    Subject() {}
    addObserver(shared_ptr&lt;Observer&gt; ob) {
        observers.push_back(ob);
    }
    // 其它代码
    ..........
};
class Observer {
private:
    shared_ptr&lt;Subject&gt; object;
public:
    Observer(shared_ptr&lt;Object&gt; obj) : object(obj) {}
    // 其它代码
    ...........
};
</code></pre>

<p>目标（Subject）类连接着多个观察者（Observer）类，当某个事件发生时，目标类可以遍历观察者数组observers，对每个观察者进行&quot;通知&quot;，而观察者类中，也保存着目标类的shared_ptr，这样多个观察者之间可以以目标类为桥梁进行沟通，除了会发生内存泄漏以外，这是很不错的设计模式嘛！等等，不是说用了shared_ptr管理资源后就不会内存泄漏了吗？怎么又漏了？</p>

<p>这就是引用计数模型失效的唯一的情况：循环引用。循环引用指的是，一个引用通过一系列的引用链，竟然引用回自身，上面的例子中，<code>Subject-&gt;Observer-&gt;Subject</code>就是这么一条环形的引用链。假设我们的程序中只有一个变量<code>shared_ptr&lt;Subject&gt; p</code>，此时，p指向的对象不仅通过该shared_ptr引用自己，还通过它包含的Observer中的object成员变量引用回自己，于是它的引用计数是2，每个Observer的引用计数都是1。当p析构时，它的引用计数减1，变成2-1=1（大于0!），p指向对象的析构函数将不会被调用，于是p和它包含的每个Observer对象在程序结束时依然驻留在内存中没被delete，形成内存泄漏。</p>

<h2 id="toc_4">weak_ptr</h2>

<p>为了解决这一问题，标准库提供了<code>std::weak_ptr</code>（弱引用），它也位于<memory>中。</p>

<p>weak_ptr是shared_ptr的&quot;观察者&quot;，它与一个shared_ptr绑定，但却不参与引用计数的计算，在需要时，它还能摇身一变，生成一个与它所&quot;观察&quot;的shared_ptr共享引用计数器的新shared_ptr。总而言之，weak_ptr的作用就是：在需要时变出一个shared_ptr，在其它时候不干扰shared_ptr的引用计数。</p>

<p>在上面的例子中，我们只需简单地将Observer中object成员的类型换成<code>std::weak_ptr&lt;Subject&gt;</code>即可解决内存泄漏的问题，此刻（接着上面的例子），p指向对象的引用计数为1，所以在p析构时，Subject指针将被delete，其中包含的observers数组在析构时，内部的Observer对象的引用计数也将变为0，故它们也被delete了，资源释放得干干净净。</p>

<p>下面，是weak_ptr的使用方法：</p>

<pre><code class="language-cpp">std::shared_ptr&lt;int&gt; sh = std::make_shared&lt;int&gt;();
// 用一个shared_ptr初始化
std::weak_ptr&lt;int&gt; w(sh);
// 变出shared_ptr
std::shared_ptr&lt;int&gt; another = w.lock();
// 判断weak_ptr所观察的shared_ptr的资源是否已经释放
bool isDeleted = w.expired();
</code></pre>

<h2 id="toc_5">错误用法2：多个无关的shared_ptr管理同一裸指针</h2>

<p>考虑下面这个情况：</p>

<pre><code class="language-cpp">int *a = new int;
std::shared_ptr&lt;int&gt; p1(a);
std::shared_ptr&lt;int&gt; p2(a);
</code></pre>

<p>p1和p2同时管理同一裸指针a，与之前的例子不同的是，此时的p1和p2有着完全独立的两个引用计数器（初始化p2时，用的是裸指针a，于是我们没有任何办法获取p1的引用计数！），于是，上面的代码会导致a被delete两次，分别由p1和p2的析构导致，电脑再一次爆炸了。</p>

<p>为了避免这种情况的发生，我们永远不要将new用在shared_ptr构造函数参数列表以外的地方，或者干脆不用new，改用make_shared。</p>

<p>即便我们的程序严格采取上述做法，C++还提供另外一种绕过shared_ptr，直接获取裸指针的方式，那就是this指针。请看下面的事故现场：</p>

<pre><code class="language-cpp">class A {
public:
    std::shared_ptr&lt;A&gt; getShared() {
        return std::shared_ptr&lt;A&gt;(this);
    }
};
int main() {
    std::shared_ptr&lt;A&gt; pa = std::make_shared&lt;A&gt;();
    std::shared_ptr&lt;A&gt; pbad = pa-&gt;getShared();
    return 0;
}
</code></pre>

<p>在此次事故中，pa和pbad拥有各自独立的引用计数器，所以程序将发生相同的&quot;delete野指针&quot;错误。总而言之，管理同一资源的shared_ptr，只能由同一个初始shared_ptr通过一系列赋值或者拷贝构造途径得来。更抽象的说，管理同一资源的shared_ptr的构造顺序，必须是一个无环有向的连通图，无环能够保证没有循环引用，连通性能够保证每个shared_ptr都来自于相同的源。</p>

<p>另外，标准库提供了一种特殊的接口，来解决&quot;生成this指针的shared_ptr&quot;的问题。</p>

<h2 id="toc_6">enable_shared_from_this</h2>

<p>enable_shared_from_this是标准库中提供的接口（一个基类啦）：</p>

<pre><code class="language-cpp">template&lt;typename T&gt;
class enable_shared_from_this {
public:
    shared_ptr&lt;T&gt; shared_from_this();
}
</code></pre>

<p>如果想要一个由shared_ptr管理的类A对象能够在方法内部得到this指针的shared_ptr，且返回的shared_ptr和管理这个类的shared_ptr共享引用计数，只需让这个类派生自<code>enable_shared_from_this&lt;A&gt;</code>即可，之后调用<code>shared_from_this()</code>即可获得正确的 shared_ptr。</p>

<p>一般来说，这个接口是通过weak_ptr实现的：enable_shared_from_this中包含一个weak_ptr，在初始化shared_ptr时，构造函数会检测到这个该类派生于enable_shared_from_this（通过模版黑魔法很容易就能实现这个功能啦），于是将这个weak_ptr指向初始化的shared_ptr。调用shared_from_this，本质上就是weak_ptr的一个lock操作：</p>

<pre><code class="language-cpp">class A : enable_shared_from_this&lt;A&gt; {
    // ......
};
int main() {
    std::shared_ptr&lt;A&gt; pa = std::make_shared&lt;A&gt;();
    std::shared_ptr&lt;A&gt; pgood = pa-&gt;shared_from_this();
    return 0;
}
</code></pre>

<h2 id="toc_7">错误用法3：直接用new构造多个shared_ptr作为实参</h2>

<p>之前提到的C++异常处理机制，让我们可以很容易发现下面的代码有内存泄漏的危险：</p>

<pre><code class="language-cpp">// 声明
void f(A *p1, B *p2);
// 使用
f(new A, new B);
</code></pre>

<p>假如new A先于new B发生（我说&quot;假如&quot;，是因为C++的函数参数的计算顺序是不确定的），那么如果new B抛出异常，那么new A分配的内存将会发生泄漏。作为一个刚学会shared_ptr的优秀程序员，我们可以如此&quot;解决&quot;该问题：</p>

<pre><code class="language-cpp">// 声明
void f(shared_ptr&lt;A&gt; p1, shared_ptr&lt;B&gt; p2);
// 使用
f(shared_ptr&lt;A&gt;(new A), shared_ptr&lt;B&gt;(new B));
</code></pre>

<p>可惜，这么写依然有可能发生内存泄漏，因为两个shared_ptr的构造有可能发生在new A与new B之后，这涉及到C++里称作sequence after，或sequence point的性质，该性质保证：</p>

<ol>
<li>new A在<code>shared_ptr&lt;A&gt;</code>构造之前发生</li>
<li>new B在<code>shared_ptr&lt;B&gt;</code>构造之前发生</li>
<li>两个shared_ptr的构造在f调用之前发生</li>
</ol>

<p>在满足以上三条性质的前提下，各操作可以以任意顺序执行。详情请见Herb Shutter的文章：Exception-Safe Function Calls 。</p>

<h2 id="toc_8">make_shared</h2>

<p>若我们这么调用f：</p>

<pre><code class="language-cpp">f(make_shared&lt;A&gt;(), make_shared&lt;B&gt;());
</code></pre>

<p>那么就不可能发生内存泄漏了，原因依然是sequence after性质。sequence after性质保证，如果两个函数的执行顺序不确定（如本例，作为另一个函数的两个参数），那么在一个函数执行时，另一个不会执行（倘若参数是１＋１和3 + 3*6这种表达式，那么加法和乘法甚至允许交错执行，sequence after性质真是有够复杂），于是，如果<code>make_shared&lt;A&gt;</code>构造完成了，<code>make_shared&lt;B&gt;</code>中抛出异常，那么A的资源能被正确释放。与上面用new来初始化的情形对比，make_shared保证了第二new发生的时候，第一个new所分配的资源已经被shared_ptr管理起来了，故在异常发生时，能正确释放资源。</p>

<p>一句话建议：总是使用make_shared来生成shared_ptr！</p>

<h2 id="toc_9">结论</h2>

<p>用shared_ptr，不用new<br/>
使用weak_ptr来打破循环引用<br/>
用make_shared来生成shared_ptr<br/>
用enable_shared_from_this来使一个类能获取自身的shared_ptr</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git]]></title>
    <link href="http://heleifz.github.io/14696393471602.html"/>
    <updated>2016-07-28T01:09:07+08:00</updated>
    <id>http://heleifz.github.io/14696393471602.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1. 对象模型</h2>

<ul>
<li>Git由一堆对象组成，每个对象都有一个SHA1的40位编码</li>
<li>每个文件都是一个blob，由内容进行hash，同样内容的文件在版本库内<strong>只有一份</strong>。</li>
<li>每个目录都是一个tree，tree中可以有tree也可以有blob。（文件名等信息在tree中保存）</li>
<li>每个commit都指向一个tree，一个或多个parent，包含作者，email，日期，消息等数据。（tag就是一个commit的引用而已）</li>
<li><strong>Git Repo就是由commit组成的网络</strong>。</li>
</ul>

<blockquote>
<p>既然如此，我们就得找到表示commit的方式</p>
</blockquote>

<h2 id="toc_1">2. Commit的表示法</h2>

<h3 id="toc_2">基础</h3>

<ul>
<li>branchname : 访问branchname最新的commit</li>
<li>tagname : 访问tagname引用的commit</li>
<li>HEAD</li>
<li>完整的hash</li>
<li>hash的前几位（7位一般就够了）</li>
</ul>

<h3 id="toc_3">组合</h3>

<ul>
<li>name^ : name的parent commit</li>
<li>name<sup>^</sup> : parent的parent</li>
<li>name<sup>n</sup> : 第 n 个 parent（在merge的情况下）</li>
<li>name~n : 倒数第 n 个 parent</li>
<li>name:path : commit中的文件</li>
</ul>

<h3 id="toc_4">范围</h3>

<ul>
<li>name1..name2 : name2能回溯到的，但是name1不能回溯到的（master..，相当于 mater..HEAD，当前分支的修改）</li>
<li>name1...name2 : name1的和name2的祖先，但不是共同祖先</li>
</ul>

<h3 id="toc_5">描述</h3>

<pre><code class="language-bash">--since=&quot;two weeks ago&quot;
--until=&quot;1 week ago&quot;
--grep=pattern
--committer=pattern
--author=pattern
--no-merges
</code></pre>

<h2 id="toc_6">3. 三个位置</h2>

<h3 id="toc_7">Working Tree</h3>

<p>当前的工作目录，包含文件。<code>git add</code>以后，进入 index</p>

<h3 id="toc_8">Index</h3>

<p>暂存区。所谓 index 是指，暂存区里包含了各种新增 object 的引用（索引）。<code>git commit</code>以后，进入 branch。</p>

<h3 id="toc_9">Branch</h3>

<p>当前的分支。</p>

<h2 id="toc_10">4. 部分操作</h2>

<h3 id="toc_11"><code>config</code></h3>

<p>在使用 git 前必须设置好名字和邮箱，如果不用 global 参数，则是设置项目专用的作者信息。</p>

<pre><code class="language-bash">$ git config --global user.name &#39;Your Name&#39;
$ git config --global user.email you@somedomain.com
</code></pre>

<h3 id="toc_12"><code>add</code></h3>

<p>将文件从 working tree 添加到 index<br/>
<code>bash<br/>
git add &lt;path&gt; # 新增，修改的文件（文件夹）<br/>
git add -u &lt;path&gt; # 修改，删除的文件（文件夹）<br/>
git add -A &lt;path&gt; # 新增，修改，删除的文件（文件夹）<br/>
</code></p>

<h3 id="toc_13"><code>commit</code></h3>

<p>将 index 的 tree 作为下一个 commit，加入当前 branch。<br/>
<code>bash<br/>
git commit -m &#39;my commit message..&#39;<br/>
git commit -a # 先 add 再 commit （-am)<br/>
git commit --amend -m &quot;correct some error&quot; # amend产生新的ID，请不要amend和别人共享的commit<br/>
</code></p>

<h3 id="toc_14"><code>diff</code></h3>

<pre><code class="language-bash">git diff # unstaged change
git diff --cached # difference between index and branch
git diff HEAD # diff between working tree + index and branch
</code></pre>

<h3 id="toc_15"><code>checkout</code></h3>

<pre><code class="language-bash"># 注：忽略未commit的文件，--force参数可以暴力覆盖

git checkout &lt;branch&gt;  # 更新三棵树和HEAD
git checkout &lt;commit&gt;  # 更新HEAD和working tree
git checkout           # 用 last commit 或 index 来复位 working tree
</code></pre>

<h3 id="toc_16"><code>reset</code></h3>

<p>reset 指令不是单纯修改HEAD（与checkout不同），而是先修改branch，然后把HEAD指向新的branch。</p>

<pre><code class="language-bash">git reset --soft  # 只修改 branch
git reset --mixed # 默认，修改 branch + index 
git reset --hard  # 修改 branch + index + working tree
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Text Rank 算法应用]]></title>
    <link href="http://heleifz.github.io/14696391901766.html"/>
    <updated>2016-07-28T01:06:30+08:00</updated>
    <id>http://heleifz.github.io/14696391901766.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[神经网络计算模型 - 上篇：理论解释]]></title>
    <link href="http://heleifz.github.io/14696391071598.html"/>
    <updated>2016-07-28T01:05:07+08:00</updated>
    <id>http://heleifz.github.io/14696391071598.html</id>
    <content type="html"><![CDATA[
<p>传统机器学习教科书中的神经网络通常是简单的多层感知机，采用全联接层作为隐层，并经过一个 softmax 输出，现代的神经网络架构脱胎于此，却早已脱离这样的简单模型，无论是 <a href="http://caffe.berkeleyvision.org">Caffe</a> 还是 <a href="http://www.deeplearning.net/software/theano/">Theano</a>，都具有可定制，可扩展的优点，允许用户自行搭建符合需求的网络架构和运算。</p>

<p>本文从 <em>数据的抽象</em>，<em>运算的抽象</em> 两个角度介绍现代神经网络的计算模型。</p>

<h2 id="toc_0">用 Tensor 抽象数据</h2>

<p>传统机器学习模型严重依赖于 <a href="http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf">feature engineering</a>，所以模型的输入一般是计算好的特征向量，但是基于神经网络的机器学习系统利用中间层自动得到合适的特征，所以数据往往以更为原始而稠密的形式输入网络（图像，音视频），并且在网络内部保持这种 N 维数组的结构向后传播，这种 N 维数组也叫 tensor。</p>

<p>采用 tensor 或向量在计算上并无本质区别，将 tensor 拉伸就变成向量，如下图所示，假设输入标量为 \(z\) ，相对于 tensor \(X\) 计算梯度时<br/>
\[<br/>
\frac {\partial z}{\partial X_{i,j,k} } = \frac {\partial z}{\partial 向量化(X)_n }<br/>
\]<br/>
其中 \(n\) 为 \( X_{i,j,k} \) 在 \( X \) 向量化后所在的位置。Tensor 的好处是降低了用户的思维负担，保持了数据的原始结构，目前所有的神经网络工具都使用 tensor 来存储数据。</p>

<p><img src="media/14696391071598/tensor.png" alt="tenso" style="width:513px;"/></p>

<p>使用向量的好处是方便写公式，后文中 \( X_i \) 表示 tensor \( X \) 以某种方式拉伸成向量后的第 \( i \) 个元素。</p>

<h2 id="toc_1">由运算构成网络</h2>

<p>运算（operation）就是函数，一个运算接收一个或多个 tensor 作为输入，产生一个 tensor 作为输出，不同的运算的组合成完整的神经网络。</p>

<p><img src="media/14696391071598/operator.png" alt="operato"/></p>

<p>下面列举几种常见的运算，默认所有大写字母都代表 tensor：</p>

<h4 id="toc_2">1. 矩阵乘法（全联接层）</h4>

<p><strong>输入</strong>: 数据 \(X\)，参数 \(W\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 运算时将 \(X\) 拉伸为向量，假设长度为 \(n\)，输出 \(Y\) 长度为 \(m\)，那么 \(W\) 矩阵的大小为 \(m\)x\(n\)，且 \(Y=WX\)。</p>

<h4 id="toc_3">2. 卷积</h4>

<p><strong>输入</strong>: 数据 \(X\)，卷积核 \(H\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 运算时将 \(X\) 与 \(H\) 做卷积（神经网络意义下的卷积，不翻转 \(H\)），假设 \(X\) 的长宽为 \(h\)x\(w\), \(H\) 的长宽为 \(m\)x\(n\)，那么输出 \(Y\) 的长宽为 \((h-m+1)\)x\((w-n+1)\)。</p>

<h4 id="toc_4">3. 非线性变换</h4>

<p><strong>输入</strong>: 数据 \(X\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 将 \(X\) 逐元素做非线性变换（例如 sigmoid，ReLu等），输出到 \(Y\)，\(Y\)与\(X\) 形状一致。</p>

<h4 id="toc_5">4. 平方误差</h4>

<p><strong>输入</strong>: 模型输出 \(f\)，训练数据 label \(y\)<br/>
<strong>输出</strong>: \(Loss\)<br/>
<strong>说明</strong>: \(Loss=0.5*(y-f)^2 \)</p>

<h3 id="toc_6">5. 内积</h3>

<p><strong>输入</strong>: 数据 \(X\)，数据 \(Y\)<br/>
<strong>输出</strong>: \(dot(X,Y)\)<br/>
<strong>说明</strong>: \(dot(X,Y)\) 为两路输入的逐元素相乘后求和，是一个标量。</p>

<p>利用以上运算节点以及基本的算术运算，我们就可以像 <strong>搭积木</strong> 一样，构建一个经典的3层神经网络，绿色为输入，黄色为模型参数，为节约空间，运算之间的 tensor 节点被我省去了。注意到，图中我包含了一个L2-正则项。</p>

<p><img src="media/14696391071598/model.png" alt="mode"/></p>

<p>构造完运算图后，我们只需要在网络的输入端提供 tensor \(X\)，并让 \(X\) 随着网络流动 (flow)到输出层即可，我想这就是 <a href="https://www.tensorflow.org">TensorFlow</a> 这个库的命名的由来。</p>

<h2 id="toc_7">使用 backprop 计算梯度</h2>

<p>神经网络相较其它模型的优势之一，就是能够使用 backprop 算法自动且高效地计算出梯度，它本质上是一种 <strong>动态规划</strong>，遵循推导动态规划算法的一般套路，我们首先在运算图中定义梯度计算的递归关系。</p>

<h4 id="toc_8">1. 计算目标</h4>

<p>假设模型的损失函数的输出是一个标量 \(z\)（大部分情况下如此），针对我们感兴趣的参数 tensor \(W_i\) 我们想要得到它们相对于 \(z\) 的梯度<br/>
\[<br/>
\frac {\partial z}{\partial W_1 },\frac {\partial z}{\partial W_2 } ...<br/>
\]</p>

<h4 id="toc_9">2. 递归关系</h4>

<p>运算图中的梯度具有递归关系，对于下图：<br/>
<img src="media/14696391071598/simple_net.png" alt="simple_net"/></p>

<p>利用导数的链式法则，假设 \(C\) 的长度为 \(m\)，\(A\) 的长度为 \(n\)，\(z\) 对于 \(A\) 的每个元素的导数为：<br/>
\[<br/>
\frac {\partial z}{\partial A_i }＝\Sigma_{j=1..m}\frac {\partial z}{\partial C_i } \frac{\partial C_j}{\partial A_i }<br/>
\]<br/>
将 \(A_i\) 组合成 tensor，得到：<br/>
\[<br/>
\frac {\partial z}{\partial A }＝\Sigma_{j=1..m}\frac {\partial z}{\partial C_j } \frac{\partial C_j}{\partial A}<br/>
\]<br/>
其中，\(\frac {\partial z}{\partial A }\) 是 tensor 拉伸成的向量，向量的元素是 \(\frac {\partial z}{\partial A_{i=1..n} }\)。接着，使用矩阵乘法将求和符号省略，我们得到：<br/>
\[<br/>
\frac {\partial z}{\partial A }＝(\frac{\partial C}{\partial A})^T \frac {\partial z}{\partial C } <br/>
\]<br/>
其中，我们利用了 <a href="http://mathworld.wolfram.com/Jacobian.html">雅可比矩阵</a><br/>
\[<br/>
\frac{\partial C}{\partial A}=<br/>
\begin{bmatrix}<br/>
  \frac{\partial C_1}{\partial A_1} &amp; \frac{\partial C_1}{\partial A_2} &amp; ...\\<br/>
  \frac{\partial C_2}{\partial A_1} &amp; \frac{\partial C_2}{\partial A_2} &amp; ...\\<br/>
  ... &amp; ... &amp; ...<br/>
 \end{bmatrix}<br/>
\]<br/>
以上推导说明，网络前级的梯度可以由后级的梯度乘以一个雅可比矩阵得到，这个结论适用于 <strong>任意</strong> 的运算节点，这给神经网络的软件架构带来了极大的灵活性：对于用户定义的任何运算，只要正确实现了这个雅可比矩阵的乘法，就能加入到梯度计算中来，这也是 Caffe 库中的每种 Layer 只要定义 Forward 和 Backward 接口就能参与网络构建的原因。</p>

<p>当然，一个 tensor 可以被一个以上的后续节点使用，如下图：<br/>
<img src="media/14696391071598/simple_net_2.png" alt="simple_net_2"/><br/>
根据导数的性质，我们有：<br/>
\[<br/>
\frac {\partial z}{\partial C_i }＝\Sigma_{j=1..m}\frac {\partial z}{\partial A_j } \frac{\partial A_j}{\partial C_i } + \Sigma_{y=1..k}\frac {\partial z}{\partial B_y } \frac{\partial B_y}{\partial C_i }<br/>
\]<br/>
类似于上面的推导，我们得到矩阵形式的递归公式：<br/>
\[<br/>
\frac {\partial z}{\partial C }＝(\frac{\partial A}{\partial C })^T \frac {\partial z} {\partial A }  + (\frac{\partial B }{\partial C })^T \frac {\partial z}{\partial B } <br/>
\]<br/>
简而言之，从两路流过来的梯度需要加起来，得到 \(C\) 节点的梯度。</p>

<h4 id="toc_10">3. 边界情况</h4>

<p>网络的最后级节点就是在输出端 \(z\)，边界情况十分简单：<br/>
\[<br/>
\frac {\partial z}{\partial z } = 1<br/>
\]</p>

<h4 id="toc_11">4. backprop</h4>

<p>定义了递归关系和边界情况后，我们可以发现大量冗余的计算：所有流向一个节点 \(A\) 的节点，总是需要计算一遍 \(\frac {\partial z}{\partial A } \)。此时我们有两种选择，一种是将计算后的结果缓存到一张查找表里，避免重复计算，也可以调整计算顺序，从网络的末端向前计算，这就是大部分动态规划采用的 bottom-up 策略，在神经网络中，这称作反向传播（backprop）</p>

<h3 id="toc_12">常见运算节点的 backprop</h3>

<p>上面提到，无论进行什么运算，backprop 总是雅可比矩阵的转置乘以后级的梯度，但是，对于某些运算来说，这个矩阵乘法可以表现为不同的形式。</p>

<h4 id="toc_13">1. 矩阵乘法（全联接层）</h4>

<p><strong>输入</strong>: 数据 \(X\)，参数 \(W\)<br/>
<strong>输出</strong>: \(Y＝WX\)<br/>
<strong>backprop</strong>: 很容易得到<br/>
\[\frac {\partial z}{\partial X }＝W^T \frac {\partial z}{\partial Y }\\<br/>
\frac {\partial z}{\partial W }＝\frac {\partial z}{\partial Y } X^T \]</p>

<h4 id="toc_14">2. 非线性变换</h4>

<p><strong>输入</strong>: 数据 \(X\)<br/>
<strong>输出</strong>: \(Y=nonlinear(X)\)<br/>
<strong>backprop</strong>: 将 \(X\) 逐元素做非线性变换（例如 sigmoid，ReLu等），输出到 \(Y\)，\(Y\)与\(X\) 形状一致，所以雅可比矩阵是一个对角矩阵，矩阵乘法退化成逐元素的乘法。</p>

<h4 id="toc_15">3. 卷积</h4>

<p>比较麻烦，留作练习吧，推导时需要草稿纸画一画，但是最终的矩阵乘法会退化成卷积。:)</p>

<h2 id="toc_16">下篇：使用 Python 实现神经网络库</h2>

<p>有了这个理论基础，我们就可以实现一个像 Caffe 一样的神经网络架构了，在下篇中，我会使用 Python 把以上抽象结构实现，并达到以下目标：</p>

<ol>
<li>网络结构可拼装，可扩展</li>
<li>在 MNIST 库上取得不错的准确率</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WSABIE 算法解释]]></title>
    <link href="http://heleifz.github.io/14696374110477.html"/>
    <updated>2016-07-28T00:36:51+08:00</updated>
    <id>http://heleifz.github.io/14696374110477.html</id>
    <content type="html"><![CDATA[
<p><img src="media/14696374110477/1369025_221049006000_2.jpg" alt="1369025_221049006000_2" style="width:443px;"/></p>

<p>算法论文：<a href="http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf">WSABIE: Scaling Up To Large Vocabulary Image Annotation</a></p>

<h2 id="toc_0">介绍</h2>

<p>WSABIE 是一个通用的打标签算法，无论对象是图像，视频，还是文本，只要能抽取出 feature，并提供一个固定的标签集合，就可以使用 WSABIE，由于算法的通用性，下文将需要打标签的东西称为 <em>对象</em>。</p>

<h2 id="toc_1">模型</h2>

<h3 id="toc_2">1. Joint Word-Image Model</h3>

<p>对于 WSABIE 来说，打标签的过程，就是计算所有标签与当前对象的相似性，并取出相似性最高的标签作为结果。由于对象的 feature 和标签是两种不同的东西，为了计算相似性，WSABIE 将它们映射到 <strong>同一个向量空间</strong> \( R^D \) 。</p>

<p><img src="media/14696374110477/wsb-1.png" alt="wsb-1" style="width:391px;"/></p>

<p>为了将对象 \(I\) 的 feature vector \( x \) 映射到 \( R^D \)，只需做线性变换 \(  \Phi _I(x)=Vx \)，其中 \(V\) 是变换矩阵。</p>

<p>由于标签是离散的符号，为了将其映射到 \( R^D \)，需要采用 Word2Vec 一样的思想，把它们转换成 embedding，换言之，我们把所有标签（假设有 \(N\) 个)对应的向量存在矩阵 \(W\) （大小 \(D \times N\)）中，第 i 个标签就是矩阵的第 i 列： \( \Phi _W(i)=W_i \)</p>

<p>于是，对象 \( I \) 与第 i 个标签的相似度就是：</p>

<p>\[ f_i(x) = \Phi _W(i)^T \Phi _I(x)=W_i^TVx \]</p>

<p>模型中的待学习参数为线性变换矩阵 \(V\)，以及所有标签的 embedding 向量 \(W\)。</p>

<h3 id="toc_3">2. 损失函数</h3>

<p>每条训练数据都以 \( (x,y) \) 的形式存在，其中 \(x\) 是对象，\(y\) 是它唯一的一条正确的标签。</p>

<p>模型的优化目标，是让正确描述对象的标签，与对象有较高的相似度。更具体地说 \[ rank_y(f(x)) = \Sigma _{i \ne y} I(f_i(x) \ge f_y(x)) \]</p>

<p>这个式子表示 <strong>&quot;排名大于 \(y\) 的标签的个数&quot;</strong>（\(I\) 是示性函数)，这条训练数据的误差是</p>

<p>\[ L(rank_y(f(x)))\]</p>

<p>其中 </p>

<p>\[ L(k) = \Sigma _{j=1}^k \alpha_j \]</p>

<p>根据常数 \( \alpha_j \) 的取值的不同，我们可以得到不同的损失函数，所以 \( L \) 其实代表一类函数。</p>

<p>如果 \( \alpha_j \) 取值都为 \(C\)， 那么如果 \(y\) 被模型排到第 \(k\) 名，损失值就是 \(Ck\)。论文中 \( \alpha_j = 1/j \)。</p>

<h2 id="toc_4">学习</h2>

<p>WSABIE 对损失函数进行了一系列近似，让模型能够使用随机梯度下降（SGD）在线更新参数，这个学习算法是本文 <em>最有价值的部分</em>。下面依次介绍近似步骤：</p>

<h3 id="toc_5">1. 改写损失函数</h3>

<p>\[ err(f(x), y) = L(rank_y(f(x)) \frac{rank_y(f(x))}{rank_y(f(x)) }\]</p>

<p>\[ =L(rank_y(f(x)) \frac{\Sigma_{i\ne y}I(f_i(x) \ge f_y(x))}{rank_y(f(x)} \]</p>

<p>\[ =\Sigma_{i \ne y} L(rank_y(f(x)) \frac{ I(f_i(x) \ge f_y(x))}{rank_y(f(x)} \]</p>

<h3 id="toc_6">2. 近似损失函数</h3>

<p>使用 hinge loss 近似 0／1 损失函数，得到如下的近似损失：</p>

<p>\[ \overline{err}(f(x), y)=\Sigma_{i \ne y} L(rank_y^1(f(x)) \frac{ |1-f_y(x)+f_i(x)|_+}{rank_y^1(f(x))} \]</p>

<p>其中：</p>

<p>\[<br/>
rank_y^1(f(x))=\Sigma_{i\ne y}I(1+f_i(x) \ge f_y(x))<br/>
\]</p>

<p>使用这个近似的损失函数，得到在当前数据集分布的期望误差：</p>

<p>\[Risk(f)=\int\overline{err}(f(x), y)dP(x,y)\]</p>

<p>按照分布 \(P(x,y)\) 随机选择一个样本 \((x,y)\)，计算其 \(\overline{err}(f(x), y)\)，可以对这个期望误差进行估计。</p>

<p>实际上 </p>

<p>\[ \Sigma_{i\ne y}\frac{ |1-f_y(x)+f_i(x)|_+}{rank_y^1(f(x))} \]</p>

<p>这个值，也可以通过采样进行估计，方法为，从满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的所有标签中，随机采样一个 \(\overline{y}\)，计算 \( 1-f_y(x)+f_{\overline{y}}(x) \)</p>

<p>综上，总共两步采样：</p>

<ol>
<li>随机选一个 \((x,y)\) </li>
<li>随机选一个标签 \( \overline{y} \)</li>
</ol>

<p>得到 \(Risk(f)\) 的一个估计：</p>

<p>\[ \overline{err}(f(x),y,\overline{y})=L(rank_y^1(f(x))(1-f_y(x)+f_{\overline{y}}(x))\]</p>

<h3 id="toc_7">3. 随机梯度下降</h3>

<p>接下来，就应用标准的随机梯度下降法，通过对 \(Risk(f)\) 的估计 \(\overline{err}(f(x),y,\overline{y})\) 求梯度，优化损失函数：</p>

<p>\[ \beta_{t+1}=\beta{t}-\gamma_t \frac {\partial\overline{err}(f(x),y,\overline{y})} {\partial\beta_{t}}\]</p>

<p>其中 \(\beta\) 是模型参数，也就是 \(W\) 和 \(V\) 矩阵。</p>

<h3 id="toc_8">4. 进一步优化性能</h3>

<p>此时，每一步优化的性能还是不好，原因是计算 \(\overline{err}(f(x),y,\overline{y})\) 时，我们需要知道所有满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的标签，因为：</p>

<ol>
<li>计算 \(L(rank_y^1(f(x))\) 要用到（记得那个求和式子吗）</li>
<li>采样的第二步要用到（因为要从这些标签里采样）</li>
</ol>

<p>论文中第 <strong>最聪明的地方</strong> 来了，作者提出了一个近似 \(\overline{err}(f(x),y,\overline{y})\) 的方法，解决以上两个问题：</p>

<p>在第二步的采样中，不再从满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的标签里采样，而是从 <strong>除去 \(y\) 的所有</strong> 标签里进行采样，如果采样到的标签 i 不满足 \( 1-f_y(x)+f_i(x) &gt; 0 \)，就把它放回，再次采样，直到找到满足条件的标签为止。</p>

<p>表面上，这解决了第二次采样的问题，这种有放回采样的效果，等价于原来的采样，而且不需要事先算出所有满足条件的标签。</p>

<p>但实际上，这也解决了第一个问题。假设一共有 \(k\) 个标签满足  \( 1-f_y(x)+f_i(x) &gt; 0 \)，那么，从所有标签中采样到这种标签的概率就是 \(P=\frac{k}{N－1}\)，由于这种有放回采样满足 <em>几何分布</em> （还记得大学概率课吗？一直投骰子，直到遇到6），所以期望的采样次数为：</p>

<p>\[ E[采样次数] = \frac{N-1}{k} \]</p>

<p>我们可以用实际的采样次数近似期望的采样次数，所以我们可以近似得到 \(k\) 的值为 \( \frac{N-1}{采样次数} \)，于是可以算出 \(L(rank_y^1(f(x))\) 的近似值。</p>

<h2 id="toc_9">其它讨论</h2>

<p>此外，论文还对 \(W\) 和 \(V\) 每一列的大小做了约束：</p>

<p>\[ ||W||_2 &lt;= C,||V||_2 &lt;= C \]</p>

<p>在优化时，如果某一列超过了 C，就将该列的值等比例缩小一下。</p>

<p>如果理解了后续的采样步骤，就会理解对损失函数的近似步骤：</p>

<blockquote>
<p>之所以将 0/1 loss 近似为 hinge loss，是为了让损失函数连续，之所以用 \(rank_y^1(f(x),y) \) 替换 \(rank_y(f(x),y) \) ，是为了在第二步采样时去掉分母</p>
</blockquote>

<p>在训练过程的每一步，都仅涉及一个训练数据 \((x,y)\)，以及后续的几条采样数据，这种 online learning 的学习方法适用于海量数据集。</p>

<p>实际应用中，每个对象会有 <em>多个标签</em>，在训练时，可以针对每个标签单独跑一次采样，</p>

<p>此外，我还有一个想法，模型中的输入 feature 已经是计算好了的，我们也可以把原始的数据直接放入模型，例如，利用 NN 自动学习出特征，实现 end-to-end 的打标签架构。</p>

<p>算法名字很像&quot;芥末&quot;，所以文章的开头我放了芥末。</p>

]]></content>
  </entry>
  
</feed>
