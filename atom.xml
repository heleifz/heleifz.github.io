<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Helei's Tech Notes]]></title>
  <link href="http://heleifz.github.io/atom.xml" rel="self"/>
  <link href="http://heleifz.github.io/"/>
  <updated>2017-05-18T22:27:01+08:00</updated>
  <id>http://heleifz.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im">MWeb</generator>

  
  <entry>
    <title type="html"><![CDATA[如何学习新技术]]></title>
    <link href="http://heleifz.github.io/14953815607558.html"/>
    <updated>2017-05-21T23:46:00+08:00</updated>
    <id>http://heleifz.github.io/14953815607558.html</id>
    <content type="html"><![CDATA[
<p>我从 2009年～2010 年左右开始学习信息技术，从初期围绕单片机做电子设计，一路浅尝辄止，走马观花。什么语言都用过，在各种平台上都干过活，有用的没用的都学了一些，一路尝鲜一路丢。</p>

<p>虽然有识之士们都说，光靠广度是没法找到工作，要对一门手艺有深度，到达深奥幽玄的境界。我自知能力有限，更何况，工作后每日加班，精力不济，能让我一窥天道的时间窗口越来越窄了。</p>

<p>即便如此，我也不觉得自己不行，这就是愚蠢人的乐趣。因为总有与你同样愚蠢的人，他们很快乐，我为什么不能快乐。我不仅快乐，而且要分享，要写作，本文就谈谈在下走马观花的本领。</p>

<h2 id="toc_0">兴味使然</h2>

<p>首先要解决驱动力的问题，即便走马观花，心里也得有股劲，是不是？所以你得喜欢编程这门手艺，不喜欢，那至少也得好奇，当个好奇宝宝。既不喜欢，也无好奇心，自主学习就没法成立。</p>

<p>既然喜欢这门手艺，热爱这个行业，那么你自然有感兴趣的领域，或者小问题，这就是学习新技术前要做的第一件事，确定一个你感兴趣的方向。</p>

<p>有的朋友爱流行技术，因为网民们在学，所以他也想学。或者就业市场需要某个技能，所以被迫去学。这当然好啦，针对性地优化你的简历，在求职时候有用，但也仅限于此了。</p>

<p>兴味使然的学习完全不同，你能用更加轻松的心态看待每个知识点，也能在适当的时候深入挖掘一些现象的背后原因，这是一个充满快感的旅程，没有人提醒你：那里不是考点。</p>

<h2 id="toc_1">探寻入口</h2>

<p>选择了方向以后，就该选学习材料了。随处可见的大型书单其实用处有限，因为材料的准备是分阶段的，侧重点不同的，这个过程伴随着你对该领域的初步认识的建立，混杂着你的直觉，常识，以及一点点随机性。</p>

<p>首先，做一些”自由探索”，闭上眼睛，听听伟大的互联网在你耳边的低语：Google，维基百科，问答社区。从庞杂的噪声中，你需要找到以下知识点：</p>

<ol>
<li>基本定义：例如，机器学习是什么，不是什么？与相关概念（人工智能）之间的区别和联系。</li>
<li>主要工具：例如，机器学习的理论工具是什么，实践方式是什么？</li>
<li>核心问题或矛盾：例如，机器学习研究或应用中面临的基本问题是什么？</li>
</ol>

<p>同时，也要了解几个主要的资料源（每个资料源不一定都适合你），例如：</p>

<ol>
<li>coursera/edx 公开课</li>
<li>大部头教材英文版，中文版</li>
<li>由网友编写的野鸡教程</li>
</ol>

<p>基本概念的探寻，学习资料的搜索，两个过程并不相互独立。基本概念的逐步建立，能帮助你更高效地筛选资料，而好的学习资料往往会花相当的篇幅来阐明基本概念。</p>

<p>不专业的作者，要么开篇就贴代码，深入细节，要么对某项技术过渡吹捧，盲目狂热。好的学习材料也许轻松幽默，也许精确冷峻，但绝对不愚蠢。请务必花时间浏览各个教材前 5% 的内容，慢慢甄别优劣。</p>

<p>此外，好的学习资料，能够包含适当的实践内容。无论是数学题还是编程题，能否找到参考答案，是否提供代码实验环境，都是很重要的因素。</p>

<p>举几个例子，<a href="https://mitpress.mit.edu/sicp/">SICP</a>，每道题都能在 Racket 上做实验，网上也能找到<a href="http://community.schemewiki.org/?SICP-Solutions">大家的解题方法</a>；斯坦福 <a href="http://web.stanford.edu/class/cs224n/">CS224N</a>课程，完整的习题解答和项目环境；<a href="The%20Elements%20of%20Computing%20Systems%20_%20Nisan%20&amp;%20Schocken">Nand2tetris</a>，甚至有一整套硬件描述语言，虚拟机的环境，非常优秀。也有不好的例子，<a href="https://www.cs.ubc.ca/%7Emurphyk/MLbook/">MLAPP</a>，整本书都是印刷错误，习题无解答，全靠个人感觉，虽说是本好书，但阅读过程中我不停地做一些人工勘误的工作，非常痛苦，不适合入门阅读。</p>

<p>最后，很重要的一点，这个资料是否适合你当前的知识水平。太简单，以至于整本书都在印证你的想法，让你觉得它”很好读”；太难，你无法通过直觉跳过某些推导步骤，每一页的阅读都需要大量的思考，哪怕看完一整章，你也没有对这个章节的完整图景。</p>

<p>合适的难度，就是让你兴奋的难度，你能从中找到你过往的一些经验，也能看到很多全新的方法和观念，学完一章后，是否有一些愉快的想象，如果有，那么它的难度刚刚好。</p>

<h2 id="toc_2">短期专注</h2>

<p>找到资料后，不要浪费它。快速，完整地把它消化掉，不要中断，每天至少两个小时以上的专注，周末也不要停，这是我的方法。</p>

<p>有些朋友喜欢同时做几件重要的事，然后在事情之间做时间片轮转，先做做A，再做做B，这种方式有助于应对给领导长辈跑腿之类的琐事，但对学习来说效率较低。</p>

<p>在短时间内全身心沉入到单一的问题中，在这段时间里，连潜意识都在帮你工作，你不仅在记忆知识点，更是在构建对这个领域的基本直觉。混合多种学习方式也很重要，不仅有被动的听和阅读，也有主动的实验，试错，推导。前文所说的资料准备，正是为了服务这个近乎宗教仪式的过程，如果选不对资料，你就没法找到这条灵性之路。</p>

<h2 id="toc_3">自然遗忘</h2>

<p>人类会遗忘，这件事情曾让我很难过。</p>

<p>半年不接触一门手艺，就会生疏，再过半年，完全遗忘。我想把所有的理论，公式，语言，框架都记住，一有生疏，我就复习，就像抱着满满一筐苹果，一路丢，一路捡。心里疲惫，身体劳累，甚至觉得，如果最终都会遗忘，那一开始为什么要学。</p>

<p>后来才想明白，苹果框满了，我们就倒掉，再来一筐梨，桃子，火龙果，葡萄。对于学习新知识的方法论，逐年在磨练精进，虽然知识点没法记住，但对于该领域的直觉已经建立。</p>

<p>现在我把学习新知识，当时一次次旅行，其中的滋味，留给大家各自体会。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[fastText 源码分析]]></title>
    <link href="http://heleifz.github.io/14732610572844.html"/>
    <updated>2016-09-07T23:10:57+08:00</updated>
    <id>http://heleifz.github.io/14732610572844.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">介绍</h2>

<p><a href="https://github.com/facebookresearch/fastText">fastText</a> 是 facebook 近期开源的一个词向量计算以及文本分类工具，该工具的理论基础是以下两篇论文：</p>

<blockquote>
<p><a href="https://arxiv.org/pdf/1607.04606v1.pdf">Enriching Word Vectors with Subword Information</a></p>
</blockquote>

<p>这篇论文提出了用 word n-gram 的向量之和来代替简单的词向量的方法，以解决简单 word2vec 无法处理同一词的不同形态的问题。fastText 中提供了 maxn 这个参数来确定 word n-gram 的 n 的大小。</p>

<blockquote>
<p><a href="https://arxiv.org/pdf/1607.01759v2.pdf">Bag of Tricks for Efficient Text Classification</a></p>
</blockquote>

<p>这篇论文提出了 fastText 算法，该算法实际上是将目前用来算 word2vec 的网络架构做了个小修改，原先使用一个词的上下文的所有词向量之和来预测词本身（CBOW 模型），现在改为用一段短文本的词向量之和来对文本进行分类。</p>

<p>在我看来，fastText 的价值是提供了一个 <strong>更具可读性，模块化程度较好</strong> 的 word2vec 的实现，附带一些新的分类功能，本文详细分析它的源码。</p>

<h2 id="toc_1">顶层结构</h2>

<p>fastText 的代码结构以及各模块的功能如下图所示：</p>

<p><img src="media/14732610572844/fasttext-arch.png" alt="fasttext-arch"/></p>

<p>分析各模块时，我只会解释该模块的 <strong>主要调用路径</strong> 下的源码，以 <strong>注释</strong> 的方式说明，其它的功能性代码请大家自行阅读。如果对 word2vec 的理论和相关术语不了解，请先阅读这篇 <a href="http://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a>。</p>

<h2 id="toc_2">训练数据格式</h2>

<p>训练数据格式为一行一个句子，每个词用空格分割，如果一个词带有前缀“<code>__label__</code>”，那么它就作为一个类标签，在文本分类时使用，这个前缀可以通过<code>-label</code>参数自定义。训练文件支持 UTF-8 格式。</p>

<h2 id="toc_3">fasttext 模块</h2>

<p>fasttext 是最顶层的模块，它的主要功能是<code>训练</code>和<code>预测</code>，首先是<code>训练</code>功能的调用路径，第一个函数是 <code>train</code>，它的主要作用是 <strong>初始化参数，启动多线程训练</strong>，请大家留意源码中的相关部分。</p>

<pre><code class="language-cpp">void FastText::train(std::shared_ptr&lt;Args&gt; args) {
  args_ = args;
  dict_ = std::make_shared&lt;Dictionary&gt;(args_);
  std::ifstream ifs(args_-&gt;input);
  if (!ifs.is_open()) {
    std::cerr &lt;&lt; &quot;Input file cannot be opened!&quot; &lt;&lt; std::endl;
    exit(EXIT_FAILURE);
  }
  // 根据输入文件初始化词典
  dict_-&gt;readFromFile(ifs);
  ifs.close();

   // 初始化输入层, 对于普通 word2vec，输入层就是一个词向量的查找表，
   // 所以它的大小为 nwords 行，dim 列（dim 为词向量的长度），但是 fastText 用了
   // word n-gram 作为输入，所以输入矩阵的大小为 (nwords + ngram 种类) * dim
   // 代码中，所有 word n-gram 都被 hash 到固定数目的 bucket 中，所以输入矩阵的大小为
   // (nwords + bucket 个数) * dim
  input_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nwords()+args_-&gt;bucket, args_-&gt;dim);
  
  // 初始化输出层，输出层无论是用负采样，层次 softmax，还是普通 softmax，
  // 对于每种可能的输出，都有一个 dim 维的参数向量与之对应
  // 当 args_-&gt;model == model_name::sup 时，训练分类器，
  // 所以输出的种类是标签总数 dict_-&gt;nlabels()
  if (args_-&gt;model == model_name::sup) {
    output_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nlabels(), args_-&gt;dim);
  } else {
  // 否则训练的是词向量，输出种类就是词的种类 dict_-&gt;nwords()
    output_ = std::make_shared&lt;Matrix&gt;(dict_-&gt;nwords(), args_-&gt;dim);
  }
  input_-&gt;uniform(1.0 / args_-&gt;dim);
  output_-&gt;zero();

  start = clock();
  tokenCount = 0;
  
  // 库采用 C++ 标准库的 thread 来实现多线程
  std::vector&lt;std::thread&gt; threads;
  for (int32_t i = 0; i &lt; args_-&gt;thread; i++) {
    // 实际的训练发生在 trainThread 中
    threads.push_back(std::thread([=]() { trainThread(i); }));
  }
  for (auto it = threads.begin(); it != threads.end(); ++it) {
    it-&gt;join();
  }
  
  // Model 的所有参数（input_, output_）是在初始化时由外界提供的，
  // 此时 input_ 和 output_ 已经处于训练结束的状态
  model_ = std::make_shared&lt;Model&gt;(input_, output_, args_, 0);

  saveModel();
  if (args_-&gt;model != model_name::sup) {
    saveVectors();
  }
}
</code></pre>

<p>下面，我们进入 <code>trainThread</code>函数，看看训练的主体逻辑，该函数的主要工作是 <strong>实现了标准的随机梯度下降</strong>，并随着训练的进行逐步降低学习率。</p>

<pre><code class="language-cpp">void FastText::trainThread(int32_t threadId) {

  std::ifstream ifs(args_-&gt;input);
  // 根据线程数，将训练文件按照总字节数（utils::size）均分成多个部分
  // 这么做的一个后果是，每一部分的第一个词有可能从中间被切断，
  // 这样的&quot;小噪音&quot;对于整体的训练结果无影响
  utils::seek(ifs, threadId * utils::size(ifs) / args_-&gt;thread);

  Model model(input_, output_, args_, threadId);
  if (args_-&gt;model == model_name::sup) {
    model.setTargetCounts(dict_-&gt;getCounts(entry_type::label));
  } else {
    model.setTargetCounts(dict_-&gt;getCounts(entry_type::word));
  }

  // 训练文件中的 token 总数
  const int64_t ntokens = dict_-&gt;ntokens();
  // 当前线程处理完毕的 token 总数
  int64_t localTokenCount = 0;
  std::vector&lt;int32_t&gt; line, labels;
  // tokenCount 为所有线程处理完毕的 token 总数
  // 当处理了 args_-&gt;epoch 遍所有 token 后，训练结束 
  while (tokenCount &lt; args_-&gt;epoch * ntokens) {
    // progress = 0 ~ 1，代表当前训练进程，随着训练的进行逐渐增大
    real progress = real(tokenCount) / (args_-&gt;epoch * ntokens);
    // 学习率根据 progress 线性下降
    real lr = args_-&gt;lr * (1.0 - progress);
    localTokenCount += dict_-&gt;getLine(ifs, line, labels, model.rng);
    // 根据训练需求的不同，这里用的更新策略也不同，它们分别是：
    // 1. 有监督学习（分类）
    if (args_-&gt;model == model_name::sup) {
      dict_-&gt;addNgrams(line, args_-&gt;wordNgrams);
      supervised(model, lr, line, labels);
    // 2. word2vec (CBOW)
    } else if (args_-&gt;model == model_name::cbow) {
      cbow(model, lr, line);
    // 3. word2vec (SKIPGRAM)
    } else if (args_-&gt;model == model_name::sg) {
      skipgram(model, lr, line);
    }
    // args_-&gt;lrUpdateRate 是每个线程学习率的变化率，默认为 100，
    // 它的作用是，每处理一定的行数，再更新全局的 tokenCount 变量，从而影响学习率
    if (localTokenCount &gt; args_-&gt;lrUpdateRate) {
      tokenCount += localTokenCount;
      // 每次更新 tokenCount 后，重置计数
      localTokenCount = 0;
      // 0 号线程负责将训练进度输出到屏幕
      if (threadId == 0) {
        printInfo(progress, model.getLoss());
      }
    }
  }
  if (threadId == 0) {
    printInfo(1.0, model.getLoss());
    std::cout &lt;&lt; std::endl;
  }
  ifs.close();
}
</code></pre>

<blockquote>
<p><strong>一哄而上的并行训练</strong>：每个训练线程在更新参数时并没有加锁，这会给参数更新带来一些噪音，但是不会影响最终的结果。无论是 google 的 word2vec 实现，还是 fastText 库，都没有加锁。</p>
</blockquote>

<p>从 <code>trainThread</code> 函数中我们发现，实际的模型更新策略发生在 <code>supervised</code>,<code>cbow</code>,<code>skipgram</code>三个函数中，这三个函数都调用同一个 <code>model.update</code> 函数来更新参数，这个函数属于 model 模块，但在这里我先简单介绍它，以方便大家理解代码。</p>

<p>update 函数的原型为</p>

<pre><code class="language-cpp">void Model::update(const std::vector&lt;int32_t&gt;&amp; input, int32_t target, real lr)
</code></pre>

<p>该函数有三个参数，分别是“输入”，“类标签”，“学习率”。</p>

<ul>
<li>输入是一个 <code>int32_t</code>数组，每个元素代表一个词在 dictionary 里的 ID。对于分类问题，这个数组代表输入的短文本，对于 word2vec，这个数组代表一个词的上下文。</li>
<li>类标签是一个 <code>int32_t</code> 变量。对于 word2vec 来说，它就是带预测的词的 ID，对于分类问题，它就是类的 label 在 dictionary 里的 ID。因为 label 和词在词表里一起存放，所以有统一的 ID 体系。</li>
</ul>

<p>下面，我们回到 fasttext 模块的三个更新函数：</p>

<pre><code class="language-cpp">void FastText::supervised(Model&amp; model, real lr,
                          const std::vector&lt;int32_t&gt;&amp; line,
                          const std::vector&lt;int32_t&gt;&amp; labels) {
  if (labels.size() == 0 || line.size() == 0) return;
  // 因为一个句子可以打上多个 label，但是 fastText 的架构实际上只有支持一个 label
  // 所以这里随机选择一个 label 来更新模型，这样做会让其它 label 被忽略
  // 所以 fastText 不太适合做多标签的分类
  std::uniform_int_distribution&lt;&gt; uniform(0, labels.size() - 1);
  int32_t i = uniform(model.rng);
  model.update(line, labels[i], lr);
}

void FastText::cbow(Model&amp; model, real lr,
                    const std::vector&lt;int32_t&gt;&amp; line) {
  std::vector&lt;int32_t&gt; bow;
  std::uniform_int_distribution&lt;&gt; uniform(1, args_-&gt;ws);
  
  // 在一个句子中，每个词可以进行一次 update
  for (int32_t w = 0; w &lt; line.size(); w++) {
    // 一个词的上下文长度是随机产生的
    int32_t boundary = uniform(model.rng);
    bow.clear();
    // 以当前词为中心，将左右 boundary 个词加入 input
    for (int32_t c = -boundary; c &lt;= boundary; c++) {
      // 当然，不能数组越界
      if (c != 0 &amp;&amp; w + c &gt;= 0 &amp;&amp; w + c &lt; line.size()) {
        // 实际被加入 input 的不止是词本身，还有词的 word n-gram
        const std::vector&lt;int32_t&gt;&amp; ngrams = dict_-&gt;getNgrams(line[w + c]);
        bow.insert(bow.end(), ngrams.cbegin(), ngrams.cend());
      }
    }
    // 完成一次 CBOW 更新
    model.update(bow, line[w], lr);
  }
}

void FastText::skipgram(Model&amp; model, real lr,
                        const std::vector&lt;int32_t&gt;&amp; line) {
  std::uniform_int_distribution&lt;&gt; uniform(1, args_-&gt;ws);
  for (int32_t w = 0; w &lt; line.size(); w++) {
    // 一个词的上下文长度是随机产生的
    int32_t boundary = uniform(model.rng);
    // 采用词+word n-gram 来预测这个词的上下文的所有的词
    const std::vector&lt;int32_t&gt;&amp; ngrams = dict_-&gt;getNgrams(line[w]);
    // 在 skipgram 中，对上下文的每一个词分别更新一次模型
    for (int32_t c = -boundary; c &lt;= boundary; c++) {
      if (c != 0 &amp;&amp; w + c &gt;= 0 &amp;&amp; w + c &lt; line.size()) {
        model.update(ngrams, line[w + c], lr);
      }
    }
  }
}

</code></pre>

<p>训练部分的代码已经分析完毕，预测部分的代码就简单多了，它的主要逻辑都在 <code>model.predict</code> 函数里。</p>

<pre><code class="language-cpp">void FastText::predict(const std::string&amp; filename, int32_t k, bool print_prob) {
  std::vector&lt;int32_t&gt; line, labels;
  std::ifstream ifs(filename);
  if (!ifs.is_open()) {
    std::cerr &lt;&lt; &quot;Test file cannot be opened!&quot; &lt;&lt; std::endl;
    exit(EXIT_FAILURE);
  }
  while (ifs.peek() != EOF) {
    // 读取输入文件的每一行
    dict_-&gt;getLine(ifs, line, labels, model_-&gt;rng);
    // 将一个词的 n-gram 加入词表，用于处理未登录词。（即便一个词不在词表里，我们也可以用它的 word n-gram 来预测一个结果）
    dict_-&gt;addNgrams(line, args_-&gt;wordNgrams);
    if (line.empty()) {
      std::cout &lt;&lt; &quot;n/a&quot; &lt;&lt; std::endl;
      continue;
    }
    std::vector&lt;std::pair&lt;real, int32_t&gt;&gt; predictions;
    // 调用 model 模块的预测接口，获取 k 个最可能的分类
    model_-&gt;predict(line, k, predictions);
    // 输出结果
    for (auto it = predictions.cbegin(); it != predictions.cend(); it++) {
      if (it != predictions.cbegin()) {
        std::cout &lt;&lt; &#39; &#39;;
      }
      std::cout &lt;&lt; dict_-&gt;getLabel(it-&gt;second);
      if (print_prob) {
        std::cout &lt;&lt; &#39; &#39; &lt;&lt; exp(it-&gt;first);
      }
    }
    std::cout &lt;&lt; std::endl;
  }
  ifs.close();
}
</code></pre>

<p>通过对 fasttext 模块的分析，我们发现它最核心的预测和更新逻辑都在 model 模块中，接下来，我们进入 model 模块一探究竟。</p>

<h2 id="toc_4">model 模块</h2>

<p>model 模块对外提供的服务可以分为 <code>update</code> 和 <code>predict</code> 两类，下面我们分别对它们进行分析。由于这里的参数较多，我们先以图示标明各个参数在模型中所处的位置，以免各位混淆。</p>

<p><img src="media/14732610572844/fasttext-model-arch.png" alt="fasttext-model-arch"/></p>

<p>图中所有变量的名字全部与 model 模块中的名字保持一致，注意到 <code>wo_</code> 矩阵在不同的输出层结构中扮演着不同的角色。</p>

<h3 id="toc_5">update</h3>

<p><code>update</code> 函数的作用已经在前面介绍过，下面我们看一下它的实现：</p>

<pre><code class="language-cpp">void Model::update(const std::vector&lt;int32_t&gt;&amp; input, int32_t target, real lr) {
  // target 必须在合法范围内
  assert(target &gt;= 0);
  assert(target &lt; osz_);
  if (input.size() == 0) return;
  // 计算前向传播：输入层 -&gt; 隐层
  hidden_.zero();
  for (auto it = input.cbegin(); it != input.cend(); ++it) {
    // hidden_ 向量保存输入词向量的均值，
    // addRow 的作用是将 wi_ 矩阵的第 *it 列加到 hidden_ 上
    hidden_.addRow(*wi_, *it);
  }
  // 求和后除以输入词个数，得到均值向量
  hidden_.mul(1.0 / input.size());
  
  // 根据输出层的不同结构，调用不同的函数，在各个函数中，
  // 不仅通过前向传播算出了 loss_，还进行了反向传播，计算出了 grad_，后面逐一分析。
  // 1. 负采样
  if (args_-&gt;loss == loss_name::ns) {
    loss_ += negativeSampling(target, lr);
  } else if (args_-&gt;loss == loss_name::hs) {
  // 2. 层次 softmax
    loss_ += hierarchicalSoftmax(target, lr);
  } else {
  // 3. 普通 softmax
    loss_ += softmax(target, lr);
  }
  nexamples_ += 1;

  // 如果是在训练分类器，就将 grad_ 除以 input_ 的大小
  // 原因不明
  if (args_-&gt;model == model_name::sup) {
    grad_.mul(1.0 / input.size());
  }
  // 反向传播，将 hidden_ 上的梯度传播到 wi_ 上的对应行
  for (auto it = input.cbegin(); it != input.cend(); ++it) {
    wi_-&gt;addRow(grad_, *it, 1.0);
  }
}
</code></pre>

<p>下面我们看看三种输出层对应的更新函数：<code>negativeSampling</code>,<code>hierarchicalSoftmax</code>,<code>softmax</code>。</p>

<p>model 模块中最有意思的部分就是将层次 softmax 和负采样统一抽象成多个二元 logistic regression 计算。</p>

<p>如果使用负采样，训练时每次选择一个正样本，随机采样几个负样本，每种输出都对应一个参数向量，保存于 <code>wo_</code> 的各行。对所有样本的参数更新，都是一次独立的 LR 参数更新。</p>

<p>如果使用层次 softmax，对于每个目标词，都可以在构建好的霍夫曼树上确定一条从根节点到叶节点的路径，路径上的每个非叶节点都是一个 LR，参数保存在 <code>wo_</code> 的各行上，训练时，这条路径上的 LR 各自独立进行参数更新。</p>

<p>无论是负采样还是层次 softmax，在神经网络的计算图中，所有 LR 都会依赖于 <code>hidden_</code>的值，所以 <code>hidden_</code> 的梯度 <code>grad_</code> 是各个 LR 的反向传播的梯度的累加。</p>

<p>LR 的代码如下：</p>

<pre><code class="language-cpp">real Model::binaryLogistic(int32_t target, bool label, real lr) {
  // 将 hidden_ 和参数矩阵的第 target 行做内积，并计算 sigmoid
  real score = utils::sigmoid(wo_-&gt;dotRow(hidden_, target));
  // 计算梯度时的中间变量
  real alpha = lr * (real(label) - score);
  // Loss 对于 hidden_ 的梯度累加到 grad_ 上
  grad_.addRow(*wo_, target, alpha);
  // Loss 对于 LR 参数的梯度累加到 wo_ 的对应行上
  wo_-&gt;addRow(hidden_, target, alpha);
  // LR 的 Loss
  if (label) {
    return -utils::log(score);
  } else {
    return -utils::log(1.0 - score);
  }
}
</code></pre>

<p>经过以上的分析，下面三种逻辑就比较容易理解了：</p>

<pre><code class="language-cpp">real Model::negativeSampling(int32_t target, real lr) {
  real loss = 0.0;
  grad_.zero();
  for (int32_t n = 0; n &lt;= args_-&gt;neg; n++) {
    // 对于正样本和负样本，分别更新 LR
    if (n == 0) {
      loss += binaryLogistic(target, true, lr);
    } else {
      loss += binaryLogistic(getNegative(target), false, lr);
    }
  }
  return loss;
}

real Model::hierarchicalSoftmax(int32_t target, real lr) {
  real loss = 0.0;
  grad_.zero();
  // 先确定霍夫曼树上的路径
  const std::vector&lt;bool&gt;&amp; binaryCode = codes[target];
  const std::vector&lt;int32_t&gt;&amp; pathToRoot = paths[target];
  // 分别对路径上的中间节点做 LR
  for (int32_t i = 0; i &lt; pathToRoot.size(); i++) {
    loss += binaryLogistic(pathToRoot[i], binaryCode[i], lr);
  }
  return loss;
}

// 普通 softmax 的参数更新
real Model::softmax(int32_t target, real lr) {
  grad_.zero();
  computeOutputSoftmax();
  for (int32_t i = 0; i &lt; osz_; i++) {
    real label = (i == target) ? 1.0 : 0.0;
    real alpha = lr * (label - output_[i]);
    grad_.addRow(*wo_, i, alpha);
    wo_-&gt;addRow(hidden_, i, alpha);
  }
  return -utils::log(output_[target]);
}
</code></pre>

<h3 id="toc_6">predict</h3>

<p>predict 函数可以用于给输入数据打上 1 ～ K 个类标签，并输出各个类标签对应的概率值，对于层次 softmax，我们需要遍历霍夫曼树，找到 top－K 的结果，对于普通 softmax（包括负采样和 softmax 的输出），我们需要遍历结果数组，找到 top－K。</p>

<pre><code class="language-cpp">void Model::predict(const std::vector&lt;int32_t&gt;&amp; input, int32_t k, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  assert(k &gt; 0);
  heap.reserve(k + 1);
  // 计算 hidden_
  computeHidden(input);
  
  // 如果是层次 softmax，使用 dfs 遍历霍夫曼树的所有叶子节点，找到 top－k 的概率
  if (args_-&gt;loss == loss_name::hs) {
    dfs(k, 2 * osz_ - 2, 0.0, heap);
  } else {
  // 如果是普通 softmax，在结果数组里找到 top-k
    findKBest(k, heap);
  }
  // 对结果进行排序后输出
  // 因为 heap 中虽然一定是 top-k，但并没有排好序
  std::sort_heap(heap.begin(), heap.end(), comparePairs);
}

void Model::findKBest(int32_t k, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  // 计算结果数组
  computeOutputSoftmax();
  for (int32_t i = 0; i &lt; osz_; i++) {
    if (heap.size() == k &amp;&amp; utils::log(output_[i]) &lt; heap.front().first) {
      continue;
    }
    // 使用一个堆来保存 top－k 的结果，这是算 top-k 的标准做法
    heap.push_back(std::make_pair(utils::log(output_[i]), i));
    std::push_heap(heap.begin(), heap.end(), comparePairs);
    if (heap.size() &gt; k) {
      std::pop_heap(heap.begin(), heap.end(), comparePairs);
      heap.pop_back();
    }
  }
}

void Model::dfs(int32_t k, int32_t node, real score, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp; heap) {
  if (heap.size() == k &amp;&amp; score &lt; heap.front().first) {
    return;
  }

  if (tree[node].left == -1 &amp;&amp; tree[node].right == -1) {
    // 只输出叶子节点的结果
    heap.push_back(std::make_pair(score, node));
    std::push_heap(heap.begin(), heap.end(), comparePairs);
    if (heap.size() &gt; k) {
      std::pop_heap(heap.begin(), heap.end(), comparePairs);
      heap.pop_back();
    }
    return;
  }
  
  // 将 score 累加后递归向下收集结果
  real f = utils::sigmoid(wo_-&gt;dotRow(hidden_, node - osz_));
  dfs(k, tree[node].left, score + utils::log(1.0 - f), heap);
  dfs(k, tree[node].right, score + utils::log(f), heap);
}
</code></pre>

<h2 id="toc_7">其它模块</h2>

<p>除了以上两个模块，dictionary 模块也相当重要，它完成了训练文件载入，哈希表构建，word n-gram 计算等功能，但是并没有太多算法在里面。</p>

<p>其它模块例如 Matrix, Vector 也只是封装了简单的矩阵向量操作，这里不再做详细分析。</p>

<h2 id="toc_8">附录：构建霍夫曼树算法分析</h2>

<p>在学信息论的时候接触过构建 Huffman 树的算法，课本中的方法描述往往是：</p>

<blockquote>
<p>找到当前权重最小的两个子树，将它们合并</p>
</blockquote>

<p>算法的性能取决于如何实现这个逻辑。网上的很多实现都是在新增节点都时遍历一次当前所有的树，这种算法的复杂度是 \(O(n^2)\)，性能很差。</p>

<p>聪明一点的方法是用一个优先级队列来保存当前所有的树，每次取 top 2，合并，加回队列。这个算法的复杂度是 \(O(nlogn)\)，缺点是必需使用额外的数据结构，而且进堆出堆的操作导致常数项较大。</p>

<p>word2vec 以及 fastText 都采用了一种更好的方法，时间复杂度是 \(O(nlogn)\)，只用了一次排序，一次遍历，简洁优美，但是要理解它需要进行一些推理。</p>

<p>算法如下：</p>

<pre><code class="language-cpp">void Model::buildTree(const std::vector&lt;int64_t&gt;&amp; counts) {
  // counts 数组保存每个叶子节点的词频，降序排列
  // 分配所有节点的空间
  tree.resize(2 * osz_ - 1);
  // 初始化节点属性
  for (int32_t i = 0; i &lt; 2 * osz_ - 1; i++) {
    tree[i].parent = -1;
    tree[i].left = -1;
    tree[i].right = -1;
    tree[i].count = 1e15;
    tree[i].binary = false;
  }
  for (int32_t i = 0; i &lt; osz_; i++) {
    tree[i].count = counts[i];
  }
  // leaf 指向当前未处理的叶子节点的最后一个，也就是权值最小的叶子节点
  int32_t leaf = osz_ - 1;
  // node 指向当前未处理的非叶子节点的第一个，也是权值最小的非叶子节点
  int32_t node = osz_;
  // 逐个构造所有非叶子节点（i &gt;= osz_, i &lt; 2 * osz - 1)
  for (int32_t i = osz_; i &lt; 2 * osz_ - 1; i++) {
    // 最小的两个节点的下标
    int32_t mini[2];
    
    // 计算权值最小的两个节点，候选只可能是 leaf, leaf - 1,
    // 以及 node, node + 1
    for (int32_t j = 0; j &lt; 2; j++) {
      // 从这四个候选里找到 top-2
      if (leaf &gt;= 0 &amp;&amp; tree[leaf].count &lt; tree[node].count) {
        mini[j] = leaf--;
      } else {
        mini[j] = node++;
      }
    }
    // 更新非叶子节点的属性
    tree[i].left = mini[0];
    tree[i].right = mini[1];
    tree[i].count = tree[mini[0]].count + tree[mini[1]].count;
    tree[mini[0]].parent = i;
    tree[mini[1]].parent = i;
    tree[mini[1]].binary = true;
  }
  // 计算霍夫曼编码
  for (int32_t i = 0; i &lt; osz_; i++) {
    std::vector&lt;int32_t&gt; path;
    std::vector&lt;bool&gt; code;
    int32_t j = i;
    while (tree[j].parent != -1) {
      path.push_back(tree[j].parent - osz_);
      code.push_back(tree[j].binary);
      j = tree[j].parent;
    }
    paths.push_back(path);
    codes.push_back(code);
  }
}
</code></pre>

<p>算法首先对输入的叶子节点进行一次排序（\(O(nlogn)\) ），然后确定两个下标 <code>leaf</code> 和 <code>node</code>，<code>leaf</code> 总是指向当前最小的叶子节点，<code>node</code> 总是指向当前最小的非叶子节点，所以，<strong>最小的两个节点可以从 leaf, leaf - 1, node, node + 1 四个位置中取得</strong>，时间复杂度 \(O(1)\)，每个非叶子节点都进行一次，所以总复杂度为 \(O(n)\)，算法整体复杂度为 \(O(nlogn)\)。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache 配置文件解释]]></title>
    <link href="http://heleifz.github.io/14698086378177.html"/>
    <updated>2016-07-30T00:10:37+08:00</updated>
    <id>http://heleifz.github.io/14698086378177.html</id>
    <content type="html"><![CDATA[
<p>我是业余 Web 开发，平时主要是做算法，大概每隔几个月会搭一个网站。服务器用的是 Apache，每次配置的时候，都奉行拿来主义，从别的地方把 <strong>httd.conf</strong> 以及 <strong>.htaccess</strong> 文件内容复制过来，对配置只知道个大概，没有深入学习。</p>

<p>周末做新项目的时候，专门去 Apache 官网阅读了一下 <a href="http://httpd.apache.org/docs/2.4/">User Guide</a>，发现还是很好懂的，以前很多的疑惑都解开了。这篇文章我整理一下 Apache 配置架构，给以后的我复习用。</p>

<h2 id="toc_0">Apache 配置架构</h2>

<p>Apache 本身的架构是一个核心＋外围的 Module，它的配置也遵循这个结构，本文介绍 Apache 的核心配置，以及 <strong>mod_wsgi</strong> 和 <strong>mod_rewrite</strong> 两个模块。</p>

<h3 id="toc_1">基本语法</h3>

<p>Apache 的配置位于 <strong>/etc/apache2/httpd.conf</strong> 或者 <strong>apache2.conf</strong>  之类的地方，具体路径各个平台不同。每条配置都是一个指令 (directive) :</p>

<blockquote>
<p>指令名(如Listen) 配置值(如80)</p>
</blockquote>

<p>利用 <strong>Include</strong> 指令，配置文件还可以包含其它的配置文件或文件夹。</p>

<h3 id="toc_2">作用域（Directory, Location, VirtualHost, .htaccess）</h3>

<p>大部分指令都可以作用于某个特定的路径，这个路径可以用本机文件系统路径表示(Directory)，也可以用网络位置表示(Location)，这两种表示方式很多时候可以互换，不过最好用本机路径，这样安全一点，因为多个 <strong>Location</strong> 都可以到达同一个位置。</p>

<p>Directory 的配置是递归传播的。</p>

<p>作用域上的配置是经过 merge 后生效的，merge 的优先级是：<br/>
1. &lt;Directory&gt; (except regular expressions) and .htaccess done simultaneously (with .htaccess, if allowed, overriding &lt;Directory&gt;)<br/>
2. &lt;DirectoryMatch&gt; (and &lt;Directory &quot;~&quot;&gt;)<br/>
3. &lt;Files&gt; and &lt;FilesMatch&gt; done simultaneously<br/>
4. &lt;Location&gt; and &lt;LocationMatch&gt; done simultaneously<br/>
5. &lt;If&gt;<br/>
配置按照出现的顺序处理，不过对于 Directory，处理顺序是 <strong>最短-&gt;最长</strong>，例如，<code>&lt;Directory &quot;/var/web/dir&quot;&gt;</code> 会先于 <code>&lt;Directory &quot;/var/web/dir/subdir&quot;&gt;</code> 处理，如果多个 Directory 指令作用于同一路径，那么后处理的会覆盖到前处理的上面。<strong>(Apache有些逻辑是 first match，例如 Alias，有些是后面覆盖前面）</strong></p>

<p>如果一个 Directory 设定了 <code>AllowOverride All</code>，那么它还可以用 .htaccess 中的配置来覆盖。</p>

<p>VirtualHost 根据访问者使用的域名(name based)或者访问的 IP 地址(IP based)，使用特定的配置，这样可以实现一个服务器 Host 多个网站。VirtualHost 的配置会覆盖全局配置。</p>

<h2 id="toc_3">mod_wsgi</h2>

<p>mod_wsgi 通过 WSGIScriptAlias 这个指令，把某些指令路由给 Python 脚本处理，和 Alias 一样，第一个 Match 的生效。</p>

<p><a href="http://ssmax.net/archives/977.html">mod_wsgi的两种工作模式</a></p>

<h2 id="toc_4">mod_rewrite</h2>

<pre><code class="language-apache">&lt;IfModule mod_rewrite.c &gt;
    RewriteEngine On
    # 拼 URL 时使用的前缀
    RewriteBase /
    # test string 可以用一堆常量，例如 %{REQUEST_URI} 
    # CondPattern 可以是正则表达式，也可以是一些其它的 test，例如文件是否存在
    # 默认是 AND，可以换成 [OR]
    RewriteCond TestString CondPattern [OR]
    # 支持 backref
    RewriteRule Pattern Substitution [flags]
&lt;/IfModuel&gt;
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SVM 推导]]></title>
    <link href="http://heleifz.github.io/14698080869715.html"/>
    <updated>2016-07-30T00:01:26+08:00</updated>
    <id>http://heleifz.github.io/14698080869715.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1. Intuition</h2>

<p>（每个）样本点距离分类面越远，说明分类面选得越好。为了度量这一点，我们引入<strong>函数间隔</strong>和<strong>几何间隔</strong>。这里的间隔（margin）指的是点与分类面的间隔。</p>

<h3 id="toc_1">函数间隔</h3>

<ul>
<li><p>单个样本<br/>
\[ \hat{\gamma}_i=y_i(\omega^Tx+b) \]</p></li>
<li><p>样本集<br/>
\[ \hat{\gamma}=\min_{1..m}\hat{\gamma}_i \]</p></li>
<li><p>性质<br/>
由于没有对 w 和 b 进行归一化，函数间隔在两者 scale 时候，将会成倍变化。</p></li>
</ul>

<h3 id="toc_2">几何间隔</h3>

<ul>
<li><p>单个样本<br/>
\[ \gamma_i = \hat{\gamma}_i / ||\omega||\]</p></li>
<li><p>样本集<br/>
\[ \gamma=\min_{1..m}\gamma_i \]</p></li>
<li><p>性质<br/>
对 w 进行了归一化，这样，在最优化的时候，我们可以针对 w 的长度施加各种限制，但不影响最终结果。</p></li>
</ul>

<h2 id="toc_3">2. 最优化问题</h2>

<h3 id="toc_4">问题化简</h3>

<ol>
<li><p>基础形态<br/>
\[ \max_{\gamma,\omega,b}\gamma \\ y_i(\omega^tx+b)\ge\gamma \\ ||w||=1\]<br/>
<em>最优化条件非凸</em></p></li>
<li><p>基础形态2<br/>
\[ \max_{\hat{\gamma},\omega,b}\hat{\gamma}/||w|| \\ y_i(\omega^tx+b)\ge\hat{\gamma} \]<br/>
<em>最优化目标非凸</em></p></li>
<li><p>QP形态（施加限制，函数间隔为1，且最大变最小）<br/>
\[ \min_{\omega,b}1/2||w||^2 \\ y_i(\omega^tx+b)\ge1 \]<br/>
<em>二次规划（凸）</em></p></li>
</ol>

<h3 id="toc_5">对偶问题</h3>

<p>对于最优化问题：</p>

<p>\[ \min_\omega f(\omega) \\ g_i(\omega)\le0\ (i=1...k) \\ h_i(\omega)=0\ (i=1..l)\]</p>

<p>有<strong>广义拉格朗日函数</strong>:</p>

<p>\[ L(\omega,\alpha,\beta) = f(\omega)+\Sigma\alpha_ig_i(\omega) + \Sigma\beta_ih_i(\omega) \]</p>

<p>其中要求 alpha 大等于0（显然，否则 Primal 问题不成立）</p>

<ul>
<li><p>Primal 问题 (p*)<br/>
\[ \min_\omega\max_{\alpha,\beta}L(\omega,\alpha,\beta) \]<br/>
一旦 w 超出限定范围，内层的 max 立马飙到正无穷</p></li>
<li><p>Dual 问题 (d*)<br/>
\[ \max_{\alpha,\beta}\min_\omega L(\omega,\alpha,\beta) \]</p></li>
<li><p>两者关系<br/>
\[ d^*\le p^*\]<br/>
根据alpha,beta,g,h的取值范围，在可行域内，拉格朗日函数必须小等于f(w)，自然也小等于p*，无论它怎么min，max，依然小等于。</p></li>
</ul>

<p><strong>在f,g是凸函数，h是仿射函数（wx+b)，且g包围的区域不为空的时候</strong></p>

<p>w*是原问题的解，alpha*，beta*是对偶问题的解，且<br/>
\[ d^*=p^*=L(\omega^*,\alpha^*,\beta^*)\]</p>

<p>此时，有KKT条件成立（充要？）</p>

<p>\[  \frac{\partial}{\partial \omega_i}L(\omega^*,\alpha^*,\beta^*)=0 \\<br/>
    \frac{\partial}{\partial \beta_i}L(\omega^*,\alpha^*,\beta^*)=0 \\<br/>
    \alpha^*_ig_i(\omega^*)=0 \\<br/>
    g_i(\omega^*)\le0 \\<br/>
    \alpha^*\ge0<br/>
\]</p>

<h2 id="toc_6">3. 将QP转换为Dual形式</h2>

<h3 id="toc_7">重写限制条件</h3>

<p>\[ g_i(\omega)=1-y_i(\omega^Tx_i+b)\le0\]<br/>
性质：根据KKT条件，<strong>当alpha_i大于0时，g_i(w)=0</strong>，此时xi是支撑矢量。</p>

<h3 id="toc_8">构造广义拉格朗日函数</h3>

<p>\[ L(\omega,b,\alpha)=1/2||w||^2+\Sigma\alpha_ig_i(\omega)\\<br/>
= 1/2||w||^2+\Sigma_{i=1..m}\alpha_i[1-y_i(\omega^Tx_i+b)]\\<br/>
= 1/2||w||^2+\Sigma\alpha_i-\Sigma\alpha_iy_i\omega^Tx_i-\Sigma\alpha_i y_i b<br/>
\]</p>

<h3 id="toc_9">对于 w 求最小值</h3>

<p>令<br/>
\[<br/>
\frac{\partial}{\partial\omega}L(\omega,b,\alpha)\\<br/>
=\omega-\Sigma\alpha_iy_ix_i=0<br/>
\]<br/>
所以有：<br/>
\[<br/>
\omega^*=\Sigma\alpha_iy_ix_i<br/>
\]<br/>
将其代入到拉格朗日函数中：<br/>
\[<br/>
L(\omega^*,b,\alpha)=1/2(\Sigma\alpha_iy_ix_i^T)(\Sigma\alpha_iy_ix_i)+\Sigma\alpha_i-\Sigma\alpha_iy_i(\Sigma\alpha_iy_ix_i^T)x_i-\Sigma\alpha_iy_ib\\<br/>
=1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i-\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j-\Sigma\alpha_iy_ib<br/>
\\<br/>
=-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i-\Sigma\alpha_iy_ib<br/>
\]</p>

<h3 id="toc_10">对于 b 求最小值</h3>

<p>令<br/>
\[<br/>
\frac{\partial}{\partial b}L(\omega,b,\alpha)\\<br/>
=-\Sigma a_iy_i=0<br/>
\]<br/>
带入到上一步中，消掉最后一项，得到：<br/>
\[<br/>
L(\omega^*,b,\alpha)=-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i<br/>
\]</p>

<h3 id="toc_11">得到对偶问题</h3>

<p>整理得到以下最终的最优化问题：</p>

<p>\[<br/>
\max_\alpha-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i\\<br/>
\alpha_i\ge0\ (i=1..m)\\<br/>
\Sigma a_iy_i=0\ (i=1..m)<br/>
\]</p>

<p>在得到alpha后，w可以根据上面的推导得到，对于b，在w得到后，将正负样本中最接近超平面的两个加起来取平均（根据图示可以很容易看出来）</p>

<p>\[<br/>
b^*=-\frac{\min{\omega^*}^Tx_++\max{\omega^*}^Tx_-}{2}<br/>
\]</p>

<p>在预测时，我们可以不用 w，而是采用如下的公式（kernel trick的基础）</p>

<p>\[<br/>
\omega^Tx+b=\Sigma\alpha_iy_i(x_i^Tx)+b<br/>
\]</p>

<p>因为只有支撑矢量的alpha才非零，所以，我们只需要保存支撑矢量（和b），就能进行分类。</p>

<h2 id="toc_12">4. 核函数</h2>

<p>给定一个feature mapping（x-&gt;phi(x)），核函数K定义为：</p>

<p>\[<br/>
K(x,z)=\phi(x)^T\phi(z)<br/>
\]</p>

<p>可以看出，当x和z相近时，核函数取值越大，x和z越相似。我们可以直接找核函数，而不用找到显式的 feature mapping，为了做到这一点，必须规定核函数所需要满足的条件。</p>

<h3 id="toc_13">Mercer定理</h3>

<p>给定函数K，对于任意m个向量，它们构成的核矩阵对称半正定 &lt;=&gt; K是一个核函数</p>

<h2 id="toc_14">5. 软间隔策略</h2>

<p>为了处理线性不可分情况，SVM可以改写为：</p>

<p>\[<br/>
\min_{\omega,b}1/2||w||^2 + C\Sigma\xi_i\\<br/>
y_i(\omega^tx+b)\ge1-\xi_i\ (i=1..m)\\<br/>
\xi_i\ge0 (i=1..m)<br/>
\]</p>

<p>化简成对偶问题后，和之前的形式惊人相近，除了alpha的范围</p>

<p>\[<br/>
\max_\alpha-1/2\Sigma_{i,j}\alpha_i\alpha_jy_iy_jx_i^Tx_j+\Sigma\alpha_i\\<br/>
C\ge\alpha_i\ge0\ (i=1..m)\\<br/>
\Sigma a_iy_i=0\ (i=1..m)<br/>
\]</p>

<p>此时，b的计算也变了（略），且KKT条件有如下推论：</p>

<p>\[<br/>
\alpha_i=0 ⇒ y_i(\omega^Tx+b)\ge1\\<br/>
\alpha_i=C ⇒ y_i(\omega^Tx+b)\le1\\<br/>
0\le\alpha_i\le C ⇒ y_i(\omega^Tx+b)=1<br/>
\]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[shared_ptr 原理及事故]]></title>
    <link href="http://heleifz.github.io/14696398760857.html"/>
    <updated>2016-07-28T01:17:56+08:00</updated>
    <id>http://heleifz.github.io/14696398760857.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">new与赋值的坑</h2>

<p>赋值（assignment）和new运算符在C++与Java（或C#）中的行为有本质的区别。在Java中，new是对象的构造，而赋值运算是引用的传递；而在C++中，赋值运算符意味着&quot;构造&quot;，或者&quot;值的拷贝&quot;，new运算符意味着在堆上分配内存空间，并将这块内存的管理权（责任）交给用户。C++中的不少坑，就是由new和赋值引起的。</p>

<p>在C++中使用new的原因除了堆上能定义体积更大的数据结构之外，就是能使用C++中的dynamic dispatch（也叫多态）了：只有指针（和引用）才能使用虚函数来展现多态性。在这时，new出来的指针变得很像Java中的普通对象，赋值意味着引用的传递，方法调用会呈现出多态性，我们进入了面向对象的世界，一切十分美好，除了&quot;要手动释放内存&quot;。</p>

<p>在简单的程序中，我们不大可能忘记释放new出来的内存，随着程序规模的增大，我们忘了delete的概率也随之增大，这是因为C++是如此一个精神分裂的语言，赋值运算符竟然同时展现出&quot;值拷贝&quot;和&quot;引用传递&quot;两种截然不同的语义，这种不一致性导致&quot;内存泄漏&quot;成为C++新手最常犯的错误之一。当然你可以说，只要细心一点，一定能把所有内存泄漏从代码中清除。但手动管理内存更严重的问题在于，内存究竟要由谁来分配和释放呢？指针的赋值将同一对象的引用散播到程序每个角落，但是该对象的删除却只能发生一次，当你在代码中用完这么一个资源指针：resourcePtr，你敢delete它吗？它极有可能同时被多个对象拥有着，而这些对象中的任何一个都有可能在之后使用该资源，而这些对象中的另外一个，可能在它的析构函数中释放该资源。&quot;那我不delete不就行了吗？&quot;，你可能这么问，当然行， 这时候你要面对另外一种可能性：也许你是这个指针的唯一使用者，如果你用完不delete，内存就泄漏了。</p>

<p>开发者日常需要在工作中使用不同的库，而以上两种情况可能会在这些库中出现，假设库作者们的性格截然不同，导致这两个库在资源释放上采取了不同的风格，在这个时候，你面对一个用完了的资源指针，是删还是不删呢？这个问题从根本上来说，是因为C++的语言特性让人容易搞错&quot;资源的拥有者&quot;这个概念，资源的拥有者，从来都只能是系统，当我们需要时便向系统请求，当我们不需要时就让系统自己捡回去（Garbage Collector），当我们试图自己当资源的主人时，一系列坑爹的问题就会接踵而来。</p>

<h2 id="toc_1">异常安全的类</h2>

<p>我们再来看另外一个与new运算符紧密相关的问题：如何写一个异常安全（exception safe）的类。</p>

<p>异常安全简单而言就是：当你的类抛出异常后，你的程序会不会爆掉。爆掉的情况主要包括：内存泄漏，以及不一致的类状态（例如一个字符串类，它的size()方法返回的字符串大小与实际的字符串大小不同），这里仅讨论内存泄漏的情况。</p>

<p>为了让用户免去手动delete资源的烦恼，不少类库采用了RAII风格，即<em>Resource Acquisition Is Initialization</em>，这种风格采用类来封装资源，在类的构造函数中获取资源，在类的析构函数中释放资源，这个资源可以是内存，可以是一个网络连接，也可以是mutex这样的线程同步量。在RAII的感召下，我们来写这么一个人畜无害的类：</p>

<pre><code class="language-cpp">class TooSimple {
private:
    Resource *a;
    Resource *b;
public
    TooSimple() {
        a = new Resource();
        b = new Resource(); //在这里抛出异常
    }
    ~TooSimple() {
        delete a;
        delete b;
    }
};
</code></pre>

<p>这个看似简单的类，是有内存泄漏危险的哟！为了理解这一点，首先简单介绍一下C++在抛出异常时所做的事吧：</p>

<p>如果一个new操作（及其调用的构造函数）中抛出了异常，那么它分配的内存空间将自动被释放。<br/>
一个函数（或方法）抛出异常，那么它首先将当前栈上的变量全部清空(unwinding)，如果变量是类对象的话，将调用其析构函数，接着，异常来到call stack的上一层，做相同操作，直到遇到catch语句。<br/>
指针是一个普通的变量，不是类对象，所以在清空call stack时，指针指向资源的析构函数将不会调用。<br/>
根据这三条规则，我们很容易发现，如果<code>b = new Resource()</code>句抛出异常，那么构造函数将被强行终止，根据规则1，b分配的资源将被释放（假设Resource类本身是异常安全的），指针a，b从call stack上清除，由于此时构造函数还未完成，所以TooSimple的析构函数也不会被调用（都没构造完呢，现在只是一个&quot;部分初始化&quot;的对象，析构函数自然没理由被调用），a已经被分配了资源，但是call stack被清空，地址已经找不到了，于是delete永远无法执行，于是内存泄漏发生了。</p>

<p>这个问题有一个很直接的&quot;解决&quot;方案，那就是把<code>b = new Resource()</code>包裹在一个try-catch块中，并在catch里将执行delete a，这样做当然没问题，但我们的代码逻辑变得复杂了，且当类需要分配的资源种类增多的时候，这种处理办法会让程序的可读性急剧下降。这时候我们不禁想：要是指针变量能像类对象一样地&quot;析构&quot;就好了，一旦指针具有类似析构的行为，那么在call stack被清空时，指针会在&quot;析构&quot;时实现自动的delete。怀着这种想法，我们写了这么一个类模版：</p>

<pre><code class="language-cpp">template &lt;typename T&gt;
class StupidPointer {
public:
    T *ptr;
    StupidPointer(T *p) : ptr(p) {}
    ~StupidPointer() { delete ptr; }
};
有了这个&quot;酷炫&quot;的类，现在我们的构造函数可以这么写：
TooSimple() {
    a = StupidPointer&lt;Resource&gt;(new Resource());
    b = StupidPointer&lt;Resource&gt;(new Resource());
};
</code></pre>

<p>由于此时的a，已经不再是指针，而是<code>StupidPointer&lt;Resource&gt;</code>类，在清空call stack时，它的析构函数被调用，于是a指向的资源被释放了。但是，StupidPointer类有一个严重的问题：当多个StupidPointer对象管理同一个指针时，一个对象析构后，剩下对象中保存的指针将变成指向无效内存地址的&quot;野指针&quot;（因为已经被delete过了啊），如果delete一个野指针，电脑就会爆炸（严肃）。</p>

<p>C++11的标准库提供了两种解决问题的思路：1、不允许多个对象管理一个指针（unique_ptr）；2、允许多个对象管理同一个指针，但仅当管理这个指针的最后一个对象析构时才调用delete（shared_ptr）。这两个思路的共同点是：只！允！许！delete一次！</p>

<p>本篇文章里，我们仅讨论shared_ptr。</p>

<h2 id="toc_2">shared_ptr</h2>

<p>在将shared_ptr的使用之前，我们首先来看看它的基本实现原理。</p>

<p>刚才说到，当多个shared_ptr管理同一个指针，仅当最后一个shared_ptr析构时，指针才被delete。这是怎么实现的呢？答案是：引用计数（reference counting）。引用计数指的是，所有管理同一个裸指针（raw pointer）的shared_ptr，都共享一个引用计数器，每当一个shared_ptr被赋值（或拷贝构造）给其它shared_ptr时，这个共享的引用计数器就加1，当一个shared_ptr析构或者被用于管理其它裸指针时，这个引用计数器就减1，如果此时发现引用计数器为0，那么说明它是管理这个指针的最后一个shared_ptr了，于是我们释放指针指向的资源。</p>

<p>在底层实现中，这个引用计数器保存在某个内部类型里（这个类型中还包含了deleter，它控制了指针的释放策略，默认情况下就是普通的delete操作），而这个内部类型对象在shared_ptr第一次构造时以指针的形式保存在shared_ptr中。shared_ptr重载了赋值运算符，在赋值和拷贝构造另一个shared_ptr时，这个指针被另一个shared_ptr共享。在引用计数归零时，这个内部类型指针与shared_ptr管理的资源一起被释放。此外，为了保证线程安全性，引用计数器的加1，减1操作都是原子操作，它保证shared_ptr由多个线程共享时不会爆掉。</p>

<p>这就是shared_ptr的实现原理，现在我们来看看怎么用它吧！（超简单）</p>

<p><code>std::shared_ptr</code>位于头文件<memory>中（这里只讲C++11，boost的shared_ptr当然是放在boost的头文件中），下面我以代码示例的形式展现它的用法，具体文档可以看这里。</p>

<pre><code class="language-cpp">// 初始化
shared_ptr&lt;int&gt; x = shared_ptr&lt;int&gt;(new int); // 这个方法有缺陷，下面我会说
shared_ptr&lt;int&gt; y = make_shared&lt;int&gt;();
shared_ptr&lt;Resource&gt; obj = make_shared&lt;Resource&gt;(arg1, arg2); // arg1, arg2是Resource构造函数的参数
// 赋值
shared_ptr&lt;int&gt; z = x; // 此时z和x共享同一个引用计数器
// 像普通指针一样使用
int val = *x;
assert (x == z);
assert (y != z);
assert (x != nullptr);
obj-&gt;someMethod();
// 其它辅助操作
x.swap(z); // 交换两个shared_ptr管理的裸指针（当然，包含它们的引用计数）
obj.reset(); // 重置该shared_ptr（引用计数减1）
</code></pre>

<p>太好用了！</p>

<h2 id="toc_3">错误用法1：循环引用</h2>

<p>shared_ptr的一个最大的缺点，或者说，引用计数策略最大的缺点，就是循环引用（cyclic reference），下面是一个典型的事故现场：</p>

<pre><code class="language-cpp">class Observer; // 前向声明
class Subject {
private:
    std::vector&lt;shared_ptr&lt;Observer&gt;&gt; observers;
public:
    Subject() {}
    addObserver(shared_ptr&lt;Observer&gt; ob) {
        observers.push_back(ob);
    }
    // 其它代码
    ..........
};
class Observer {
private:
    shared_ptr&lt;Subject&gt; object;
public:
    Observer(shared_ptr&lt;Object&gt; obj) : object(obj) {}
    // 其它代码
    ...........
};
</code></pre>

<p>目标（Subject）类连接着多个观察者（Observer）类，当某个事件发生时，目标类可以遍历观察者数组observers，对每个观察者进行&quot;通知&quot;，而观察者类中，也保存着目标类的shared_ptr，这样多个观察者之间可以以目标类为桥梁进行沟通，除了会发生内存泄漏以外，这是很不错的设计模式嘛！等等，不是说用了shared_ptr管理资源后就不会内存泄漏了吗？怎么又漏了？</p>

<p>这就是引用计数模型失效的唯一的情况：循环引用。循环引用指的是，一个引用通过一系列的引用链，竟然引用回自身，上面的例子中，<code>Subject-&gt;Observer-&gt;Subject</code>就是这么一条环形的引用链。假设我们的程序中只有一个变量<code>shared_ptr&lt;Subject&gt; p</code>，此时，p指向的对象不仅通过该shared_ptr引用自己，还通过它包含的Observer中的object成员变量引用回自己，于是它的引用计数是2，每个Observer的引用计数都是1。当p析构时，它的引用计数减1，变成2-1=1（大于0!），p指向对象的析构函数将不会被调用，于是p和它包含的每个Observer对象在程序结束时依然驻留在内存中没被delete，形成内存泄漏。</p>

<h2 id="toc_4">weak_ptr</h2>

<p>为了解决这一问题，标准库提供了<code>std::weak_ptr</code>（弱引用），它也位于<memory>中。</p>

<p>weak_ptr是shared_ptr的&quot;观察者&quot;，它与一个shared_ptr绑定，但却不参与引用计数的计算，在需要时，它还能摇身一变，生成一个与它所&quot;观察&quot;的shared_ptr共享引用计数器的新shared_ptr。总而言之，weak_ptr的作用就是：在需要时变出一个shared_ptr，在其它时候不干扰shared_ptr的引用计数。</p>

<p>在上面的例子中，我们只需简单地将Observer中object成员的类型换成<code>std::weak_ptr&lt;Subject&gt;</code>即可解决内存泄漏的问题，此刻（接着上面的例子），p指向对象的引用计数为1，所以在p析构时，Subject指针将被delete，其中包含的observers数组在析构时，内部的Observer对象的引用计数也将变为0，故它们也被delete了，资源释放得干干净净。</p>

<p>下面，是weak_ptr的使用方法：</p>

<pre><code class="language-cpp">std::shared_ptr&lt;int&gt; sh = std::make_shared&lt;int&gt;();
// 用一个shared_ptr初始化
std::weak_ptr&lt;int&gt; w(sh);
// 变出shared_ptr
std::shared_ptr&lt;int&gt; another = w.lock();
// 判断weak_ptr所观察的shared_ptr的资源是否已经释放
bool isDeleted = w.expired();
</code></pre>

<h2 id="toc_5">错误用法2：多个无关的shared_ptr管理同一裸指针</h2>

<p>考虑下面这个情况：</p>

<pre><code class="language-cpp">int *a = new int;
std::shared_ptr&lt;int&gt; p1(a);
std::shared_ptr&lt;int&gt; p2(a);
</code></pre>

<p>p1和p2同时管理同一裸指针a，与之前的例子不同的是，此时的p1和p2有着完全独立的两个引用计数器（初始化p2时，用的是裸指针a，于是我们没有任何办法获取p1的引用计数！），于是，上面的代码会导致a被delete两次，分别由p1和p2的析构导致，电脑再一次爆炸了。</p>

<p>为了避免这种情况的发生，我们永远不要将new用在shared_ptr构造函数参数列表以外的地方，或者干脆不用new，改用make_shared。</p>

<p>即便我们的程序严格采取上述做法，C++还提供另外一种绕过shared_ptr，直接获取裸指针的方式，那就是this指针。请看下面的事故现场：</p>

<pre><code class="language-cpp">class A {
public:
    std::shared_ptr&lt;A&gt; getShared() {
        return std::shared_ptr&lt;A&gt;(this);
    }
};
int main() {
    std::shared_ptr&lt;A&gt; pa = std::make_shared&lt;A&gt;();
    std::shared_ptr&lt;A&gt; pbad = pa-&gt;getShared();
    return 0;
}
</code></pre>

<p>在此次事故中，pa和pbad拥有各自独立的引用计数器，所以程序将发生相同的&quot;delete野指针&quot;错误。总而言之，管理同一资源的shared_ptr，只能由同一个初始shared_ptr通过一系列赋值或者拷贝构造途径得来。更抽象的说，管理同一资源的shared_ptr的构造顺序，必须是一个无环有向的连通图，无环能够保证没有循环引用，连通性能够保证每个shared_ptr都来自于相同的源。</p>

<p>另外，标准库提供了一种特殊的接口，来解决&quot;生成this指针的shared_ptr&quot;的问题。</p>

<h2 id="toc_6">enable_shared_from_this</h2>

<p>enable_shared_from_this是标准库中提供的接口（一个基类啦）：</p>

<pre><code class="language-cpp">template&lt;typename T&gt;
class enable_shared_from_this {
public:
    shared_ptr&lt;T&gt; shared_from_this();
}
</code></pre>

<p>如果想要一个由shared_ptr管理的类A对象能够在方法内部得到this指针的shared_ptr，且返回的shared_ptr和管理这个类的shared_ptr共享引用计数，只需让这个类派生自<code>enable_shared_from_this&lt;A&gt;</code>即可，之后调用<code>shared_from_this()</code>即可获得正确的 shared_ptr。</p>

<p>一般来说，这个接口是通过weak_ptr实现的：enable_shared_from_this中包含一个weak_ptr，在初始化shared_ptr时，构造函数会检测到这个该类派生于enable_shared_from_this（通过模版黑魔法很容易就能实现这个功能啦），于是将这个weak_ptr指向初始化的shared_ptr。调用shared_from_this，本质上就是weak_ptr的一个lock操作：</p>

<pre><code class="language-cpp">class A : enable_shared_from_this&lt;A&gt; {
    // ......
};
int main() {
    std::shared_ptr&lt;A&gt; pa = std::make_shared&lt;A&gt;();
    std::shared_ptr&lt;A&gt; pgood = pa-&gt;shared_from_this();
    return 0;
}
</code></pre>

<h2 id="toc_7">错误用法3：直接用new构造多个shared_ptr作为实参</h2>

<p>之前提到的C++异常处理机制，让我们可以很容易发现下面的代码有内存泄漏的危险：</p>

<pre><code class="language-cpp">// 声明
void f(A *p1, B *p2);
// 使用
f(new A, new B);
</code></pre>

<p>假如new A先于new B发生（我说&quot;假如&quot;，是因为C++的函数参数的计算顺序是不确定的），那么如果new B抛出异常，那么new A分配的内存将会发生泄漏。作为一个刚学会shared_ptr的优秀程序员，我们可以如此&quot;解决&quot;该问题：</p>

<pre><code class="language-cpp">// 声明
void f(shared_ptr&lt;A&gt; p1, shared_ptr&lt;B&gt; p2);
// 使用
f(shared_ptr&lt;A&gt;(new A), shared_ptr&lt;B&gt;(new B));
</code></pre>

<p>可惜，这么写依然有可能发生内存泄漏，因为两个shared_ptr的构造有可能发生在new A与new B之后，这涉及到C++里称作sequence after，或sequence point的性质，该性质保证：</p>

<ol>
<li>new A在<code>shared_ptr&lt;A&gt;</code>构造之前发生</li>
<li>new B在<code>shared_ptr&lt;B&gt;</code>构造之前发生</li>
<li>两个shared_ptr的构造在f调用之前发生</li>
</ol>

<p>在满足以上三条性质的前提下，各操作可以以任意顺序执行。详情请见Herb Shutter的文章：Exception-Safe Function Calls 。</p>

<h2 id="toc_8">make_shared</h2>

<p>若我们这么调用f：</p>

<pre><code class="language-cpp">f(make_shared&lt;A&gt;(), make_shared&lt;B&gt;());
</code></pre>

<p>那么就不可能发生内存泄漏了，原因依然是sequence after性质。sequence after性质保证，如果两个函数的执行顺序不确定（如本例，作为另一个函数的两个参数），那么在一个函数执行时，另一个不会执行（倘若参数是１＋１和3 + 3*6这种表达式，那么加法和乘法甚至允许交错执行，sequence after性质真是有够复杂），于是，如果<code>make_shared&lt;A&gt;</code>构造完成了，<code>make_shared&lt;B&gt;</code>中抛出异常，那么A的资源能被正确释放。与上面用new来初始化的情形对比，make_shared保证了第二new发生的时候，第一个new所分配的资源已经被shared_ptr管理起来了，故在异常发生时，能正确释放资源。</p>

<p>一句话建议：总是使用make_shared来生成shared_ptr！</p>

<h2 id="toc_9">结论</h2>

<p>用shared_ptr，不用new<br/>
使用weak_ptr来打破循环引用<br/>
用make_shared来生成shared_ptr<br/>
用enable_shared_from_this来使一个类能获取自身的shared_ptr</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git]]></title>
    <link href="http://heleifz.github.io/14696393471602.html"/>
    <updated>2016-07-28T01:09:07+08:00</updated>
    <id>http://heleifz.github.io/14696393471602.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1. 对象模型</h2>

<ul>
<li>Git由一堆对象组成，每个对象都有一个SHA1的40位编码</li>
<li>每个文件都是一个blob，由内容进行hash，同样内容的文件在版本库内<strong>只有一份</strong>。</li>
<li>每个目录都是一个tree，tree中可以有tree也可以有blob。（文件名等信息在tree中保存）</li>
<li>每个commit都指向一个tree，一个或多个parent，包含作者，email，日期，消息等数据。（tag就是一个commit的引用而已）</li>
<li><strong>Git Repo就是由commit组成的网络</strong>。</li>
</ul>

<blockquote>
<p>既然如此，我们就得找到表示commit的方式</p>
</blockquote>

<h2 id="toc_1">2. Commit的表示法</h2>

<h3 id="toc_2">基础</h3>

<ul>
<li>branchname : 访问branchname最新的commit</li>
<li>tagname : 访问tagname引用的commit</li>
<li>HEAD</li>
<li>完整的hash</li>
<li>hash的前几位（7位一般就够了）</li>
</ul>

<h3 id="toc_3">组合</h3>

<ul>
<li>name^ : name的parent commit</li>
<li>name<sup>^</sup> : parent的parent</li>
<li>name<sup>n</sup> : 第 n 个 parent（在merge的情况下）</li>
<li>name~n : 倒数第 n 个 parent</li>
<li>name:path : commit中的文件</li>
</ul>

<h3 id="toc_4">范围</h3>

<ul>
<li>name1..name2 : name2能回溯到的，但是name1不能回溯到的（master..，相当于 mater..HEAD，当前分支的修改）</li>
<li>name1...name2 : name1的和name2的祖先，但不是共同祖先</li>
</ul>

<h3 id="toc_5">描述</h3>

<pre><code class="language-bash">--since=&quot;two weeks ago&quot;
--until=&quot;1 week ago&quot;
--grep=pattern
--committer=pattern
--author=pattern
--no-merges
</code></pre>

<h2 id="toc_6">3. 三个位置</h2>

<h3 id="toc_7">Working Tree</h3>

<p>当前的工作目录，包含文件。<code>git add</code>以后，进入 index</p>

<h3 id="toc_8">Index</h3>

<p>暂存区。所谓 index 是指，暂存区里包含了各种新增 object 的引用（索引）。<code>git commit</code>以后，进入 branch。</p>

<h3 id="toc_9">Branch</h3>

<p>当前的分支。</p>

<h2 id="toc_10">4. 部分操作</h2>

<h3 id="toc_11"><code>config</code></h3>

<p>在使用 git 前必须设置好名字和邮箱，如果不用 global 参数，则是设置项目专用的作者信息。</p>

<pre><code class="language-bash">$ git config --global user.name &#39;Your Name&#39;
$ git config --global user.email you@somedomain.com
</code></pre>

<h3 id="toc_12"><code>add</code></h3>

<p>将文件从 working tree 添加到 index<br/>
<code>bash<br/>
git add &lt;path&gt; # 新增，修改的文件（文件夹）<br/>
git add -u &lt;path&gt; # 修改，删除的文件（文件夹）<br/>
git add -A &lt;path&gt; # 新增，修改，删除的文件（文件夹）<br/>
</code></p>

<h3 id="toc_13"><code>commit</code></h3>

<p>将 index 的 tree 作为下一个 commit，加入当前 branch。<br/>
<code>bash<br/>
git commit -m &#39;my commit message..&#39;<br/>
git commit -a # 先 add 再 commit （-am)<br/>
git commit --amend -m &quot;correct some error&quot; # amend产生新的ID，请不要amend和别人共享的commit<br/>
</code></p>

<h3 id="toc_14"><code>diff</code></h3>

<pre><code class="language-bash">git diff # unstaged change
git diff --cached # difference between index and branch
git diff HEAD # diff between working tree + index and branch
</code></pre>

<h3 id="toc_15"><code>checkout</code></h3>

<pre><code class="language-bash"># 注：忽略未commit的文件，--force参数可以暴力覆盖

git checkout &lt;branch&gt;  # 更新三棵树和HEAD
git checkout &lt;commit&gt;  # 更新HEAD和working tree
git checkout           # 用 last commit 或 index 来复位 working tree
</code></pre>

<h3 id="toc_16"><code>reset</code></h3>

<p>reset 指令不是单纯修改HEAD（与checkout不同），而是先修改branch，然后把HEAD指向新的branch。</p>

<pre><code class="language-bash">git reset --soft  # 只修改 branch
git reset --mixed # 默认，修改 branch + index 
git reset --hard  # 修改 branch + index + working tree
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[神经网络计算模型 - 上篇：理论解释]]></title>
    <link href="http://heleifz.github.io/14696391071598.html"/>
    <updated>2016-07-28T01:05:07+08:00</updated>
    <id>http://heleifz.github.io/14696391071598.html</id>
    <content type="html"><![CDATA[
<p>传统机器学习教科书中的神经网络通常是简单的多层感知机，采用全联接层作为隐层，并经过一个 softmax 输出，现代的神经网络架构脱胎于此，却早已脱离这样的简单模型，无论是 <a href="http://caffe.berkeleyvision.org">Caffe</a> 还是 <a href="http://www.deeplearning.net/software/theano/">Theano</a>，都具有可定制，可扩展的优点，允许用户自行搭建符合需求的网络架构和运算。</p>

<p>本文从 <em>数据的抽象</em>，<em>运算的抽象</em> 两个角度介绍现代神经网络的计算模型。</p>

<h2 id="toc_0">用 Tensor 抽象数据</h2>

<p>传统机器学习模型严重依赖于 <a href="http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf">feature engineering</a>，所以模型的输入一般是计算好的特征向量，但是基于神经网络的机器学习系统利用中间层自动得到合适的特征，所以数据往往以更为原始而稠密的形式输入网络（图像，音视频），并且在网络内部保持这种 N 维数组的结构向后传播，这种 N 维数组也叫 tensor。</p>

<p>采用 tensor 或向量在计算上并无本质区别，将 tensor 拉伸就变成向量，如下图所示，假设输入标量为 \(z\) ，相对于 tensor \(X\) 计算梯度时<br/>
\[<br/>
\frac {\partial z}{\partial X_{i,j,k} } = \frac {\partial z}{\partial 向量化(X)_n }<br/>
\]<br/>
其中 \(n\) 为 \( X_{i,j,k} \) 在 \( X \) 向量化后所在的位置。Tensor 的好处是降低了用户的思维负担，保持了数据的原始结构，目前所有的神经网络工具都使用 tensor 来存储数据。</p>

<p><img src="media/14696391071598/tensor.png" alt="tenso" style="width:513px;"/></p>

<p>使用向量的好处是方便写公式，后文中 \( X_i \) 表示 tensor \( X \) 以某种方式拉伸成向量后的第 \( i \) 个元素。</p>

<h2 id="toc_1">由运算构成网络</h2>

<p>运算（operation）就是函数，一个运算接收一个或多个 tensor 作为输入，产生一个 tensor 作为输出，不同的运算的组合成完整的神经网络。</p>

<p><img src="media/14696391071598/operator.png" alt="operato"/></p>

<p>下面列举几种常见的运算，默认所有大写字母都代表 tensor：</p>

<h4 id="toc_2">1. 矩阵乘法（全联接层）</h4>

<p><strong>输入</strong>: 数据 \(X\)，参数 \(W\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 运算时将 \(X\) 拉伸为向量，假设长度为 \(n\)，输出 \(Y\) 长度为 \(m\)，那么 \(W\) 矩阵的大小为 \(m\)x\(n\)，且 \(Y=WX\)。</p>

<h4 id="toc_3">2. 卷积</h4>

<p><strong>输入</strong>: 数据 \(X\)，卷积核 \(H\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 运算时将 \(X\) 与 \(H\) 做卷积（神经网络意义下的卷积，不翻转 \(H\)），假设 \(X\) 的长宽为 \(h\)x\(w\), \(H\) 的长宽为 \(m\)x\(n\)，那么输出 \(Y\) 的长宽为 \((h-m+1)\)x\((w-n+1)\)。</p>

<h4 id="toc_4">3. 非线性变换</h4>

<p><strong>输入</strong>: 数据 \(X\)<br/>
<strong>输出</strong>: \(Y\)<br/>
<strong>说明</strong>: 将 \(X\) 逐元素做非线性变换（例如 sigmoid，ReLu等），输出到 \(Y\)，\(Y\)与\(X\) 形状一致。</p>

<h4 id="toc_5">4. 平方误差</h4>

<p><strong>输入</strong>: 模型输出 \(f\)，训练数据 label \(y\)<br/>
<strong>输出</strong>: \(Loss\)<br/>
<strong>说明</strong>: \(Loss=0.5*(y-f)^2 \)</p>

<h3 id="toc_6">5. 内积</h3>

<p><strong>输入</strong>: 数据 \(X\)，数据 \(Y\)<br/>
<strong>输出</strong>: \(dot(X,Y)\)<br/>
<strong>说明</strong>: \(dot(X,Y)\) 为两路输入的逐元素相乘后求和，是一个标量。</p>

<p>利用以上运算节点以及基本的算术运算，我们就可以像 <strong>搭积木</strong> 一样，构建一个经典的3层神经网络，绿色为输入，黄色为模型参数，为节约空间，运算之间的 tensor 节点被我省去了。注意到，图中我包含了一个L2-正则项。</p>

<p><img src="media/14696391071598/model.png" alt="mode"/></p>

<p>构造完运算图后，我们只需要在网络的输入端提供 tensor \(X\)，并让 \(X\) 随着网络流动 (flow)到输出层即可，我想这就是 <a href="https://www.tensorflow.org">TensorFlow</a> 这个库的命名的由来。</p>

<h2 id="toc_7">使用 backprop 计算梯度</h2>

<p>神经网络相较其它模型的优势之一，就是能够使用 backprop 算法自动且高效地计算出梯度，它本质上是一种 <strong>动态规划</strong>，遵循推导动态规划算法的一般套路，我们首先在运算图中定义梯度计算的递归关系。</p>

<h4 id="toc_8">1. 计算目标</h4>

<p>假设模型的损失函数的输出是一个标量 \(z\)（大部分情况下如此），针对我们感兴趣的参数 tensor \(W_i\) 我们想要得到它们相对于 \(z\) 的梯度<br/>
\[<br/>
\frac {\partial z}{\partial W_1 },\frac {\partial z}{\partial W_2 } ...<br/>
\]</p>

<h4 id="toc_9">2. 递归关系</h4>

<p>运算图中的梯度具有递归关系，对于下图：<br/>
<img src="media/14696391071598/simple_net.png" alt="simple_net"/></p>

<p>利用导数的链式法则，假设 \(C\) 的长度为 \(m\)，\(A\) 的长度为 \(n\)，\(z\) 对于 \(A\) 的每个元素的导数为：<br/>
\[<br/>
\frac {\partial z}{\partial A_i }＝\Sigma_{j=1..m}\frac {\partial z}{\partial C_i } \frac{\partial C_j}{\partial A_i }<br/>
\]<br/>
将 \(A_i\) 组合成 tensor，得到：<br/>
\[<br/>
\frac {\partial z}{\partial A }＝\Sigma_{j=1..m}\frac {\partial z}{\partial C_j } \frac{\partial C_j}{\partial A}<br/>
\]<br/>
其中，\(\frac {\partial z}{\partial A }\) 是 tensor 拉伸成的向量，向量的元素是 \(\frac {\partial z}{\partial A_{i=1..n} }\)。接着，使用矩阵乘法将求和符号省略，我们得到：<br/>
\[<br/>
\frac {\partial z}{\partial A }＝(\frac{\partial C}{\partial A})^T \frac {\partial z}{\partial C } <br/>
\]<br/>
其中，我们利用了 <a href="http://mathworld.wolfram.com/Jacobian.html">雅可比矩阵</a><br/>
\[<br/>
\frac{\partial C}{\partial A}=<br/>
\begin{bmatrix}<br/>
  \frac{\partial C_1}{\partial A_1} &amp; \frac{\partial C_1}{\partial A_2} &amp; ...\\<br/>
  \frac{\partial C_2}{\partial A_1} &amp; \frac{\partial C_2}{\partial A_2} &amp; ...\\<br/>
  ... &amp; ... &amp; ...<br/>
 \end{bmatrix}<br/>
\]<br/>
以上推导说明，网络前级的梯度可以由后级的梯度乘以一个雅可比矩阵得到，这个结论适用于 <strong>任意</strong> 的运算节点，这给神经网络的软件架构带来了极大的灵活性：对于用户定义的任何运算，只要正确实现了这个雅可比矩阵的乘法，就能加入到梯度计算中来，这也是 Caffe 库中的每种 Layer 只要定义 Forward 和 Backward 接口就能参与网络构建的原因。</p>

<p>当然，一个 tensor 可以被一个以上的后续节点使用，如下图：<br/>
<img src="media/14696391071598/simple_net_2.png" alt="simple_net_2"/><br/>
根据导数的性质，我们有：<br/>
\[<br/>
\frac {\partial z}{\partial C_i }＝\Sigma_{j=1..m}\frac {\partial z}{\partial A_j } \frac{\partial A_j}{\partial C_i } + \Sigma_{y=1..k}\frac {\partial z}{\partial B_y } \frac{\partial B_y}{\partial C_i }<br/>
\]<br/>
类似于上面的推导，我们得到矩阵形式的递归公式：<br/>
\[<br/>
\frac {\partial z}{\partial C }＝(\frac{\partial A}{\partial C })^T \frac {\partial z} {\partial A }  + (\frac{\partial B }{\partial C })^T \frac {\partial z}{\partial B } <br/>
\]<br/>
简而言之，从两路流过来的梯度需要加起来，得到 \(C\) 节点的梯度。</p>

<h4 id="toc_10">3. 边界情况</h4>

<p>网络的最后级节点就是在输出端 \(z\)，边界情况十分简单：<br/>
\[<br/>
\frac {\partial z}{\partial z } = 1<br/>
\]</p>

<h4 id="toc_11">4. backprop</h4>

<p>定义了递归关系和边界情况后，我们可以发现大量冗余的计算：所有流向一个节点 \(A\) 的节点，总是需要计算一遍 \(\frac {\partial z}{\partial A } \)。此时我们有两种选择，一种是将计算后的结果缓存到一张查找表里，避免重复计算，也可以调整计算顺序，从网络的末端向前计算，这就是大部分动态规划采用的 bottom-up 策略，在神经网络中，这称作反向传播（backprop）</p>

<h3 id="toc_12">常见运算节点的 backprop</h3>

<p>上面提到，无论进行什么运算，backprop 总是雅可比矩阵的转置乘以后级的梯度，但是，对于某些运算来说，这个矩阵乘法可以表现为不同的形式。</p>

<h4 id="toc_13">1. 矩阵乘法（全联接层）</h4>

<p><strong>输入</strong>: 数据 \(X\)，参数 \(W\)<br/>
<strong>输出</strong>: \(Y＝WX\)<br/>
<strong>backprop</strong>: 很容易得到<br/>
\[\frac {\partial z}{\partial X }＝W^T \frac {\partial z}{\partial Y }\\<br/>
\frac {\partial z}{\partial W }＝\frac {\partial z}{\partial Y } X^T \]</p>

<h4 id="toc_14">2. 非线性变换</h4>

<p><strong>输入</strong>: 数据 \(X\)<br/>
<strong>输出</strong>: \(Y=nonlinear(X)\)<br/>
<strong>backprop</strong>: 将 \(X\) 逐元素做非线性变换（例如 sigmoid，ReLu等），输出到 \(Y\)，\(Y\)与\(X\) 形状一致，所以雅可比矩阵是一个对角矩阵，矩阵乘法退化成逐元素的乘法。</p>

<h4 id="toc_15">3. 卷积</h4>

<p>比较麻烦，留作练习吧，推导时需要草稿纸画一画，但是最终的矩阵乘法会退化成卷积。:)</p>

<h2 id="toc_16">下篇：使用 Python 实现神经网络库</h2>

<p>有了这个理论基础，我们就可以实现一个像 Caffe 一样的神经网络架构了，在下篇中，我会使用 Python 把以上抽象结构实现，并达到以下目标：</p>

<ol>
<li>网络结构可拼装，可扩展</li>
<li>在 MNIST 库上取得不错的准确率</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WSABIE 算法解释]]></title>
    <link href="http://heleifz.github.io/14696374110477.html"/>
    <updated>2016-07-28T00:36:51+08:00</updated>
    <id>http://heleifz.github.io/14696374110477.html</id>
    <content type="html"><![CDATA[
<p><img src="media/14696374110477/1369025_221049006000_2.jpg" alt="1369025_221049006000_2" style="width:443px;"/></p>

<p>算法论文：<a href="http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf">WSABIE: Scaling Up To Large Vocabulary Image Annotation</a></p>

<h2 id="toc_0">介绍</h2>

<p>WSABIE 是一个通用的打标签算法，无论对象是图像，视频，还是文本，只要能抽取出 feature，并提供一个固定的标签集合，就可以使用 WSABIE，由于算法的通用性，下文将需要打标签的东西称为 <em>对象</em>。</p>

<h2 id="toc_1">模型</h2>

<h3 id="toc_2">1. Joint Word-Image Model</h3>

<p>对于 WSABIE 来说，打标签的过程，就是计算所有标签与当前对象的相似性，并取出相似性最高的标签作为结果。由于对象的 feature 和标签是两种不同的东西，为了计算相似性，WSABIE 将它们映射到 <strong>同一个向量空间</strong> \( R^D \) 。</p>

<p><img src="media/14696374110477/wsb-1.png" alt="wsb-1" style="width:391px;"/></p>

<p>为了将对象 \(I\) 的 feature vector \( x \) 映射到 \( R^D \)，只需做线性变换 \(  \Phi _I(x)=Vx \)，其中 \(V\) 是变换矩阵。</p>

<p>由于标签是离散的符号，为了将其映射到 \( R^D \)，需要采用 Word2Vec 一样的思想，把它们转换成 embedding，换言之，我们把所有标签（假设有 \(N\) 个)对应的向量存在矩阵 \(W\) （大小 \(D \times N\)）中，第 i 个标签就是矩阵的第 i 列： \( \Phi _W(i)=W_i \)</p>

<p>于是，对象 \( I \) 与第 i 个标签的相似度就是：</p>

<p>\[ f_i(x) = \Phi _W(i)^T \Phi _I(x)=W_i^TVx \]</p>

<p>模型中的待学习参数为线性变换矩阵 \(V\)，以及所有标签的 embedding 向量 \(W\)。</p>

<h3 id="toc_3">2. 损失函数</h3>

<p>每条训练数据都以 \( (x,y) \) 的形式存在，其中 \(x\) 是对象，\(y\) 是它唯一的一条正确的标签。</p>

<p>模型的优化目标，是让正确描述对象的标签，与对象有较高的相似度。更具体地说 \[ rank_y(f(x)) = \Sigma _{i \ne y} I(f_i(x) \ge f_y(x)) \]</p>

<p>这个式子表示 <strong>&quot;排名大于 \(y\) 的标签的个数&quot;</strong>（\(I\) 是示性函数)，这条训练数据的误差是</p>

<p>\[ L(rank_y(f(x)))\]</p>

<p>其中 </p>

<p>\[ L(k) = \Sigma _{j=1}^k \alpha_j \]</p>

<p>根据常数 \( \alpha_j \) 的取值的不同，我们可以得到不同的损失函数，所以 \( L \) 其实代表一类函数。</p>

<p>如果 \( \alpha_j \) 取值都为 \(C\)， 那么如果 \(y\) 被模型排到第 \(k\) 名，损失值就是 \(Ck\)。论文中 \( \alpha_j = 1/j \)。</p>

<h2 id="toc_4">学习</h2>

<p>WSABIE 对损失函数进行了一系列近似，让模型能够使用随机梯度下降（SGD）在线更新参数，这个学习算法是本文 <em>最有价值的部分</em>。下面依次介绍近似步骤：</p>

<h3 id="toc_5">1. 改写损失函数</h3>

<p>\[ err(f(x), y) = L(rank_y(f(x)) \frac{rank_y(f(x))}{rank_y(f(x)) }\]</p>

<p>\[ =L(rank_y(f(x)) \frac{\Sigma_{i\ne y}I(f_i(x) \ge f_y(x))}{rank_y(f(x)} \]</p>

<p>\[ =\Sigma_{i \ne y} L(rank_y(f(x)) \frac{ I(f_i(x) \ge f_y(x))}{rank_y(f(x)} \]</p>

<h3 id="toc_6">2. 近似损失函数</h3>

<p>使用 hinge loss 近似 0／1 损失函数，得到如下的近似损失：</p>

<p>\[ \overline{err}(f(x), y)=\Sigma_{i \ne y} L(rank_y^1(f(x)) \frac{ |1-f_y(x)+f_i(x)|_+}{rank_y^1(f(x))} \]</p>

<p>其中：</p>

<p>\[<br/>
rank_y^1(f(x))=\Sigma_{i\ne y}I(1+f_i(x) \ge f_y(x))<br/>
\]</p>

<p>使用这个近似的损失函数，得到在当前数据集分布的期望误差：</p>

<p>\[Risk(f)=\int\overline{err}(f(x), y)dP(x,y)\]</p>

<p>按照分布 \(P(x,y)\) 随机选择一个样本 \((x,y)\)，计算其 \(\overline{err}(f(x), y)\)，可以对这个期望误差进行估计。</p>

<p>实际上 </p>

<p>\[ \Sigma_{i\ne y}\frac{ |1-f_y(x)+f_i(x)|_+}{rank_y^1(f(x))} \]</p>

<p>这个值，也可以通过采样进行估计，方法为，从满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的所有标签中，随机采样一个 \(\overline{y}\)，计算 \( 1-f_y(x)+f_{\overline{y}}(x) \)</p>

<p>综上，总共两步采样：</p>

<ol>
<li>随机选一个 \((x,y)\) </li>
<li>随机选一个标签 \( \overline{y} \)</li>
</ol>

<p>得到 \(Risk(f)\) 的一个估计：</p>

<p>\[ \overline{err}(f(x),y,\overline{y})=L(rank_y^1(f(x))(1-f_y(x)+f_{\overline{y}}(x))\]</p>

<h3 id="toc_7">3. 随机梯度下降</h3>

<p>接下来，就应用标准的随机梯度下降法，通过对 \(Risk(f)\) 的估计 \(\overline{err}(f(x),y,\overline{y})\) 求梯度，优化损失函数：</p>

<p>\[ \beta_{t+1}=\beta{t}-\gamma_t \frac {\partial\overline{err}(f(x),y,\overline{y})} {\partial\beta_{t}}\]</p>

<p>其中 \(\beta\) 是模型参数，也就是 \(W\) 和 \(V\) 矩阵。</p>

<h3 id="toc_8">4. 进一步优化性能</h3>

<p>此时，每一步优化的性能还是不好，原因是计算 \(\overline{err}(f(x),y,\overline{y})\) 时，我们需要知道所有满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的标签，因为：</p>

<ol>
<li>计算 \(L(rank_y^1(f(x))\) 要用到（记得那个求和式子吗）</li>
<li>采样的第二步要用到（因为要从这些标签里采样）</li>
</ol>

<p>论文中第 <strong>最聪明的地方</strong> 来了，作者提出了一个近似 \(\overline{err}(f(x),y,\overline{y})\) 的方法，解决以上两个问题：</p>

<p>在第二步的采样中，不再从满足 \( 1-f_y(x)+f_i(x) &gt; 0 \) 的标签里采样，而是从 <strong>除去 \(y\) 的所有</strong> 标签里进行采样，如果采样到的标签 i 不满足 \( 1-f_y(x)+f_i(x) &gt; 0 \)，就把它放回，再次采样，直到找到满足条件的标签为止。</p>

<p>表面上，这解决了第二次采样的问题，这种有放回采样的效果，等价于原来的采样，而且不需要事先算出所有满足条件的标签。</p>

<p>但实际上，这也解决了第一个问题。假设一共有 \(k\) 个标签满足  \( 1-f_y(x)+f_i(x) &gt; 0 \)，那么，从所有标签中采样到这种标签的概率就是 \(P=\frac{k}{N－1}\)，由于这种有放回采样满足 <em>几何分布</em> （还记得大学概率课吗？一直投骰子，直到遇到6），所以期望的采样次数为：</p>

<p>\[ E[采样次数] = \frac{N-1}{k} \]</p>

<p>我们可以用实际的采样次数近似期望的采样次数，所以我们可以近似得到 \(k\) 的值为 \( \frac{N-1}{采样次数} \)，于是可以算出 \(L(rank_y^1(f(x))\) 的近似值。</p>

<h2 id="toc_9">其它讨论</h2>

<p>此外，论文还对 \(W\) 和 \(V\) 每一列的大小做了约束：</p>

<p>\[ ||W||_2 &lt;= C,||V||_2 &lt;= C \]</p>

<p>在优化时，如果某一列超过了 C，就将该列的值等比例缩小一下。</p>

<p>如果理解了后续的采样步骤，就会理解对损失函数的近似步骤：</p>

<blockquote>
<p>之所以将 0/1 loss 近似为 hinge loss，是为了让损失函数连续，之所以用 \(rank_y^1(f(x),y) \) 替换 \(rank_y(f(x),y) \) ，是为了在第二步采样时去掉分母</p>
</blockquote>

<p>在训练过程的每一步，都仅涉及一个训练数据 \((x,y)\)，以及后续的几条采样数据，这种 online learning 的学习方法适用于海量数据集。</p>

<p>实际应用中，每个对象会有 <em>多个标签</em>，在训练时，可以针对每个标签单独跑一次采样，</p>

<p>此外，我还有一个想法，模型中的输入 feature 已经是计算好了的，我们也可以把原始的数据直接放入模型，例如，利用 NN 自动学习出特征，实现 end-to-end 的打标签架构。</p>

<p>算法名字很像&quot;芥末&quot;，所以文章的开头我放了芥末。</p>

]]></content>
  </entry>
  
</feed>
